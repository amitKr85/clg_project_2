__label__ClusteringAlgorithms __label__ClassificationAlgorithms __label__SupportVectorMachines __label__Education __label__Databases __label__MachineLearning __label__Algorithms __label__LearningSystems using support vector machines to classify student attentiveness for the development of personalized learning systems there have been many studies in which researchers have attempted to classify student attentiveness many of these approaches depended on a qualitative analysis and lacked any quantitative analysis therefore this work is focused on bridging the gap between qualitative and quantitative approaches to classify student attentiveness thus this research applies machine learning algorithms kmeans and svm to automatically classify students as attentive or inattentive using data from a consumer rgbd sensor results of this research can be used to improve teaching strategies for instructors at all levels and can aid instructors in implementing personalized learning systems which is a national academy of engineering grand challenge this research applies machine learning algorithms to an educational setting data from these algorithms can be used by instructors to provide valuable feedback on the effectiveness of their instructional strategies and pedagogies instructors can use this feedback to improve their instructional strategies and students will benefit by achieving improved learning and subject mastery ultimately this will result in the students increased ability to do work in their respective areas broadly this work can help advance efforts in many areas of education and instruction it is expected that improving instructional strategies and implementing personalized learning will help create more competent capable and prepared persons available for the future workforce 
__label__LearningSystems __label__DataModels __label__PredictiveModels __label__ComputerSecurity __label__MachineLearningAlgorithms __label__MathematicalModel a usercentric machine learning framework for cyber security operations center to assure cyber security of an enterprise typically siem security information and event management system is in place to normalize security events from different preventive technologies and flag alerts analysts in the security operation center soc investigate the alerts to decide if it is truly malicious or not however generally the number of alerts is overwhelming with majority of them being false positive and exceeding the soc s capacity to handle all alerts because of this potential malicious attacks and compromised hosts may be missed machine learning is a viable approach to reduce the false positive rate and improve the productivity of soc analysts in this paper we develop a usercentric machine learning framework for the cyber security operation center in real enterprise environment we discuss the typical data sources in soc their work flow and how to leverage and process these data sets to build an effective machine learning system the paper is targeted towards two groups of readers the first group is data scientists or machine learning researchers who do not have cyber security domain knowledge but want to build machine learning systems for security operations center the second group of audiences are those cyber security practitioners who have deep knowledge and expertise in cyber security but do not have machine learning experiences and wish to build one by themselves throughout the paper we use the system we built in the symantec soc production environment as an example to demonstrate the complete steps from data collection label creation feature engineering machine learning algorithm selection model performance evaluations to risk score generation 
__label__MachineLearning __label__DataMining __label__ClassificationAlgorithms __label__MachineLearningAlgorithms __label__AlgorithmDesignAndAnalysis __label__LearningSystems __label__Computers comparison of classification techniques used in machine learning as applied on vocational guidance data recent developments in information systems as well as computerization of business processes by organizations have led to a faster easier and more accurate data analysis data mining and machine learning techniques have been used increasingly in the analysis of data in various fields ranging from medicine to finance education and energy applications machine learning techniques make it possible to deduct meaningful further information from those data processed by data mining such meaningful and significant information helps organizations to establish their future policies on a sounder basis and to gain major advantages in terms of time and cost this study applies classification algorithms used in data mining and machine learning techniques on those data obtained from individuals during the vocational guidance process and tries to determine the most appropriate algorithm 
__label__MachineLearning __label__MachineLearningAlgorithms __label__Fatigue __label__PredictiveModels __label__AlgorithmDesignAndAnalysis __label__SupportVectorMachines __label__LeastSquaresMethods __label__DesignOptimization __label__LifeEstimation __label__LifetimeEstimation fitting and prediction for crack propagation rate based on machine learning optimal algorithm establishing fatigue crack propagation rate is the key to forecasting structure fatigue lifetime nine parameters fatigue crack propagation rate model and mcevily model are widely applied at present but it is very complex to realize these models partial derivative must be calculated and there is large deviation between fitted static parameter and actual value and physical conception is nt clear in accordance with the disadvantage above methods based on optimum parameter selection with grid search and cross validation we presented optimal common machine learning algorithm least squares support vector machinelssvm method for fatigue crack propagation rate forecast complicated and strong nonlinear fatigue crack propagation rate curve was simulated by network design and conformation of lssvm learning algorithm and the optimized svm parameters were selected by the method of network searching and cross validation compared the errors with output value of the optimized model and output value from nine parameters fatigue crack propagation rate fitting model lssvm whose parameter was optimized with cross validation had excellent ability of nonlinear modeling and generalization it provided a simple and feasible intelligent approach for material fatigue analysis 
__label__DataMining __label__SupportVectorMachines __label__MachineLearning __label__MachineLearningAlgorithms __label__Boosting __label__SupportVectorMachineClassification __label__RiskManagement __label__PatternRecognition __label__LearningSystems __label__Automation financial data mining based on support vector machines and ensemble learning with the rapid development of ecommerce financial data mining has been one of the most important research topics in the data mining community support vector machines svms and ensemble learning are two popular techniques in the machine learning field in this paper support vector machines and ensemble learning are used to classify financial data respectively the experiments conducted on the public dataset show that compared with svms ensemble learning achieves obvious improvement of performance 
__label__HeuristicAlgorithms __label__MachineLearningAlgorithms __label__SupportVectorMachines __label__MachineLearning __label__SupportVectorMachineClassification __label__TrainingData __label__PartitioningAlgorithms __label__DataEngineering __label__History __label__EducationalInstitutions a heuristic algorithm to incremental support vector machine learning incremental learning techniques are possible solutions to handle vast data as information from internet updating gets faster support vector machine works well for incremental learning model with impressive performance for its outstanding power to summarize the data space in a concise way this paper proposes a heuristic algorithm to incremental learning with svm taking the possible impact of new training data to history data into account the idea of this heuristic algorithm is that the partition difference set has less elements and existing hyperplane is much closer to the optimal one new support vectors in this algorithm consist of existing support vectors and partition difference set of new training data and history data by separating hyperplane the algorithm improves classification precision by adding partition difference set and decreases the computation complexity by constructing new classification hyperplane on support vector set the experimental results show that this heuristic algorithm is efficient and effective to improve the classification precision 
__label__MachineLearning __label__ElectronicMail __label__Frequency __label__LearningSystems __label__Cybernetics __label__IntelligentNetworks __label__Filters __label__UnsolicitedElectronicMail __label__Protection __label__PostalServices statistical machine learning used in integrated antispam system iass is the integrated antispam system which adopts machine learning to filter spam in a intelligent flexible precise and selfadaptive way the methods of linear classification based on optimal separating hyperplane and kmeans clustering are used in action recognition layer the method of improved naive bayes is used in content analysis layer the application of machine learning helps improve the performance of iass 
__label__SingleMachineScheduling __label__Dispatching __label__IntelligentAgent __label__JobShopScheduling __label__SimulatedAnnealing __label__MachineIntelligence __label__SchedulingAlgorithm __label__Manufacturing __label__Routing __label__JobProductionSystems dynamic single machine scheduling using qlearning agent single machine scheduling methods have attracted a lot of attentions in recent years most dynamic single machine scheduling problems in practice have been addressed using dispatching rules however no single dispatching rule has been found to perform well for all important criteria and no rule takes into account the status or the other resources of system s environment in this research an intelligent agentbased single machine scheduling system is proposed where the agent is trained by a new improved qlearning algorithm in such scheduling system agent selects one of appropriate dispatching rules for machine based on available information the agent was trained by a new simulated annealingbased qlearning algorithm the simulation results show that the simulated annealingbased qlearning agent is able to learn to select the best dispatching rule for different system objectives the results also indicate that simulated annealingbased qlearning agent could perform well for all criteria which is impossible when using only one dispatching rule independently 
__label__Neurons __label__Accuracy __label__Sensitivity __label__Training __label__RadialBasisFunctionNetworks __label__Heart __label__MachineLearning sensitivity based growing and pruning method for rbf network in online learning environments how to define the architecture of classifiers dynamically is one of the major research topics in online learning this paper presents a new online learning algorithm for radial basis function network named sensitivity based neurons growing and pruning method for rbf network sbgap  the performance of sbgap is evaluated experimentally by comparing accuracy and the number of neurons with the existing methods the experimental results show that sbgap achieve litter higher accuracy with fewer hidden units in most situations 
__label__SupportVectorMachines __label__SupportVectorMachineClassification __label__FuzzySets __label__MachineLearning __label__NoiseReduction __label__Cybernetics __label__Mathematics __label__ComputerScience __label__ResearchAndDevelopment __label__ElectronicMail a new fuzzy multicategory support vector machines classifier this paper proposes a new fuzzy multicategory support vector machines fmsvm classifier the main idea is that the proposed fmsvm uses knowledge of the ambiguity associated with the membership of samples for a given class and the relative location of samples to the origin compared with the existing svms the new proposed fmsvm that uses the l2norm in the objective function has the improvement in aspects of classification accuracy and reducing the effects of noises and outliers 
__label__SupportVectorMachines __label__SupportVectorMachineClassification __label__Testing __label__MachineLearning __label__Mathematics __label__ComputerScience __label__ElectronicMail __label__ClassificationAlgorithms __label__NumericalSimulation __label__QuadraticProgramming improved fuzzy multicategory support vector machines classifier this paper investigates an improved fuzzy multicategory support vector machines classifier ifmsvm  it uses knowledge of the ambiguity associated with the membership of data samples of a given class and relative location to the origin to improve classification performance with high generalization capability in some aspects classifying accuracy of the new algorithm is better than that of the classical support vector classification algorithms numerical simulations show the feasibility and effectiveness of this algorithm
__label__MachineLearningAlgorithms __label__Hazards __label__SupportVectorMachines __label__AssociationRules __label__DataMining __label__Insurance __label__SupportVectorMachineClassification __label__Databases __label__MachineLearning __label__Kernel the application of machine learning algorithm in underwriting proces this paper firstly analyses the actual underwriting methods of chinese life insurance companies and points out the merits and shortcomings of these methods then the incomplete database of insurance company is mined by the data mining s association rule algorithm thirdly the support vector machine svm is applied to the underwriting process to classify the applicants finally the directions for improving this algorithm are pointed out the algorithm proposed in this paper has promising future in underwriting process 
__label__MachineLearning __label__Training __label__ClassificationAlgorithms __label__Uncertainty __label__LearningSystems __label__ComplexityTheory __label__SupportVectorMachines a survey on active learning strategy active learning is a hot topic in machine learning field the main task of active learning is to automatically select the representative instances for efficiently reducing the sample complexity this paper presents a brief survey of active learning regarding selection methods query strategies applications and other related works 
__label__SupportVectorMachines __label__SupportVectorMachineClassification __label__MachineLearning __label__ChemicalAnalysis __label__InformationTechnology __label__Petroleum __label__ChemicalTechnology __label__InformationAnalysis __label__CharacterRecognition __label__SpeechRecognition a new nusupport vector machine for training sets with duplicate samples analyzed theoretically spl nusvm was found to be overdependent on each training sample even if the samples have same value this dependence would result in more time for training more support vectors and more decision time in order to overcome this problem we propose a new spl nusvm this new spl nusvm multiplies each slack variable in the objective function by a weight factor and automatically computes each weight factor by the number of corresponding samples with same value before training theoretical analysis and the results of experiments show that the new spl nusvm has the same classification precision rate as the standard spl nusvm and the new spl nusvm is faster than the spl nusvm in training and decision if the training sets have same value samples 
__label__NonlinearSystems __label__MachineLearning __label__ProbabilityDistribution __label__Training __label__SearchMethods __label__UnsupervisedLearning __label__BenchmarkTesting restricted boltzmann machine for nonlinear system modeling in this paper we use a deep learning method restricted boltzmann machine for nonlinear system identification the neural model has deep architecture and is generated by a random search method the initial weights of this deep neural model are obtained from the restricted boltzmann machines to identify nonlinear systems we propose special unsupervised learning methods with input data the normal supervised learning is used to train the weights with the output data the modified algorithm is validated by modeling two benchmark systems 
__label__MachineLearning __label__SupportVectorMachines __label__ElectronicLearning __label__Training __label__EducationalInstitutions __label__Materials __label__BrainModeling application research of support vector machine in elearning for personality in order to accurately build the learner s learning style in elearning according to the needs and preferences to provide personalized learning materials and harmonious humancomputer interaction environment this paper combines feldersilverman learning style with support vector machine technology and use machine learning technologies for learners to build dynamic learning style through the analysis of the emotion and recognition interaction of the personalized elearning based on statistical learning theory and support vector machine technology it demonstrates the correctness and feasibility using support vector machine to build learning styles the combination of support vector machine emotion and recognition interaction in the personalized elearning makes great contribution to build humancomputer interaction environment 
__label__MachineLearning __label__LearningSystems __label__StatisticalLearning __label__FunctionApproximation __label__SupportVectorMachines __label__SupportVectorMachineClassification __label__Focusing __label__Bioinformatics __label__ObjectDetection __label__FaceDetection from bits to information with learning machines theory and applications summary form only given learning is becoming the central problem in trying to understand intelligence and in trying to develop intelligent machines the paper outlines some previous efforts in developing machines that learn it sketches the authors s work on statistical learning theory and theoretical results on the problem of classification and function approximation that connect regularization theory and support vector machines the main application focus is classification and regression in various domainssuch as sound text video and bioinformatics in particular the paper describe the evolution of a trainable object detection system for classifying objectssuch as faces and people and carsin complex cluttered images finally it speculates on the implications of this research for how the brain works and review some data which provide a glimpse of how 3d objects are represented in the visual cortex 
__label__MachineLearning __label__Ontologies __label__NaturalLanguages __label__NaturalLanguageProcessing __label__KnowledgeAcquisition __label__Tagging __label__LearningSystems __label__InformationRetrieval __label__ComputerScience __label__KnowledgeEngineering machine learning for automatic acquisition of chinese linguistic ontology knowledge due to the complexity and flexibility of natural language automatic linguistic knowledge acquisition and its application research becomes difficult in this paper we present a machine learning method to automatically acquire chinese linguistic ontology knowledge from typical corpus this study first defined the description frame of chinese linguistic ontology knowledge and then automatically acquired the usage of a chinese word with its cooccurrence of context in using semantic pragmatics syntactic etc from the corpus final the above information and their representation act as chinese linguistic ontology knowledge bank we completed two groups of experiments ie documents similarity computing text reordering for information retrieval compared with previous works the proposed method solves the inferior precision of nature language processing 
__label__Semantics __label__BigData __label__DataModels __label__Ontologies __label__TimeSeriesAnalysis __label__Databases __label__TriplesDataStructure integrated access to big data polystores through a knowledgedriven framework the recent successes of commercial cognitive and ai applications have cast a spotlight on knowledge graphs and the benefits of consuming structured semantic data today knowledge graphs are ubiquitous to the extent that organizations often view them as a single source of truth for all of their data and other digital artifacts in most organizations however big data comes in many different forms including time series images and unstructured text which often are not suitable for efficient storage within a knowledge graph this paper presents the semantics toolkit semtk  a framework that enables access to polyglotpersistent big data stores while giving the appearance that all data is fully captured within a knowledge graph semtk allows data to be stored across multiple storage platforms eg big data stores such as hadoop graph databases and semantic triple stores  with the bestsuited platform adopted for each data type while maintaining a single logical interface and point of access thereby giving users a knowledgedriven veneer across their data we describe the ease of use and benefits of constructing and querying polystore knowledge graphs with semtk via four industrial use cases at ge 
__label__BigData __label__Portals __label__Collaboration __label__WebServices __label__Monitoring __label__StreamingMedia webbased collaborative big data analytics on big data as a service platform as data has been increasing explosively due to development of social networks and cloud computing there has been a new challenge for storing processing and analyzing a large volume of data the traditional technologies do not become a proper solution to process big data so that a big data platform has begun to emerge it is certain that big data platform helps users develop analysis service effectively however it still takes a long time to collect data develop algorithms and analytics services we present a collaborative big data analytics platform for big data as a service developers can collaborate with each other on the platform by sharing data algorithms and services therefore this paper describes big data analytics platform that effectively supports to manage big data and develop analytics algorithms and services collaborating with data owners data scientists and service developers on the web finally we introduce a cctv metadata analytics service developed on the platform 
__label__Roads __label__BigData __label__DataVisualization __label__DataMining __label__Conferences __label__CloudComputing __label__ImageColorAnalysis a timeline visualization system for road traffic big data the rapid converging of big data and iot internet of things technologies provides more opportunities in the area of road traffic applications in this paper we discuss a timeline visualization tool which enables us to better understand of traffic behaviors from road traffic big data 
__label__BigData __label__Software __label__RequirementsEngineering __label__DataModels __label__ComputationalModeling __label__ContextModeling __label__SoftwareEngineering towards a big data requirements engineering artefact model in the context of big data software development projects poster extended abstract in this paper we describe our ongoing research aimed at defining a requirements engineering artefact model ream in the context of big data software applications this model aims to provide a big picture of the requirements engineering work products created and used in big data software development projects ream are important tools that can be used as references for the definition of domainspecific re models system lifecycle processes and artefactcentered processes currently bereft in the big data software engineering research 
__label__DecisionTrees __label__SupportVectorMachines __label__Entropy __label__SupportVectorMachineClassification __label__MachineLearning __label__ClassificationTreeAnalysis __label__InductionGenerators __label__InverseProblems __label__MachineLearningAlgorithms __label__Cybernetics a new heuristic of the decision tree induction decision tree induction is one of the useful approaches for extracting classification knowledge from a set of featurebased instances the most popular heuristic information used in the decision tree generation is the minimum entropy this heuristic information has a serious disadvantagethe poor generalization capability 3  support vector machine svm is a classification technique of machine learning based on statistical learning theory it has good generalization considering the relationship between the classification margin of support vector machine svm and the generalization capability the large margin of svm can be used as the heuristic information of decision tree in order to improve its generalization capability this paper proposes a decision tree induction algorithm based on large margin heuristic comparing with the binary decision tree using the minimum entropy as the heuristic information the experiments show that the generalization capability has been improved by using the new heuristic 
__label__BigData __label__Government __label__Semantics __label__Communities __label__Conferences __label__BestPractices __label__DataPrivacy sharing best practices for the implementation of big data applications in government and science communities the federal big data working group supports the federal big data initiative but is not endorsed by the federal government or its agencies this working group uses meetups with onsite and virtual participation to share best practices for the implementation of big data applications in government and science communities decisionmakers and the scientific community interact with data science in order to take advantage of the big data transformation of how information is used in science decision support data discovery and data publishing the working group federates use cases data publications solutions and technologies the range of topics is illustrated in a keynote and panel discussion at a recent big data conference and in a summary of recent working group meetups 
__label__BigData __label__PortsComputers __label__DataModels __label__Twitter __label__Sparks __label__Synchronization design of a scalable data stream channel for big data processing this paper outlines big data infrastructure for processing data streams our project is distributed stream computing platform that provides costeffective and largescale big data services by developing data stream management system this research contributes to advancing feasibility of big data processing for distributed realtime computation even when they are overloaded 
__label__MultimediaCommunication __label__MultimediaDatabases __label__AdaptationModels __label__DataModels __label__Indexing __label__BigData __label__QueryProcessing gradientbased signatures for big multimedia data with the continuous increase of heterogeneous multimedia data the question of how to access big multimedia data efficiently has become of crucial importance in order to provide fast access to complex multimedia data we propose to approximate contentbased features of multimedia objects by means of generative models the proposed gradientbased signatures epitomize a high quality contentbased approximation of multimedia objects and facilitate efficient indexing and query processing at large scale 
__label__Protocols __label__BigData __label__Encryption __label__ParallelProcessing __label__ComputationalEfficiency __label__Software data confidentiality challenges in big data applications in this paper we address the problem of data confidentiality in big data analytics in many fields much useful patterns can be extracted by applying machine learning techniques to big data however data confidentiality must be protected in many scenarios data confidentiality could well be a prerequisite for data to be shared we present a scheme to provide provable secure data confidentiality and discuss various techniques to optimize performance of such a system 
__label__DataEngineering __label__DataAnalysis __label__DataIntegration __label__BigData __label__DistributedDatabases __label__DataModels __label__Uncertainty some key problems of data management in army data engineering based on big data this paper analyzed the challenges of data management in army data engineering such as big data volume data heterogeneous high rate of data generation and update high time requirement of data processing and widely separated data sources we discussed the disadvantages of traditional data management technologies to deal with these problems we also highlighted the key problems of data management in army data engineering including data integration data analysis representation of data analysis results and evaluation of data quality 
__label__DataMining __label__BigData __label__ResourceDescriptionFramework __label__DataModels __label__KnowledgeBasedSystems __label__DataAnalysis __label__Proposals linked open data mining for democratization of big data data is everywhere and nonexpert users must be able to exploit it in order to extract knowledge get insights and make wellinformed decisions the value of the discovered knowledge from big data could be of greater value if it is available for later consumption and reusing in this paper we present an infrastructure that allows nonexpert users to i apply userfriendly data mining techniques on big data sources and ii share results as linked open data lod  the main contribution of this paper is an approach for democratizing big data through reusing the knowledge gained from data mining processes after being semantically annotated as lod then obtaining linked open knowledge our work is based on a modeldriven viewpoint in order to easily deal with the wide diversity of open data formats 
__label__BigData __label__DataPrivacy __label__DistributedDatabases __label__Proposals __label__CloudComputing __label__Protocols __label__DataModels pedigreeing your big data datadriven big data privacy in distributed environments this paper introduces a general framework for supporting datadriven privacypreserving big data management in distributed environments such as emerging cloud settings the proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via securityinspired protocols that check several protocol layers in order to achieve the desired privacy unfortunately this injects considerable computational overheads in the overall process thus introducing relevant challenges to be considered our approach instead tries to recognize the pedigree of suitable summary data representatives computed on top of the target big data repositories hence avoiding computational overheads due to protocol checking we also provide a relevant realization of the framework above the socalled datadriven aggregateprovenance privacypreserving big multidimensional data driprom framework which specifically considers multidimensional data as the case of interest 
__label__BigData __label__GraphicsProcessingUnits __label__Memory __label__Companies __label__InformationTechnology data science vs big data utm big data centre big data tsunami has hit malaysia recently that has awakening the industry and academy communities to aggressively address the insight hindsight and foresight challenges ensuring malaysia to be among the top world players in big data information economy for the next decade rapid development of information and communication technology ict in this era is very significant due to increasing number of users accessing data keeps growing by the time this phenomenon has been coined as big data what is big data  we address big data as assets that needs unique platform to deal with bizarre behavior of datasets whose size is beyond the ability of typical data storage to manage mine and analyze accordingly this bizarre behavior requires three main personalities volume velocity and variety that basically need new architecture techniques algorithms and analytics to uncover the golden and hidden knowledge from information obesity from these perspectives we demonstrate our experiences in setting up our data sciencebig data platform algorithms and tool to align with big data plug and play within the academic environment as well as our services to the community and industries 
__label__BigData __label__Optimization __label__DataModels __label__QualityAssessment big data preprocessing closing the data quality enforcement loop in the big data era data is the core for any governmental institutional and private organization efforts were geared towards extracting highly valuable insights that can not happen if data is of poor quality therefore data quality dq is considered as a key element in big data processing phase in this stage low quality data is not penetrated to the big data value chain this paper addresses the data quality rules discovery dqr after the evaluation of quality and prior to big data preprocessing we propose a dqr discovery model to enhance and accurately target the preprocessing activities based on quality requirements we defined a set of preprocessing activities associated with data quality dimensions dqd s to automatize the dqr generation process rules optimization are applied on validated rules to avoid multipasses preprocessing activities and eliminates duplicate rules conducted experiments showed an increased quality scores after applying the discovered and optimized dqr s on data 
__label__DecisionTrees __label__Training __label__TrainingData __label__BigData __label__PredictionAlgorithms __label__Ethics __label__Contracts a big data analytics framework for forecasting rare customer complaints a use case of predicting ma members complaints to cms centers for medicare medicaid services cms publishes medicare part c star ratings each year to measure the quality of care of medicare advantage ma contracts one of the key measures is complaints about the health plan which is captured in complaints tracking module ctm  complaints resulted in ctm are rare events for ma contracts with 25 star ratings number of complaints for every 1000 members range from 10 to 184 over last 5 years reducing number of complaints is extremely important to ma plans as they impact cms reimbursements to ma plans forecasting and reducing complaints is an extremely technically challenging task and involves ethics considerations in patients rights and privacy in this research we constructed a big data analytics framework for forecasting rare customer complaints first we built a big data ingestion pipelines on a hadoop platform a ingest ma plan s customer complaints data from ctm from past 3 years b ingest health plan s call center data for ma members from past 3 years including both structured data and unstructured text script for the calls c ingest ma members medical claims including members demographics and enrollment history d ingest ma members pharmacy claims e integrate and unified data from above sources and enrich the data with additional engineered features into a big wide table one row per member for analysis and modeling second we designed a unique decision tree based large ensemble with oversampling leos algorithm which mimics random forest but with extreme oversampling of target class to increase bias and leverages the parallel computing of hadoop clusters by generating thousands of fixed size training data sets and for each such dataset training a decision trees with similar fixed tree structure and ensemble them third we validated our framework and leos learning algorithm with real data and also discussed ethics issues we encountered in handling data and applying findings from research 
__label__Neurons __label__ComplexityTheory __label__ComputationalModeling __label__Training __label__ErrorAnalysis __label__TrainingData __label__Standards an incremental learning approach for restricted boltzmann machines determination of model complexity is a challenging issue to solve computer vision problems using restricted boltzmann machines rbms  many algorithms for feature learning depend on crossvalidation or empirical methods to optimize the number of features in this work we propose an learning algorithm to find the optimal model complexity for the rbms by incrementing the hidden layer the proposed algorithm is composed of two processes 1 determining incrementation necessity of neurons and 2 computing the number of additional features for the increment specifically the proposed algorithm uses a normalized reconstruction error in order to determine incrementation necessity and prevent unnecessary increment for the number of features during training our experimental results demonstrated that the proposed algorithm converges to the optimal number of features in a single layer rbms in the classification results our model could outperform the nonincremental rbm 
__label__BigData __label__Pipelines __label__CustomerRelationshipManagement __label__DataMining __label__BenchmarkTesting __label__Storms __label__Software decision trees training training data big data prediction algorithms ethics contracts big data gathering and mining pipelines for crm using opensource customer relationship management crm is currently the fastest growing sector of enterprise software estimated to increase to worldwide by 2017 crm technologies increasingly use data mining primitives across multiple applications at the same time the growth of big data has led to the evolution of an open source big data software stack primarily powered by apache software that rivals traditional enterprise database rdbms stacks new technologies such as kafka storm hbase have significantly enriched this open source stack alongside more established technologies such as hadoop mapreduce and mahout today enterprises have a choice to make regarding which stack they will choose to power their big data applications however there are no published studies in literature on enterprise big data pipelines built using open source components supporting crm specific questions that enterprises have include how is the data processed and analyzed in such pipelines what are the building blocks of such pipelines how long does each step of this processing take in this work we answer these questions for a large scale serving over a 100m customers industrial crm pipeline that incorporates data mining and serves several applications our pipeline has broadly two parts the first is a data gathering part that uses kafka storm and hbase the second is a data mining part that uses mahout and hadoop mapreduce we also provide timings for common tasks in the second part such as data preprocessing for machine learning clustering reservoir sampling and frequent itemset extraction 
__label__Coal __label__InformationServices __label__Metadata __label__BigData __label__DistributedDatabases __label__CopyrightProtection __label__CoalMining digital construction of coal mine big data for different platforms based on life cycle big data has penetrated into various industries and business functions and become important factors of production in the global economy in the big data technology system big data collection is the basis the storage analysis integration and visualization of unstructured data and semistructured data will become an important focus of the big data innovation traditional structured data will no longer be the core of big data based on the life cycle theory using new digital technology such as acquisition processing storage organization and copyright protection clusters of high concurrency retrieval and dynamic scheduling intelligent digital display coal mine industry information data can be collected and integrated to realize centralized management unified retrieval and joint exhibition of information resources to provide technical means and reference for the digital construction of heterogeneous coal mine information data by means of big data thinking 
__label__MedicalServices __label__Sparks __label__BigData __label__Java __label__Context __label__ComputerArchitecture __label__Business a novel bigdata processing framwork for healthcare applications bigdatahealthcareinabox herein we present a novel bigdata framework for healthcare applications healthcare data is well suited for bigdata processing and analytics because of the variety veracity and volume of these types of data in recent times many areas within healthcare have been identified that can directly benefit from such treatment however setting up these types of architecture is not trivial we present a novel approach of building a bigdata framework that can be adapted to various healthcare applications with relative use making this a onestop bigdatahealthcareinabox 
__label__DataIntegrity __label__PowerQuality __label__Monitoring __label__PowerMeasurement __label__Redundancy __label__BigData __label__Business data quality assessment for online monitoring and measuring system of power quality based on big data and data provenance theory currently online monitoring and measuring system of power quality has accumulated a huge amount of data in the age of big data those data integrated from various systems will face big data application problems this paper proposes a data quality assessment system method for online monitoring and measuring system of power quality based on big data and data provenance to assess integrity redundancy accuracy timeliness intelligence and consistency of data set and single data specific assessment rule which conforms to the situation of online monitoring and measuring system of power quality will be devised to found data quality problems thus it will provide strong data support for big data application of power quality 
__label__UrbanAreas __label__BigData __label__Government __label__DecisionMaking __label__DataModels __label__DataMining __label__MarketResearch research on analysis system of city price based on big data this paper attempts to construct an analysis model of city price by combining the big data system with the information of city price so as to provide the reference for the government to implement the policy of the accurate price control this paper considers that the analysis system of city price should include decisionmaking layer supporting layer and showing layer on the conceptual level and its transmission path should include the data collection data management data mining decision making and safety protection when it comes to the construction of the subsystem technological tools such as data mining cloud computing and visualization should be used mainly in order to build data acquisition subsystem data management subsystem data analysis subsystem data transmission subsystem and so on and provide the graphic description for the corresponding technological path at the same time 
__label__Safety __label__Business __label__Monitoring __label__DataVisualization __label__BigData __label__DataAnalysis __label__Decoding big data analytics platform for flight safety monitoring the conventional methods of data analytics for flight safety monitoring have met many bottlenecks this paper analyzes the insufficiencies of the preliminary business process of an airline for the purpose of meeting requirements of efficiency and accuracy and avoiding the drawbacks encountered before the architectural framework of the flight safety monitoring platform utilizing big data technology is proposed and demonstrated by the function module structure and logical structure the platform is implemented by dividing the system into five subsystems namely data acquisition data decoding data storage data analysis and visualization 
__label__BigData __label__IterativeMethods __label__DataVisualization __label__DataModels __label__Systematics __label__BestPractices __label__DataMining an iterative methodology for big data management analysis and visualization big data constitutes an opportunity for companies to empower their analysis however at the moment there is no standard way for approaching big data projects this coupled with the complex nature of big data is the cause that many big data projects fail or rarely obtain the expected return of investment in this paper we present a methodology to tackle big data projects in a systematic way avoiding the aforementioned problems to this end we review the state of the art identifying the most prominent problems surrounding big data projects best practices and methods then we define a methodology describing step by step how these techniques could be applied and combined in order to tackle the problems identified and increase the success rate of big data projects 
__label__Instruments __label__BigData __label__Oceans __label__Observatories __label__Vocabulary __label__Standards __label__Sensors where big data meets linked data applying standard data models to environmental data streams in august 2015 a new seafloor observatory was deployed in galway bay ireland the sensors on the observatory platform are connected by fibreoptic cable to a shore station where a broadband connection allows data transfer to the marine institute s data centre this setup involved the development of a new data acquisition system which takes advantage of open source streaming data solutions developed in response to the big data paradigm in particular the velocity aspect this activity merges concepts from the arenas of both big data and internet of things where data standardisation is not normally considered this paper considers the architecture implemented to stream marine data from instrument to end user and offers suggestions on how to standardise these data streams 
__label__BigData __label__DataPrivacy __label__Security __label__Production __label__MarketResearch __label__Databases metaanalysis of big data security and privacy scholarly literature gaps we collected 79012 articles from 19162016 related to big data to determine which topics were being studied and how much of the literature was focused on privacy or securityrelated keywords the analysis demonstrated that the big data paradigm commenced in late 2011 and the research production exponentially rose starting in 2012 which approximated a weibull distribution that captured 82 of the variance p  01  we found there were 13 dominant topics capturing 49 of the big data production in journals during 20112016 but privacy and security topics accounted for only 2 and this trend recently dropped to less than 1 thus we argued that we need to stimulate more big data privacysecurity research 
__label__BigData __label__MarketResearch __label__DataVisualization __label__Surgery __label__CardiovascularDiseases big data analysis of youth tobacco smoking trends in the united states as large amounts of data are now generated in the healthcare industry big data technologies are used to process these data tobacco smoking has been significant among the youth in the united states in this research we use big data techniques such as r and tableau to explore the tobacco smoking trends among the youth in the united states results indicate that there is more number of youth male smokers than youth female smokers the results also indicate that more than 51 percent of current youth smokers want to quit smoking 
__label__TextCategorization __label__MachineLearning __label__NearestNeighborSearches __label__Testing __label__EuclideanDistance __label__MachineLearningAlgorithms __label__ClassificationTreeAnalysis __label__DecisionTrees __label__SupportVectorMachines __label__SupportVectorMachineClassification chinese text categorization study based on feature weight learning text categorization tc is an important component in many information organization and information management tasks two key issues in tc are feature coding and classifier design the euclidean distance is usually chosen as the similarity measure in knearest neighbor classification algorithm all the features of each vector have different functions in describing samples so we can decide different function of every feature by using feature weight learning in this paper text categorization via knearest neighbor algorithm based on feature weight learning is described the numerical experiments prove the validity of this learning algorithm 
__label__Ontologies __label__Owl __label__DataModels __label__BigData __label__Semantics __label__AnalyticalModels an owl ontology for supporting semantic services in big data platforms in the last years there was a growing interest in the use of big data models to support advanced data analysis functionalities many companies and organizations lack it expertise and adequate budget to have benefits from them in order to fill this gap a modelbased approach for big data analyticsasaservice mbdaaas can be used the proposed model composed by declarative procedural and deployment sub models can be used to select a deployable set of services based on a set of user preferences shaping a big data campaign bdc  the deployment of a bdc requires that the selection of services has to be carried out on the basis of coherent and non conflictual user preferences in this paper we propose an owl ontology in order to solve this issue 
__label__MachineLearning __label__IntelligentSystems __label__LearningSystems __label__Education __label__Internet __label__EducationalInstitutions __label__Courseware __label__HybridIntelligentSystems __label__BayesianMethods __label__SupportVectorMachines application of machine learning techniques to webbased intelligent learning diagnosis system this work proposes an intelligent learning diagnosis system that supports a webbased thematic learning model which aims to cultivate learners ability of knowledge integration by giving the learners the opportunities to select the learning topics that they are interested and gain knowledge on the specific topics by surfing on the internet to search related learning courseware and discussing what they have learned with their colleagues based on the log files that record the learners past online learning behavior an intelligent diagnosis system is used to give appropriate learning guidance to assist the learners in improving their study behaviors and grade online class participation for the instructor the achievement of the learners final reports can also be predicted by the diagnosis system accurately our experimental results reveal that the proposed learning diagnosis system can efficiently help learners to expand their knowledge while surfing in cyberspace webbased themebased learning model 
__label__MachineLearning __label__SupportVectorMachines __label__MachineLearningAlgorithms __label__SupportVectorMachineClassification __label__SemisupervisedLearning __label__PredictiveModels __label__EducationalInstitutions __label__Petroleum __label__Costs __label__PatternRecognition a new semisupervised support vector machine learning algorithm based on active learning semisupervised support vector machine is an extension of standard support vector machine in machine learning problem in real life however the existing semisupervised support vector machine algorithm has some drawbacks such as slower training speed lower accuracy etc this paper presents a semisupervised support vector machine learning algorithm based on active learning which trains early learner by a spot of labeleddata selects the best training samples for training and learning by active learning and reduces learning cost by deleting non support vector simulative experiments have shown that the algorithm may get good learning effect at less learning cost 
__label__Training __label__MachineLearning __label__Vectors __label__Accuracy __label__Kernel __label__DataModels __label__RandomVariables deep transfer learning via restricted boltzmann machine for document classification transfer learning aims to improve a targeted learning task using other related auxiliary learning tasks and data most current transferlearning methods focus on scenarios where the auxiliary and the target learning tasks are very similar either some of the auxiliary data can be directly used as training examples for the target task or the auxiliary and the target data share the same representation however in many cases the connection between the auxiliary and the target tasks can be remote only a few features derived from the auxiliary data may be helpful for the target learning we call such scenario the deep transferlearning scenario and we introduce a novel transferlearning method for deep transfer our method uses restricted boltzmann machine to discover a set of hierarchical features from the auxiliary data we then select from these features a subset that are helpful for the target learning using a selection criterion based on the concept of kerneltarget alignment finally the target data are augmented with the selected features before training our experiment results show that this transfer method is effective it can improve classification accuracy by up to more than 10  even when the connection between the auxiliary and the target tasks is not apparent 
__label__SupportVectorMachines __label__MachineLearning __label__DataMining __label__SupportVectorMachineClassification __label__MachineLearningAlgorithms __label__RiskManagement __label__StatisticalAnalysis __label__CustomerRelationshipManagement __label__Cybernetics __label__EducationalInstitutions apply support vector machine for crm problem data mining in the crm aiming at learning available knowledge from the customer relationship by machine learning or statistical method to instruct the strategic behavior so that obtain the most profit in recent years support vector machine svms has been proposed as a power tool in machine leaning and data mining this paper applies the svms to resolve the practical crm problem in a company the final results report the good general performance of svms for crm problem 
__label__MachineLearning __label__Datamining __label__ArtificialNeuralNetworks __label__DataAnalysis __label__Databases __label__InformationSystems __label__LearningSystems __label__ClassificationtReeanalysis __label__DataVisualization __label__Hospitals __label__CasebasedReasoning __label__DataMining __label__LearningArtificialIntelligence __label__NeuralNets __label__MachineLearning __label__SubsymbolicDataMining __label__ArtificialNeuralNetwork __label__CasedBasedReasoning __label__DataMining __label__MachineLearning __label__ArtificialNeuralNetworAnn __label__CaseBasedReasoningCbr __label__HybridApproach  hybrid machine learning approach in data mining in this paper we discuss various machine learning approaches used in mining of data further we distinguish between symbolic and subsymbolic data mining methods we also attempt to propose a hybrid method with the combination of artificial neural network ann and cased based reasoning cbr in mining of data 
__label__MachineLearning __label__Drugs __label__MedicalDiagnosticImaging __label__KnowledgeBasedSystems __label__DecisionMaking __label__Accuracy __label__Semantics __label__DecisionMaking __label__DecisionSupportSystems __label__InferenceMechanisms __label__LearningArtificialIntelligence __label__MedicalInformationSystems __label__OntologiesArtificialIntelligence __label__OpenSystems __label__MedicalDecisionSupportSystem __label__MissingPatientData __label__MedicalDecisionMaking __label__OntologybasedAutomatedReasoning __label__MachineLearningTechniques __label__PatientDatasets __label__Interoperability __label__HealthInformationSystems __label__DrugdrugInteractions __label__DrugPrescriptionRules __label__MedicalProfessionals __label__MachineLearningInMedicine __label__KnowledgeRepresentationAndReasoning __label__FeatureExtractionAndClassification  integrating machine learning into a medical decision support system to address the problem of missing patient data in this paper we present a framework which enables medical decision making in the presence of partial information at its core is ontologybased automated reasoning machine learning techniques are integrated to enhance existing patient datasets in order to address the issue of missing data our approach supports interoperability between different health information systems this is clarified in a sample implementation that combines three separate datasets patient data drugdrug interactions and drug prescription rules to demonstrate the effectiveness of our algorithms in producing effective medical decisions in short we demonstrate the potential for machine learning to support a task where there is a critical need from medical professionals by coping with missing or noisy patient data and enabling the use of multiple medical datasets 
__label__SupportVectorMachines __label__SupportVectorMachineClassification __label__PatternRecognition __label__MachineLearning __label__LargescaleSystems __label__RiskManagement __label__SpaceTechnology __label__TrainingData __label__History __label__Testing __label__SupportVectorMachines __label__LearningArtificialIntelligence __label__PatternClassification __label__ComputationalComplexity __label__IncrementalSupprotVectorMachine __label__RecombiningMethod __label__PatternRecognition __label__IncrementalLearning __label__DataDistribution __label__HistoryTrainingDataset __label__ClassifierTraining __label__ParallelStructure __label__ComputationComplexity __label__SupportVectorMachine __label__IncrementalLearning __label__DatasetRecombining __label__DataDistribution  a training algorithm of incremental supprot vector machine with recombining method support vector machine svm has become a popular tool of pattern recognition in recent years for its outstanding learning performance when dealing with largescale learning problems incremental svm framework is generally used because svm can summarize the data space in a concise way this paper proposes a training algorithm of incremental svm with recombining method considering the differences of data distribution and the impact of new training data on history data the history training dataset and the new training one are divided into independent groups and are recombined to train a classifier in fact this method can be implemented in a parallel structure for the actions of dividing may decrease the computation complexity of training a svm meanwhile the actions of recombining may weaken the potential impact caused by the difference of data distribution the experiment results on a text dataset show that this training algorithm is effective and the classification accuracy of proposed incremental algorithm is superior to that using batch svm model 
__label__XrayImaging __label__Estimation __label__SupportVectorMachines __label__Arthritis __label__Training __label__Bones __label__Thumb __label__ComputerisedTomography __label__Diseases __label__LearningArtificialIntelligence __label__MedicalImageProcessing __label__PatientDiagnosis __label__PatientTreatment __label__SupportVectorMachines __label__DiagnosisSystem __label__MachineLearning __label__RheumatoidArthritis __label__HandXrayImage __label__FootXrayImage __label__FingerJointDetection __label__PatientPrognosis __label__Japan __label__ComputeraidedDiagnosisSystem __label__SupportVectorMachine __label__ProgressionEvaluation __label__ModifiedTotalSharpScore __label__RheumatoidArthritis __label__XrayImage __label__ModifiedTotalSharpScore __label__MachineLearning __label__ComputeraidedDiagnosis  computeraided diagnosis system for rheumatoid arthritis using machine learning there are 700000 rheumatoid arthritis ra patients in japan and the number of patients is increased by 30000 annually the early detection and appropriate treatment according to the progression of ra are effective to improve the patient s prognosis the modified total sharp mts score is widely used for the progression evaluation of rheumatoid arthritis the mts score assessments on hand or foot xray image is required several times a year and it takes very long time the automatic mts score calculation system is required this paper proposes the finger joint detection method and the mts score estimation method using support vector machine experimental results on 45 ra patient s xray images showed that the proposed method detects finger joints with accuracy of 814  and estimated the erosion and jsn score with accuracy of 509 643  respectively 
__label__DiscreteWaveletTransforms __label__TimeSeriesAnalysis __label__LearningSystems __label__MachineLearningAlgorithms __label__WaveletAnalysis __label__CloudComputing __label__DiscreteWaveletTransforms __label__GraphTheory __label__HaarTransforms __label__PeertopeerComputing __label__SecurityOfData __label__UnsupervisedLearning __label__InsiderThreatDetection __label__CloudFilesharingServices __label__CompanyData __label__CompanyIp __label__TwostageMachineLearningSystem __label__RelationshipGraphs __label__GraphbasedUnsupervisedMachineLearningMethods __label__Oddball __label__Pagerank __label__LocaloutlierFactor __label__Lof __label__DiscreteWaveletTransform __label__Dwt __label__HaarWaveletFunction __label__WaveletCoefficients __label__DiscreteWaveletTransform __label__HaarWavelet __label__InsiderThreatDetection __label__CloudFilesharing __label__GraphbasedUnsupervisedLearning  wavelet transform and unsupervised machine learning to detect insider threat on cloud filesharing as increasingly more enterprises are deploying cloud filesharing services this adds a new channel for potential insider threats to company data and ips in this paper we introduce a twostage machine learning system to detect anomalies in the first stage we project the access logs of cloud filesharing services onto relationship graphs and use three complementary graphbased unsupervised learning methods oddball pagerank and local outlier factor lof to generate outlier indicators in the second stage we ensemble the outlier indicators and introduce the discrete wavelet transform dwt method and propose a procedure to use wavelet coefficients with the haar wavelet function to identify outliers for insider threat the proposed system has been deployed in a real business environment and demonstrated effectiveness by selected case studies 
__label__Training __label__Accuracy __label__SupportVectorMachines __label__ClassificationAlgorithms __label__RemoteSensing __label__SemisupervisedLearning __label__MachineLearning __label__GeophysicalImageProcessing __label__ImageClassification __label__LearningArtificialIntelligence __label__RemoteSensing __label__SupportVectorMachines __label__RemoteSensingDataClassification __label__ImageClassification __label__SemisupervisedMachineLearning __label__ActiveLearning __label__TrainingSamples __label__TrainingSetInformation __label__ClassificationAccuracy __label__SupportVectorMachines __label__MachineLearning __label__SupervisedClassification __label__SupportVectorMachines __label__SemisupervisedLearning __label__ActiveLearning __label__RemoteSensing  recent trends in classification of remote sensing data active and semisupervised machine learning paradigms this paper addresses the recent trends in machine learning methods for the automatic classification of remote sensing rs images in particular we focus on two new paradigms semisupervised and active learning these two paradigms allow one to address classification problems in the critical conditions where the available labeled training samples are limited these operational conditions are very usual in rs problems due to the high cost and time associated with the collection of labeled samples semisupervised and active learning techniques allow one to enrich the initial training set information and to improve classification accuracy by exploiting unlabeled samples or requiring additional labeling phases from the user respectively the two aforementioned strategies are theoretically and experimentally analyzed considering svmbased techniques in order to highlight advantages and disadvantages of both strategies 
__label__MachineLearningAlgorithms __label__InformationFiltering __label__InformationFilters __label__WebPages __label__SupportVectorMachines __label__Internet __label__DecisionTrees __label__BayesianMethods __label__FilteringAlgorithms __label__ApplicationSoftware __label__LearningArtificialIntelligence __label__InformationFilters __label__Internet __label__DecisionTrees __label__BayesMethods __label__SupportVectorMachines __label__MachineLearningAlgorithm __label__ChineseWebFiltering __label__Internet __label__UnfilteredWebPage __label__DecisionTree __label__RuleInduction __label__BayesianAlgorithm __label__SupportVectorMachine __label__ComputerResource  comparison of machine learning algorithms in chinese web filtering web filtering based on user s demand has witnessed a booming interest due to the development of internet in the research community the dominant approach to this problem is based on machine learning algorithms web filtering is an inductive process which automatically builds a filter by learning from a set of preassigned document and the description of user s interest and then uses it to assign unfiltered web pages this survey compares four main machine learning algorithms decision tree rule induction bayesian algorithm and support vector machines on chinese web pages set of their filtering effectiveness and computer resources consumed focusing on the influence of feature set size and training set size it induces that support vector machines earn high score in chinese web filtering applications 
__label__Accuracy __label__MachineLearning __label__Inspection __label__Training __label__Cybernetics __label__Assembly __label__Indexes __label__LearningArtificialIntelligence __label__PatternClassification __label__ParallelClassifiersEnsemble __label__HierarchicalMachineLearning __label__ImbalancedDistributions __label__MachineVisionQualityInspection __label__PatternRecognition __label__ImbalancedClasses __label__HierarchicalMachineLearning __label__ParallelProcessing __label__Roc  parallel classifiers ensemble with hierarchical machine learning for imbalanced classes imbalanced distributions and misclassified costs of two classes made conventional classification methods suffered this paper proposed a new fast parallel classification method for imbalanced classes considering imbalanced distributions the approach adopted a fast simple classifier with less features input working parallel with a complicated one most samples would be correctly recognized by the first classifier and the second relatively slower classifier could be ended the second one was only trained and worked for less difficult samples experimental results in machine vision quality inspection showed that the approach could effectively improve classification speed and decrease total risk for imbalanced classespsila classification 
__label__MachineLearning __label__DataMining __label__EducationalInstitutions __label__Statistics __label__Dictionaries __label__Mathematics __label__InformationProcessing __label__NaturalLanguages __label__TextProcessing __label__Probability __label__Dictionaries __label__LearningArtificialIntelligence __label__NaturalLanguages __label__TextAnalysis __label__ChineseNewWordExtraction __label__MachineLearningApproach __label__ChineseInformationProcessing __label__ContextInformation __label__WordConstructionRules __label__Twocharacternouns __label__StatisticInformation __label__Dictionary __label__NewWordsExtraction __label__WordSegmentation __label__MachineLearning __label__WordConstructionRules  chinese new words extraction based on machine learning approach chinese new words extraction is an important problem for chinese information processing in this paper a new words extraction method based on machine learning is proposed where the context information the word construction rules and statistic information are combined to extract new words an experiment based on twocharacternouns shows that this method can well improve the efficiency and accuracy of extracting new words
__label__MultilayerNeuralNetwork __label__NeuralNetworks __label__NeuralNetworkHardware __label__ComputerArchitecture __label__Buildings __label__NetworkTopology __label__Arithmetic __label__Concrete __label__PatternClassification __label__SpeechRecognition __label__MultilayerPerceptrons __label__NeuralChips __label__FieldProgrammableGateArrays __label__NeuralNetArchitecture __label__PatternClassification __label__2dcompatibleMultilayerNeuralNetwork __label__NeuralNetworkHardwareImplementations __label__HardwareTopologies __label__NeuralArchitectures __label__FieldProgrammableNeuralArrays __label__Fpna __label__Fpga __label__VirtualNeuralLinks __label__CommunicationLinks __label__PatternClassification __label__HardwarefriendlyNeuralStructures __label__HighdimensionalRealworldApplications __label__MultibandSpeechRecognition  building a 2dcompatible multilayer neural network neural network hardware implementations have to reconcile simple hardware topologies with often complex neural architectures field programmable neural arrays fpna are defined for that their computation scheme creates numerous virtual neural links by means of a limited set of communication links whatever the device the arithmetic and the neural structure their concrete use has proved that they allow to have the computation power of standard neural models with a reduced set of neural resources easy to map directly to digital hardware a simple pattern classification problem is chosen in this paper so as to show how fpna allow to replace complex standard neural architectures by hardwarefriendly neural structures fpna have been applied to numerous other problems with similar benefits they are now applied to highdimensional realworld applications such as multiband speech recognition 
__label__ArtificialNeuralNetworks __label__LiverDiseases __label__NeuralNetworks __label__BackpropagationAlgorithms __label__FeedforwardNeuralNetworks __label__Proteins __label__MedicalDiagnosis __label__MedicalDiagnosticImaging __label__Databases __label__ElectronicMail __label__RadialBasisFunctionNetworks __label__Liver __label__Backpropagation __label__NeuralNetArchitecture __label__PatientDiagnosis __label__Diseases __label__MultilayerPerceptrons __label__SamplingMethods __label__MedicalDiagnosticComputing __label__ArtificialNeuralNetworks __label__OrdinaryLeastSquaresAlgorithm __label__HepatitisDiseaseDiagnosis __label__MedicalDiagnostics __label__NeuralNetworkArchitectures __label__StandardFeedforwardNetworks __label__HybridNetwork __label__MultilayerPerceptronStructure __label__StandardBackpropagation __label__RadialBasisFunctionNetworkStructure __label__OlsAlgorithm __label__ConicSectionFunctionNeuralNetwork __label__AdaptiveLearning  artificial neural networks for diagnosis of hepatitis disease recently neural networks have become a very important method in the field of medical diagnostics the objective of this work is to diagnose hepatitis disease by using different neural network architectures standard feedforward networks and a hybrid network were investigated results obtained show that especially the hybrid network can be successfully used for diagnosing of hepatitis 
__label__DecisionTrees __label__MachineLearning __label__MatrixDecomposition __label__DataPreprocessing __label__DataMining __label__LearningSystems __label__Bismuth __label__BooleanFunctions __label__DataHandling __label__DecisionTrees __label__FormalConceptAnalysis __label__LearningArtificialIntelligence __label__BooleanFactorAnalysis __label__MachineLearning __label__InputDataPreprocessingMethods __label__InputDataTable __label__FormalConceptAnalysis __label__FactorConcepts __label__DecisionTreeInductionAlgorithmsId3 __label__DecisionTreeInductionAlgorithmsC45 __label__DataPreprocessing __label__MachineLearning __label__DecisionTrees __label__MatrixDecomposition __label__FormalConcept  boolean factor analysis for data preprocessing in machine learning we present two input data preprocessing methods for machine learning ml  the first one consists in extending the set of attributes describing objects in input data table by new attributes and the second one consists in replacing the attributes by new attributes the methods utilize formal concept analysis fca and boolean factor analysis recently described by fca in that the new attributes are defined by socalled factor concepts computed from input data table the methods are demonstrated on decision tree induction the experimental evaluation and comparison of performance of decision trees induced from original and preprocessed input data is performed with standard decision tree induction algorithms id3 and c45 on several benchmark datasets 
__label__OpticalFiberNetworks __label__NeuralNetworks __label__WavelengthRouting __label__WavelengthDivisionMultiplexing __label__HopfieldNeuralNetworks __label__OpticalFiberCommunication __label__CommunicationNetworks __label__HighSpeedOpticalTechniques __label__OpticalComputing __label__WdmNetworks __label__HopfieldNeuralNets __label__OpticalFibreNetworks __label__TelecommunicationNetworkRouting __label__WavelengthDivisionMultiplexing __label__OpticalNetworks __label__WavelengthDivisionMultiplexing __label__RoutingAlgorithm __label__HopfieldNeuralNetwork __label__OpticalNetworks __label__WavelengthDivisionMultiplexing __label__Routing __label__HopfieldNeuralNetwork  routing in optical networks by using neural network routing in optical especially wavelength division multiplexing networks is very hard task this paper defines a new routing algorithm based on hopfield neural network it is improvement of previous research now applied to optical communications
__label__BiologicalNeuralNetworks __label__BiologicalSystemModeling __label__Neuroscience __label__Hippocampus __label__HopfieldNeuralNetworks __label__BrainModeling __label__KineticTheory __label__Equations __label__Codes __label__OpticalFiberTheory __label__BrainModels __label__Neurophysiology __label__HopfieldNeuralNets __label__DualCodes __label__InformationStorageMechanism __label__BiologicalNeuralNetwork __label__Neuroscience __label__Hippocampus __label__CoreMemory __label__Brain __label__HopfieldNeuralNetwork __label__ElectronicNeuronicModel __label__HippocampalLtpProcess __label__PostSynapticPotentials __label__SynapticsynapticInteractionEquations __label__DualCodingTheory __label__MossyFiberSynapticGlomerulus __label__GeometricalMapping __label__MultidimensionalUnitron __label__ComputationEnergyDistribution __label__DynamicLoci __label__MemoryRetrievalProcess  information storage mechanism of the biological neural network the information storage mechanism of the biological neural network is the most important problem in neuroscience our observations showed that the structure of the hippocampus the core memory of the brain is very similar to the hopfield s neural network an electronic neuronic model was constructed to simulate the dynamic process of the hippocampal ltp process the kinetic process of the post synaptic potentials and the synapticsynaptic interaction equations were also discussed we proposed a dual coding theory of biological neural information and assumed that the messy fiber synaptic glomerulus may be the chief storage medium of the neural information a geometrical mapping of n dimensional unitron was constructed to analyse the distribution of the computation energy and to trace the dynamic loci in memory retrieve process 
__label__Chaos __label__NeuralNetworks __label__ArtificialNeuralNetworks __label__ChaoticCommunication __label__ComputerSimulation __label__ComputationalModeling __label__FeedforwardNeuralNetworks __label__RecurrentNeuralNetworks __label__Limitcycles __label__Logistics __label__FeedforwardNeuralNets __label__Chaos __label__LearningArtificialIntelligence __label__TimeSeries __label__Chaos __label__ChaoticTimeSeries __label__FeedforwardNeuralNetwork __label__TemporalSeries __label__DynamicalLearningProcess __label__TriangularMaps __label__CriticalTime __label__OrderParameterCoherency  dynamical process of learning chaotic time series by neural networks we report the result of computer simulations on the learning process of temporal series by artificial neural networks in our simulation we used a feedforward neural network model with 4layers to study the capability and dynamical learning process of chaotic time series produced by triangular maps we found a critical time tsub cr at which the learning process proceeds abruptly we also found that the critical time tsub cr is shorter the larger the initial deviation from the target of learning we provide detailed discussion about the learning process to explain these interesting phenomena and a new order parameter coherency is introduced to characterize these processes 
__label__NeuralNetworks __label__FuzzyNeuralNetworks __label__MultilayerNeuralNetwork __label__FeedforwardNeuralNetworks __label__Neurofeedback __label__DecisionSupportSystems __label__ExpertSystems __label__FuzzySets __label__PowerGenerationEconomics __label__SystemsEngineeringAndTheory __label__FeedforwardNeuralNets __label__MultilayerPerceptrons __label__Feedback __label__LearningArtificialIntelligence __label__FuzzyLogic __label__Identification __label__FuzzyReferencePoints __label__HiddenUnits __label__HybridMultilayerFeedforwardNeuralNetwork __label__GeneralFeedbackLearningMethod __label__Dss __label__ProcessIdentification __label__ExpertSystems  a hybrid neural network using fuzzy reference points as hidden units in this paper a hybrid multilayer feedforward neural network in which hidden units are defined as fuzzy reference points and a general feedback learning method are presented this model has been used in a dss results of an experiment on process identification are given the proposed model is able to overcome some limitations of conventional multilayer feedforward neural networks applied in expert systems 
__label__IntelligentNetworks __label__NeuralNetworks __label__BiologicalNeuralNetworks __label__Equations __label__Tellurium __label__RecurrentNeuralNetworks __label__NeuralNetworkHardware __label__Neurons __label__IdentitybasedEncryption __label__StateEstimation __label__RecurrentNeuralNets __label__CorrelationTheory __label__ContentaddressableStorage __label__WeightingFunction __label__RecurrentCorrelationNeuralNetworks __label__HighcapacityAssociativeMemory  weighting function in neural network the recurrent correlation neural networks have highcapacity associative memory when the weighting function satisfies certain condition but this always causes the high dynamics in the neural network and the hardware realization is difficult this paper gives the relationship between the capacity and dynamics and provides a general principle for the choice of the weighting function and give a kind of weighting function it has highcapacity and avoids the high dynamics finally the simulated results are given 
__label__GeneticProgramming __label__CellularNeuralNetworks __label__NeuralNetworks __label__ArtificialNeuralNetworks __label__GeneticAlgorithms __label__Embryo __label__AutomaticControl __label__BiologicalCells __label__BiologicalControlSystems __label__Robots __label__GeneticAlgorithms __label__CellularAutomata __label__Programming __label__CellularNeuralNets __label__NeuriteNetworks __label__GeneticProgramming __label__CellularAutomata __label__NeuralNets __label__GeneticAlgorithms __label__Gennets __label__DarwinMachines  neurite networks the genetic programming of cellular automata based neural nets which grow this paper proposes a new branch of neural networks called neurite networks  it is a neural network that grows ie it has an embryological component the artificial neurite network introduced is based on a cellular automata ca network whose branchings are genetically programmed ie they are grown under the control of a genetic algorithm  a sequence of ca signals is sent down the middle of a ca trail  when a signal hits the end of a trail it can make the trail extend turn left turn right branch left branch right split etc depending upon the state of the ca signal these signal sequences are treated as the chromosomes of a genetic algorithm once the ca network is formed a second set of ca state transition rules is switched on to make it behave like a neural network the fitness of this ca based neural network is measured in terms of how well it controls some behavior of a biological robot 
__label__NeuralNetworks __label__Testing __label__Filtering __label__MatchedFilters __label__UpperBound __label__Feeds __label__FeedforwardNeuralNetworks __label__PatternRecognition __label__EnergyStates __label__PatternMatching __label__LearningArtificialIntelligence __label__GeneralisationArtificialIntelligence __label__FeedforwardNeuralNets __label__PatternClassification __label__SpeechRecognition __label__FilteringTheory __label__FilterNeuralNetwork __label__DotProductMatchingNeuralNetwork __label__TestPattern __label__SupervisedFastLearningFilterNeuralNetwork __label__GeneralisationCapability __label__SpeakerindependentSpokenNumberRecognition __label__RecognitionRate  a filter neural network this paper proposes to add a filter layer to a dot product matching neural network the purpose of the filter layer is to discard those unfavourable choices by checking the lower and upper bounds of each exemplar with the test pattern the product is a supervised fast learning filter neural network it has a better generalisation capability than an ordinary dot product matching neural network the new neural network is tested for speakerindependent spoken number in english recognition an accuracy of 965 is reported for the test data without the filter layer the recognition rate falls to 940 
__label__NeuralNetworks __label__MultilayerNeuralNetwork __label__ComputerArchitecture __label__VeryLargeScaleIntegration __label__SignalProcessing __label__Neurons __label__HighPerformanceComputing __label__Kernel __label__SignalProcessingAlgorithms __label__ApplicationSoftware __label__NeuralNetArchitecture __label__ParallelArchitectures __label__Transputers __label__NeuralChips __label__Vlsi __label__FeedforwardNeuralNets __label__DigitalArchitecture __label__Neuroboard __label__Transputers __label__DedicatedVlsi __label__DigitalNeurochip __label__MultilayerNeuralNetworks __label__ProgrammableBitsWeights __label__Neurochips __label__Neurocomputer  digital architecture for neural networks discussing a new architecture for high performance computers based on transputers and dedicated vlsi the author proposes a new general purpose custommade digital neurochip for flexible implementation multilayer neural networks with programmable bits weights and inputs signals the neurotram consists of transputers memory and neurochips for neurocomputer implementation 
__label__OptimizationMethods __label__NeuralNetworks __label__BiologicalNeuralNetworks __label__ApplicationSoftware __label__GeneticEngineering __label__Laboratories __label__OutputFeedback __label__Neurofeedback __label__FeedforwardNeuralNetworks __label__LearningSystems __label__NeuralNets __label__GeneticAlgorithms __label__ComputerGames __label__GeneticMethod __label__AsynchronousRandomNeuralNetworks __label__ActionControl __label__AsynchronousThresholdingNeuralUnits __label__InputUnits __label__HiddenUnits __label__OutputUnits __label__Feedforward __label__Feedback __label__MutualConnections __label__VirtualLivingThings __label__GenerationIteration __label__ComputerGame  a genetic method for optimization of asynchronous random neural networks and its application to action control a genetic method is proposed to optimize random neural networks composed of asynchronous thresholding neural units each unit belongs to one of three categories input units hidden units and output units and any kinds of connections among units including feedforward feedback and mutual connections are allowable except connections to input units several virtual living things whose genotype are the connections among neural units are randomly generated and generation iteration is repeated in order to optimize them in the generation iteration individuals adequate to a given problem make their children and inferior ones are removed from the population optimized neural networks are obtained as evolved individuals an action control problem for a computer game is treated as an application of this method 
__label__RoadAccidents __label__ClassificationAlgorithms __label__DataMining __label__DataModels __label__AlgorithmDesignAndAnalysis __label__MachineLearningAlgorithms __label__DataMining __label__LearningArtificialIntelligence __label__PatternClassification __label__RiskAnalysis __label__RoadAccidents __label__RoadTraffic __label__TrafficEngineeringComputing __label__RoadAccidentOccuranceStatusAnalysis __label__AccidentRiskDetermination __label__Istanbul __label__RoadTraffic __label__PublicHealth __label__CountryEconomy __label__CalibratedDataAgglomerations __label__DataStorage __label__DataMining __label__MachineLearningClassificationTechniques __label__MachineLearning __label__DataMining __label__ClassificationTechniques __label__RoadAccident  analysis for status of the road accident occurance and determination of the risk of accident by machine learning in istanbul the traffic has been transformed into the difficult structure in points of designing and managing by the reason of increasing number of vehicle this situation has discovered road accidents problem influenced public health and country economy and done the studies on solution of the problem large calibrated data agglomerations have increased by the reasons of the technological improvements and data storage with low cost arising the need of accession to information from this large calibrated data obtained the corner stone of the data mining in this study assignment of the most compatible machine learning classification techniques for road accidents estimation by data mining has been intended 
__label__MachineLearning __label__SupportVectorMachines __label__TrainingData __label__Neurons __label__MachineLearningAlgorithms __label__Organizations __label__Security __label__InferenceMechanisms __label__LearningArtificialIntelligence __label__PatternClassification __label__SecurityOfData __label__TextAnalysis __label__MachineLearningClassifier __label__DeepLearning __label__ExploratoryMachineLearningAttack __label__FunctionallyEquivalentMachine __label__TrainingData __label__Crowdsourcing __label__DomainspecificKnowledge __label__HyperparameterOptimization __label__ClassifierStructure __label__BlackboxAttackApproach __label__TextClassificationApplication __label__NaiveBayesClassifier __label__SvmClassifierInference __label__SecurityChallenges __label__OnlineMachineLearningAlgorithms __label__MitigationStrategies __label__HighFidelityInferenceCapability __label__MachineLearning __label__AdversarialMachineLearning __label__Classifier __label__DeepLearning __label__ExploratoryAttacks  how to steal a machine learning classifier with deep learning this paper presents an exploratory machine learning attack based on deep learning to infer the functionality of an arbitrary classifier by polling it as a black box and using returned labels to build a functionally equivalent machine typically it is costly and time consuming to build a classifier because this requires collecting training data eg through crowdsourcing  selecting a suitable machine learning algorithm through extensive tests and using domainspecific knowledge  and optimizing the underlying hyperparameters applying a good understanding of the classifier s structure  in addition all this information is typically proprietary and should be protected with the proposed blackbox attack approach an adversary can use deep learning to reliably infer the necessary information by using labels previously obtained from the classifier under attack and build a functionally equivalent machine learning classifier without knowing the type structure or underlying parameters of the original classifier results for a text classification application demonstrate that deep learning can infer naive bayes and svm classifiers with high accuracy and steal their functionalities this new attack paradigm with deep learning introduces additional security challenges for online machine learning algorithms and raises the need for novel mitigation strategies to counteract the high fidelity inference capability of deep learning 
__label__NeuralNetworks __label__BrainModeling __label__BiologicalNeuralNetworks __label__Neurons __label__Cognition __label__Chemicals __label__AdaptiveFilters __label__AdaptiveControl __label__BasalGanglia __label__Circuits __label__BrainModels __label__NeuralNets __label__Psychology __label__NeuralNetworkModels __label__Brain __label__LargescaledNeuralSystems __label__MajorBrainFunctions __label__BasalGanglia __label__HippocampalCircuit __label__CognitiveMemory __label__Learning __label__NeocorticalNetwork __label__Language __label__EmotionalSystems  what can we expect from neural network models summary form only given in each area of the brain numerous neurons constitute elaborate networks these networks are linked through numerous connections and compose largescaled neural systems the neural systems generate major brain functions such as movement cognition emotion and memorylearning the brain as such has extensively been studied anatomically physiologically and chemically and our knowledge ever grows to cover every details of the brain important principles such as activitydependent synaptic plasticity multilayered integration of neuronal networks modular organization of brain tissues have been revealed yet shortage of knowledge is obvious when one tries to reproduce brain functions with models while the simple perceptron model adaptive filter model feedforward adaptive control system model have successfully reproduced functions of the cerebellum efforts to model other parts of the brain have met greater difficulties it is still difficult to answer fundamental questions such as what is meant by the intricate structure of the basal ganglia how the hippocampal circuit serves for cognitive memory and learning how language is encoded within the neocortical network structures of motor cognitive and emotional systems in the brain have been dissected to some extent but it is yet difficult to figure out where and how our volition emerges how we sense beauty truth and virtue intuitively and where in our brain consciousness resides complete understanding of the brain could be achieved only when one successfully models neural networks and systems so as to reproduce entire aspects of brain functions 
__label__NeuralNetworkHardware __label__NeuralNetworks __label__FieldProgrammableGateArrays __label__Neurons __label__ComputerArchitecture __label__ConcurrentComputing __label__MulticastProtocols __label__CommunicationStandards __label__ComputerScience __label__SoftwarePrototyping __label__NeuralNetArchitecture __label__FieldProgrammableGateArrays __label__NeuralChips __label__DigitalIntegratedCircuits __label__FeedforwardNeuralNets __label__RecurrentNeuralNets __label__DigitalHardwareImplementation __label__2dCompatibleNeuralNetworks __label__NeuralArchitectures __label__Fpga __label__DataExchangeScheme __label__FieldProgrammableNeuralArrays __label__Fpna __label__HardwareTopologicalConstraints __label__Fpnn __label__FieldProgrammedNeuralNetworks __label__LimitedInterconnectionScheme __label__FeedforwardNeuralNets __label__RecurrentNeuralNets __label__AsynchronousBlocks  digital hardware implementation of 2d compatible neural networks the work described in this paper aims at developing neural architectures that are easy to map onto fpga thanks to a simplified topology and an original data exchange scheme without significant loss of approximation capability it has been achieved thanks to the definition of a set of neural models called field programmable neural arrays fpna  fpna may lead to the definition of neural networks adapted to hardware topological constraints different such neural networks may be derived from a given fpna they are called field programmed neural networks fpnn  they reconcile the high connection density of neural architectures with the need of a limited interconnection scheme in hardware implementations this paper focuses on the definition and implementation of fpnn parallel computation it briefly defines the fpnafpnn concept it introduces a parallel form of fpnn computation for feedforward and recurrent fpnn it describes a fpgabased modular implementation based on asynchronous blocks a few results of fpnn applications are briefly discussed 
__label__NeuralNetworks __label__InferenceMechanisms __label__LogicProgramming __label__HopfieldNeuralNetworks __label__Neurons __label__ParallelProcessing __label__InferenceAlgorithms __label__Engines __label__ArtificialNeuralNetworks __label__Prototypes __label__LogicProgrammingLanguages __label__InferenceMechanisms __label__HopfieldNeuralNets __label__RelaxationTheory __label__NeuralNetworkApproach __label__InferenceMechanism __label__LogicProgrammingLanguage __label__FinegrainParallelComputing __label__HopfieldtypeNeuralNetwork __label__RelaxationTechniques  a neural network approach to inference mechanism for logic programming language presents an inference mechanism for logic programming languages using neural networks that is flexible and suited for finegrain parallel computing the authors approach is radically different from the conventional methods based on refutation processes programs written in the logic programming language are transformed into a hopfieldtype neural network and relaxation techniques are applied to this network to inference solutions the authors propose an algorithm to transform logic programs into hopfieldtype neural networks and implement a prototype of the inference system based on this mechanism the authors tested the system with some preliminary problems preliminary results confirm that the algorithm is correct 
__label__Capacitors __label__NeuralNetworks __label__AnalogComputers __label__Circuits __label__ParasiticCapacitance __label__ComputerArchitecture __label__EnergyConsumption __label__ComputerNetworks __label__ConcurrentComputing __label__LargescaleSystems __label__Eprom __label__AnalogueStorage __label__AnalogueProcessingCircuits __label__NeuralChips __label__NeuralNetArchitecture __label__MosCapacitors __label__SwitchedCapacitorNetworks __label__AnalogueComputerCircuits __label__FlashbasedCapacitor __label__ProgrammableNonlinearCapacitor __label__SwitchedcapacitorImplementations __label__ScImplementation __label__NeuralNetworks __label__ChargeDomainCharacteristics __label__AnalogStorage __label__AnalogComputation __label__Capflash  flashbased programmable nonlinear capacitor for switchedcapacitor implementations of neural networks the use of flash devices for both analog storage and analog computation can result in highly efficient switchedcapacitor implementations of neural networks the standard flash device suffers from severe limitations in this application due to relatively large parasitic overlap capacitances this paper introduces the computational concept circuit and architecture we are exploring as well as a novel flashbased programmable nonlinear capacitor with much improved charge domain characteristics for our application these devices are demonstrated in a novel circuit consisting of only two devices and capable of computing a 5bit absolutevalueofdifference at an energy consumption of less than 1 pj 
__label__MathematicalModel __label__MachineLearning __label__PredictionAlgorithms __label__PredictiveModels __label__Vegetation __label__ClassificationAlgorithms __label__DataModels __label__LearningArtificialIntelligence __label__ArtificialIntelligenceFunction __label__RandomForest __label__HtmCorticalLearningAlgorithm __label__HighPerformance __label__DeepLearningPerformance __label__RecordProcessing __label__SelectionConstruction __label__Rfhtmc __label__MeanAbsolutePercentageError __label__DeepLearning __label__RandomForestAlgorithm __label__HtmAlgorithm __label__MeanAbsolutePercentageError __label__DutyCycle  improving deep learning performance using random forest htm cortical learning algorithm deep learning is an artificial intelligence function that imitates the mechanisms of the human mind in processing records and developing shapes to be used in selection construction the objective of the paper is to improve the performance of the deep learning using a proposed algorithm called rfhtmc this proposed algorithm is a merged version from random forest and htm cortical learning algorithm the methodology for improving the performance of deep learning depends on the concept of minimizing the mean absolute percentage error which is an indication of the high performance of the forecast procedure in addition to the overlap duty cycle which its high percentage is an indication of the speed of the processing operation of the classifier the outcomes depict that the proposed set of rules reduces the absolute percent errors by using half of the value and increase the percentage of the overlap duty cycle with 15 
__label__HandheldComputers __label__CognitiveInformatics __label__Software __label__Mathematics __label__Algebra __label__CognitiveSystems __label__InferenceMechanisms __label__IntelligentRobots __label__KnowledgeAcquisition __label__LearningArtificialIntelligence __label__NeuralNets __label__CognitiveFoundations __label__KnowledgeScience __label__DeepKnowledgeLearning __label__CognitiveRobots __label__FundamentalAiProblems __label__NaturalIntelligence __label__MachineUnderstandableForms __label__CognitiveProcess __label__BehaviorAcquisition __label__ObjectIdentification __label__ClusterClassification __label__BehaviorGeneration __label__KnowledgeAcquisition __label__BinaryRelation __label__NeuralNetworkTechnologies __label__CognitiveMachineLearning __label__DenotationalMathematics __label__MathematicalEngineering __label__FormalBrainStudies __label__CognitiveSystems __label__DeepReasoning __label__DeepLearning __label__MachinableThoughts __label__CognitiveKnowledgeBases __label__FundamentalTheories __label__DeepThinkingRobots __label__CognitiveInformatics __label__CognitiveComputers __label__CognitiveRobotics __label__BraininspiredSystems __label__DeepLearning __label__DeepReasoning __label__DeepThinking __label__KnowledgeLearning __label__DenotationalMathematics __label__MathematicalEngineering __label__SemanticComputing __label__CognitiveLinguistics __label__Applications  cognitive foundations of knowledge science and deep knowledge learning by cognitive robots recent basic studies reveal that novel solutions to fundamental ai problems are deeply rooted in both the understanding of the natural intelligence and the maturity of suitable mathematical means for rigorously modeling the brain in machine understandable forms learning is a cognitive process of knowledge and behavior acquisition learning can be classified into five categories known as object identification cluster classification functional regression behavior generation and knowledge acquisition the latest discovery in knowledge science by wang revealed that the basic unit of knowledge is a binary relation bir as that of bit for information and data a fundamental challenge to knowledge learning different from those of deep and recurring neural network technologies has led to the emergence of the field of cognitive machine learning on the basis of recent breakthroughs in denotational mathematics and mathematical engineering this keynote lecture presents latest advances in formal brain studies and cognitive systems for deep reasoning and deep learning it is recognized that key technologies enabling cognitive robots mimicking the brain rely not only on deep learning but also on deep reasoning and thinking towards machinable thoughts and cognitive knowledge bases built by cognitive systems fundamental theories and novel technologies for implementing deep thinking robots are demonstrated based on concept algebra semantics algebra and inference algebra 
__label__MachineLearning __label__LearningArtificialIntelligence __label__Training __label__GraphicsProcessingUnits __label__NeuralNetworks __label__SoftwareAlgorithms __label__DynamicProgramming __label__GraphicsProcessingUnits __label__LearningArtificialIntelligence __label__NeuralNets __label__NumericalAnalysis __label__PublicDomainSoftware __label__OpenSourceMachineLearning __label__DistributedEnvironment __label__OpenaiGym __label__GraphicsProcessingUnit __label__HardwareResourceUtilization __label__DistributedTensorfiow __label__Architecture __label__Gpu __label__NumericalComputationLibrary __label__Hyperparameters __label__QlearningLossValue __label__Training __label__DeepConvolutionalNeuralNetwork __label__DeepQlearningNetworkAlgorithm __label__DeepReinforcementLearning __label__Tensorflow __label__DeepQnetworks __label__DeepQlearning __label__ArtificialGeneralizedIntelligence  distributed deep reinforcement learning using tensorflow deep reinforcement learning is the combination of reinforcement learning algorithms with deep neural network which had recent success in learning complicated unknown environments the trained model is a convolutional neural network trained using qlearning loss value the agent takes in observation ie raw pixel image and reward from the environment for each step as input the deep qlearning algorithm gives out the optimal action for every observation and reward pair the hyperparameters of deep qnetwork remain unchanged for any environment tensorfiow an open source machine learning and numerical computation library is used to implement the deep qlearning algorithm on gpu the distributed tensorfiow architecture is used to maximize the hardware resource utilization and reduce the training time the usage of graphics processing unit gpu in the distributed environment accelerated the training of deep qnetwork on implementing the deep qlearning algorithm for many environments from openai gym the agent outperforms a decent human reference player with few days of training 
__label__MachineLearning __label__Proposals __label__Visualization __label__FeatureExtraction __label__NoiseMeasurement __label__NeuralNetworks __label__SupervisedLearning __label__ImageClassification __label__LearningArtificialIntelligence __label__DeepMultipleInstanceLearning __label__Mil __label__ImageClassification __label__ImageAnnotation __label__SupervisedDeepLearningFramework  deep multiple instance learning for image classification and autoannotation the recent development in learning deep representations has demonstrated its wide applications in traditional vision tasks like classification and detection however there has been little investigation on how we could build up a deep learning framework in a weakly supervised setting in this paper we attempt to model deep learning in a weakly supervised learning multiple instance learning framework in our setting each image follows a dual multiinstance assumption where its object proposals and possible text annotations can be regarded as two instance sets we thus design effective systems to exploit the mil property with deep learning strategies from the two ends we also try to jointly learn the relationship between object and annotation proposals we conduct extensive experiments and prove that our weakly supervised deep learning framework not only achieves convincing performance in vision tasks including classification and image annotation but also extracts reasonable regionkeyword pairs with little supervision on both widely used benchmarks like pascal voc and mit indoor scene 67 and also a dataset for imageand patchlevel annotations 
__label__Training __label__MachineLearning __label__ComputerArchitecture __label__UnsupervisedLearning __label__NeuralNetworks __label__SupervisedLearning __label__ConvolutionalCodes __label__ComputerVision __label__FeatureExtraction __label__ImageClassification __label__ImageRepresentation __label__LearningArtificialIntelligence __label__NeuralNets __label__ObjectDetection __label__ImagenetDataset __label__HierarchicalFeatureRepresentation __label__SceneUnderstanding __label__FaceRecognition __label__SignRecognition __label__TextRecognition __label__ObjectClassification __label__ObjectDetection __label__ComputerVision __label__Dcnn __label__UnsupervisedFeatureLearning __label__DeepConvolutionalNeuralNetwork __label__DeepLearning __label__ConvolutionalNeuralNetwork __label__DeepConvolutionalBeliefNetwork __label__UnsupervisedDeepLearning __label__SupervisedDeepLearning  improving deep convolutional neural networks with unsupervised feature learning the latest generation of deep convolutional neural networks dcnn have dramatically advanced challenging computer vision tasks especially in object detection and object classification achieving stateoftheart performance in several computer vision tasks including text recognition sign recognition face recognition and scene understanding the depth of these supervised networks has enabled learning deeper and hierarchical representation of features in parallel unsupervised deep learning such as convolutional deep belief network cdbn has also achieved stateoftheart in many computer vision tasks however there is very limited research on jointly exploiting the strength of these two approaches in this paper we investigate the learning capability of both methods we compare the output of individual layers and show that many learnt filters and outputs of the corresponding level layer are almost similar for both approaches stacking the dcnn on top of unsupervised layers or replacing layers in the dcnn with the corresponding learnt layers in the cdbn can improve the recognitionclassification accuracy and training computational expense we demonstrate the validity of the proposal on imagenet dataset 
__label__Dictionaries __label__Training __label__MachineLearning __label__NoiseMeasurement __label__Robustness __label__BenchmarkTesting __label__NoiseReduction __label__Diseases __label__MedicalComputing __label__PatternClassification __label__UnsupervisedLearning __label__NoisyDeepDictionaryLearning __label__AlzheimersDiseaseClassification __label__UnsupervisedDeepLearningApproach __label__StackedDenoisingAutoencoders __label__RobustDeepModels __label__CleanData __label__DeepDictionaryLearningFramework __label__BenchmarkDeepLearningDatasets __label__AdClassification __label__DictionaryLearning __label__DeepLearning __label__AdClassification  noisy deep dictionary learning application to alzheimer s disease classification a recent work introduced the concept of deep dictionary learning in deep dictionary learning the first level proceeds like standard dictionary learning in subsequent layers the scaled output coefficients from the previous layer are used as inputs for dictionary learning this is an unsupervised deep learning approach the features from the final deepest layer are employed for subsequent analysis and classification the seminal paper of stacked denoising autoencoders have shown that robust deep models can be learnt when noisy data is used for training stacked autoencoders instead of clean data we adopt this idea into the deep dictionary learning framework instead of using only clean data we augment the training dataset by adding noise this improves robustness experimental evaluation on benchmark deep learning datasets and real world problem of ad classification show that our proposal yields considerable improvement 
__label__CognitiveSystems __label__HumanoidRobots __label__InferenceMechanisms __label__KnowledgeAcquisition __label__KnowledgeBasedSystems __label__LearningArtificialIntelligence __label__DeepReasoning __label__DeepLearning __label__CognitiveRobots __label__BraininspiredSystems __label__NaturalIntelligence __label__CognitiveProcess __label__KnowledgeAcquisition __label__BehaviorAcquisition __label__ObjectIdentification __label__ClusterClassification __label__FunctionalRegression __label__BehaviorGeneration __label__KnowledgeLearning __label__CognitiveMachineLearning __label__DenotationalMathematics __label__MathematicalEngineering __label__FormalBrainStudies __label__CognitiveSystems __label__CognitiveKnowledgeBases __label__ConceptAlgebra __label__SemanticsAlgebra __label__InferenceAlgebra __label__CognitiveInformatics __label__CognitiveComputers __label__CognitiveRobotics __label__BraininspiredSystems __label__DeepLearning __label__DeepReasoning __label__DeepThinking __label__KnowledgeLearning __label__DenotationalMathematics __label__MathematicalEngineering  deep reasoning and thinking beyond deep learning by cognitive robots and braininspired systems summary form only given recent basic studies reveal that ai problems are deeply rooted in both the understanding of the natural intelligence and the adoption of suitable mathematical means for rigorously modeling the brain in machine understandable forms learning is a cognitive process of knowledge and behavior acquisition learning can be classified into five categories known as object identification cluster classification functional regression behavior generation and knowledge acquisition a fundamental challenge to knowledge learning different from the deep and recurring neural network technologies has led to the emergence of the field of cognitive machine learning on the basis of recent breakthroughs in denotational mathematics and mathematical engineering this keynote lecture presents latest advances in formal brain studies and cognitive systems for deep reasoning and deep learning it is recognized that key technologies enabling cognitive robots mimicking the brain rely not only on deep learning but also on deep reasoning and thinking towards machinable thoughts and cognitive knowledge bases built by a cognitive systems a fundamental theory and novel technology for implementing deep thinking robots are demonstrated based on concept algebra semantics algebra and inference algebra 
__label__EconomicIndicators __label__BeliefNetworks __label__BoltzmannMachines __label__RecommenderSystems __label__UnsupervisedLearning __label__ProspectiveSystems __label__DeepLearningNetworks __label__UnsupervisedLearning __label__DeepLearningMethod __label__RecommenderSystemProblem __label__PythonbasedDeepLearningLibrary __label__DeepLearningBasedRecommenderSystems __label__DeepBeliefNetwork __label__Dbn __label__Rbm __label__RestrictedBoltzmanMachine __label__Keras __label__Time200Year __label__RecommenderSystems __label__DeepLearning __label__GradientBasedLearningAlgorithms __label__Adam __label__Keras  deep learning based recommender systems in parallel with the rapid development of prospective systems in the last 20 years many methods have been applied to this field one of them is the deep learning networks that have attracted the interest of researchers in recent years the dbn deep belief network  which trains one layer at a time greedily uses unsupervised learning for each layer and is composed of rbms restricted boltzman machine  has become a turning point in this area in this study the deep learning method is applied to the recommender system problem the pythonbased deep learning library keras is used and the existing learning algorithms are compared 
__label__RecurrentNeuralNetworks __label__MachineLearning __label__LearningArtificialIntelligence __label__BigData __label__Finance __label__ArtificialNeuralNetworks __label__BigData __label__LearningArtificialIntelligence __label__RiskManagement __label__StockMarkets __label__UnlabeledunsupervisedData __label__HierarchicalDeepLearningModels __label__RiskManagement __label__StockMarketForecasting __label__StockMarketPredictionProblem __label__BigData __label__Portfolios __label__MachineLearning __label__FinancialMarket __label__DeepLearning __label__MachineLearning __label__BigData __label__ArtificialIntelligence __label__Finance __label__MarketPrediction  predicting financial market in big data deep learning deep learning is appealing for learning from large amounts of unlabeledunsupervised data making it attractive for extracting meaningful representations and patterns from big data deep learning by its simplest definition is expressed as the application of machine learning methods to the big data in this study it was investigated how to apply hierarchical deep learning models for the problems in finance such as prediction and classification the design and pricing of securities construction of portfolios risk management and stock market forecasting are some of important prediction problems in finance these kind of problems include large data sets with complex relationship among data and events it is very difficult or sometimes impossible to represent these complex relationships in a full economic model deep learning methods by representing complex relationships among data allows the production of more useful results than standard methods in finance in this study we introduced and applied deep learning methods to stock market prediction problem and obtained successful results 
__label__Kernel __label__FeatureExtraction __label__MachineLearning __label__Training __label__SupportVectorMachines __label__ClassificationAlgorithms __label__MachineLearningAlgorithms __label__DataAnalysis __label__LearningArtificialIntelligence __label__NeuralNets __label__IntelligentDataAnalysis __label__MachineLearning __label__IntelligentAlgorithms __label__NetworkStructures __label__TargetData __label__DataSpace __label__DeepNeuralNetwork __label__DeepCoreLearning __label__FeatureSpace __label__KernelbasedDeepLearning __label__DepthKernelLearning __label__KernelMethod __label__DeepNeuralNetwork __label__DeepKernelLearning  kernelbased deep learning for intelligent data analysis machine learning especially neural networks has attracted more and more attention in the past few decades with the further research of intelligent algorithms and network structures machine learning has been widely used in data mining computer vision data recognition and classification because the target data is nonlinear and complex the research needs to extract accurate feature space from the data space this process relies on machine learning to perform better because manual rules do not achieve the most efficient functions the researchers combine the kernel approach with the deep neural network to maintain their advantages and compensate for their defects and then apply depth kernel learning to improve the performance of the algorithm in this paper we present an overview of the progress and applications of deep core learning we introduce the basic theory and their fusion to form several deep core learning structures to improve the performance and performance of the algorithm in practice 
__label__MachineLearning __label__QualityOfService __label__TaskAnalysis __label__ComputationalModeling __label__TimeFactors __label__PerformanceEvaluation __label__DataCompression __label__InferenceMechanisms __label__Internet __label__InternetOfThings __label__LearningArtificialIntelligence __label__QualityOfService __label__IotDevices __label__RobustInference __label__HighlyAccurateInference __label__HighlyEfficientDeepLearningInferenceMechanisms __label__DeepLearningModels __label__IotApplications __label__DeepLearningInferenceRuntime __label__Deeprt __label__PredictableInferencePerformance __label__InferenceFramework __label__Qualityofservice __label__QosRequirements __label__DeepLearning __label__Iot __label__EmbeddedSystems __label__RealTime __label__Qos __label__QualityOfService __label__MachineLearning  poster abstract deeprt a predictable deep learning inference framework for iot devices recently deep learning is emerging as a stateoftheart approach in delivering robust and highly accurate inference in many domains including internetofthings iot  deep learning is already changing the way computers embedded in iot devices to make intelligent decisions using sensor feeds in the real world there have been significant efforts to develop lightweight and highly efficient deep learning inference mechanisms for resourceconstrained mobile and iot devices some approaches propose a hardwarebased accelerator and some approaches propose to reduce the amount of computation of deep learning models using various model compression techniques even though these efforts have demonstrated significant gains in performance and efficiency they are not aware of the qualityofservice qos requirements of various iot applications and hence manifest unpredictable besteffort performance in terms of inference latency power consumption resource usage etc in iot devices with temporal constraints such unpredictability might result in undesirable effects such as compromising safety in this work we present a novel deep learning inference runtime called deeprt unlike previous inference accelerators deeprt focuses on supporting predictable inference performance both temporally and spatially 
__label__MachineLearning __label__Servers __label__Training __label__BigData __label__DistributedDatabases __label__TrainingData __label__Libraries __label__BigData __label__LearningArtificialIntelligence __label__BigData __label__Baipas __label__DistributedDeepLearningPlatform __label__DeepLearningModels __label__DataScheduling __label__DistributedTrainingData __label__DeepLearningOperations __label__DataLocalityManagement __label__DataLocalityManager __label__ShufflingMethod __label__Kubernetes __label__Docker __label__DistributedDeepLarningPlatform __label__DataLocality __label__DeepLearning __label__DistributedTensorflow __label__DataShuffling  baipas distributed deep learning platform with data locality and shuffling in this paper we introduce a distributed deep learning platform baipas big data and ai based predication and analysis system in the case of deep learning using big data it takes much time to train with data to reduce training time there is a method that uses distributed deep learning when big data exists in external storage training takes a long time because it takes a lot of network io time when data is loaded during deep learning operations we propose data locality management as a way to reduce training time with big data baipas is a distributed deep learning platform that aims to provide quick learning from big data easy installation and monitoring of the platform and convenience for developers of deep learning models in order to provide fast training using big data data is distributed and stored in workerserver storage using a data locality and shuffling and then training is performed the data locality manager analyzes the training data and the state information of the worker servers this distributes the data scheduling according to the available storage space of the worker server and the learning performance of the worker server however if each worker server conducts deep learning using the distributed training data model overfitting may occur as compared with the method of learning with the full training data set to solve this problem we applied a shuffling method that moves already learned data to another worker server when training is performed thereby each worker server can contain the full training data set baipas uses kubernetes and docker to provide easy installation and monitoring of the platform it also provides preprocessing modules management tools automation of cluster creation resource monitoring and other resources so developers can easily develop deep learning models 
__label__LearningArtificialIntelligence __label__Games __label__Training __label__NeuralNetworks __label__MachineLearning __label__FeatureExtraction __label__ComputerGames __label__LearningArtificialIntelligence __label__NeuralNets __label__DeepReinforcementLearning __label__OnpolicyReinforcementLearning __label__VideoGamesControlProblems __label__DeepConvolutionalNeuralNetwork __label__ScalableMachineLearningProblems __label__DeepSarsaLearning __label__QLearning __label__SarsaLearning __label__QLearning __label__ExperienceReplay __label__DeepReinforcementLearning __label__DeepLearning  deep reinforcement learning with experience replay based on sarsa sarsa as one kind of onpolicy reinforcement learning methods is integrated with deep learning to solve the video games control problems in this paper we use deep convolutional neural network to estimate the stateaction value and sarsa learning to update it besides experience replay is introduced to make the training process suitable to scalable machine learning problems in this way a new deep reinforcement learning method called deep sarsa is proposed to solve complicated control problems such as imitating human to play video games from the experiments results we can conclude that the deep sarsa learning shows better performances in some aspects than deep q learning 
__label__Training __label__BiologicalNeuralNetworks __label__FeatureExtraction __label__Convolution __label__MachineLearningAlgorithms __label__Visualization __label__LearningArtificialIntelligence __label__MultilayerPerceptrons __label__DeepLearningStructure __label__DeepLearningAlgorithm __label__MultilayerNeuralNetwork __label__MachineLearning __label__ShallowLearning __label__LearningAlgorithm __label__DeepLearning __label__LearningAlgorithm __label__MachineLearning  preview on structures and algorithms of deep learning deep learning proposed by hinton et al is a new learning algorithm of multilayer neural network and it is also a new study field in machine learning this paper describes the structures and advantages to shallow learning of deep learning and analyzes current popular learning algorithm in detail finally this paper analyzes research directions and future prospects of deep learning 
__label__ProbabilityDensityFunction __label__MachineLearning __label__NeuralNetworks __label__Geometry __label__Training __label__Seals __label__LossMeasurement __label__LearningArtificialIntelligence __label__NeuralNets __label__Optimisation __label__PatternClustering __label__DeepLearningResearch __label__DeepDivergencebasedClustering __label__RepresentationsLearning __label__DiscriminativeLossFunctionOptimization __label__DeepClusteringNetwork __label__InformationTheoreticDivergenceMeasures __label__DeepNeuralNetworks __label__UnlabeledData __label__ClusterStructure __label__DeepLearning __label__Clustering __label__InformationTheoreticLearning  deep divergencebased clustering a promising direction in deep learning research is to learn representations and simultaneously discover cluster structure in unlabeled data by optimizing a discriminative loss function contrary to supervised deep learning this line of research is in its infancy and the design and optimization of a suitable loss function with the aim of training deep neural networks for clustering is still an open challenge in this paper we propose to leverage the discriminative power of information theoretic divergence measures which have experienced success in traditional clustering to develop a new deep clustering network our proposed loss function incorporates explicitly the geometry of the output space and facilitates fully unsupervised training endtoend experiments on real datasets show that the proposed algorithm achieves competitive performance with respect to other stateoftheart methods 
__label__MachineLearning __label__SupportVectorMachines __label__SupportVectorMachineClassification __label__MachineLearningAlgorithms __label__RiskManagement __label__QuadraticProgramming __label__Equations __label__SystemsEngineeringAndTheory __label__ReadwriteMemory __label__History __label__SupportVectorMachines __label__LearningArtificialIntelligence __label__ClassificationIncrementalLearningAlgorithm __label__SupportVectorMachine __label__TraditionalTrainingMethod __label__Svm __label__StorageCost __label__IterationAlgorithm  an incremental learning algorithm for support vector machine the traditional svm does not support incremental learning and the traditional training method of svm is not working when the amount of training samples are so large that they can not be put into the ram of computer in order to solve this problem and improve the speed of training svm the natural characteristics of sv are analyzed in this paper an incremental learning algorithm isvm for svm with discarding part of history samples is presented the theoretical analysis and experimental results show that this algorithm can not only speed up the training process but also reduce the storage cost while the classification precision is also guaranteed 
__label__PoseEstimation __label__ComputerVision __label__HeatingSystems __label__NeuralNetworks __label__Systematics __label__GestureRecognition __label__MachineLearning __label__ComputerVision __label__FeedforwardNeuralNets __label__GraphicsProcessingUnits __label__NeuralNetArchitecture __label__PoseEstimation __label__RecurrentNeuralNets __label__UnsupervisedLearning __label__VideoSignalProcessing __label__DeepArchitectures __label__ConvolutionStructure __label__ComputerVision __label__GraphicsProcessingUnit __label__GpuProcessingPower __label__DeepLearningAlgorithms __label__OnlineData __label__DeepNeuralNetworkTraining __label__ConvolutionNeuralNetworks __label__NetworkType __label__LearningParadigm __label__HumanPoseEstimation __label__VideoFrames __label__VideoAnalysis __label__CnnImplementations __label__RecurrentNeuralNetworks __label__UnsupervisedLearning __label__ConvolutionalNeuralNetwork __label__DeepLearningAlgorithm __label__ComputerVision __label__HumanPoseEstimation  computer vision approaches based on deep learning and neural networks deep neural networks for video analysis of human pose estimation deep architectures with convolution structure have been found highly effective and commonly used in computer vision with the introduction of graphics processing unit gpu for general purpose issues there has been an increasing attention towards exploiting gpu processing power for deep learning algorithms also large amount of data online has made possible to train deep neural networks efficiently the aim of this paper is to perform a systematic mapping study in order to investigate existing research about implementations of computer vision approaches based on deep learning algorithms and convolutional neural networks cnn  we selected a total of 119 papers which were classified according to field of interest network type learning paradigm research and contribution type our study demonstrates that this field is a promising area for research we choose human pose estimation in video frames as a possible computer vision task to explore in our research after careful studying we propose three different research direction related to improving existing cnn implementations using recurrent neural networks rnns for human pose estimation and finally relying on unsupervised learning paradigm to train nns 
__label__Games __label__MultiagentSystems __label__MarkovProcesses __label__MachineLearning __label__LearningArtificialIntelligence __label__NeuralNetworks __label__ComputerGames __label__DecisionMaking __label__Groupware __label__LearningArtificialIntelligence __label__MultiagentSystems __label__DeepReinforcementLearning __label__DoublesPongGame __label__CoordinatedBehaviors __label__JointLearningAgents __label__ConcurrentLearningAgents __label__DeepQlearning __label__MultiagentSystems __label__CollectiveEffort __label__MainBuildingBlocks __label__FundamentalSystems __label__CollaborativeWork __label__IntelligentCooperativeMultipleAgents __label__SharedGoal __label__IndividualGoals __label__DeepQnetworks __label__Dqn __label__IteratedGameExecutions __label__BehaviorChanges __label__RewardSchemes __label__LearningTechniques __label__EffectiveCooperativeBehaviors __label__ResultingJointActions __label__SequentialDecisionMaking __label__CooperativeBehaviors __label__Mas __label__CooperativeBehavior __label__EnvironmentalCharacteristics __label__WorkloadBalancedDivision __label__ArtificialIntelligence __label__DeepReinforcementLearning __label__MultiAgentSystems __label__CooperativeSystems  learning to coordinate with deep reinforcement learning in doubles pong game this paper discusses the emergence of cooperative and coordinated behaviors between joint and concurrent learning agents using deep qlearning multiagent systems mas arise in a variety of domains the collective effort is one of the main building blocks of many fundamental systems that exist in the world and thus sequential decision making under uncertainty for collaborative work is one of the important and challenging issues for intelligent cooperative multiple agents however the decisions for cooperation are highly sophisticated and complicated because agents may have a certain shared goal or individual goals to achieve and their behavior is inevitably influenced by each other therefore we attempt to explore whether agents using deep qnetworks dqn can learn cooperative behavior we use doubles pong game as an example and we investigate how they learn to divide their works through iterated game executions in our approach agents jointly learn to divide their area of responsibility and each agent uses its own dqn to modify its behavior we also investigate how learned behavior changes according to environmental characteristics including reward schemes and learning techniques our experiments indicate that effective cooperative behaviors with balanced division of workload emerge these results help us to better understand how agents behave and interact with each other in complex environments and how they coherently choose their individual actions such that the resulting joint actions are optimal 
__label__NeuralNetworks __label__Training __label__FeatureExtraction __label__UnsupervisedLearning __label__ClassificationAlgorithms __label__Fitting __label__MathematicalModel __label__BeliefNetworks __label__BoltzmannMachines __label__LearningArtificialIntelligence __label__DeepBeliefNetworks __label__DeepLearning __label__NeuralNetwork __label__DeepLayers __label__RestrictedBoltzmannMachines __label__SoftmaxClassifier __label__HandwrittenNumberRecognition __label__DeepLearning __label__DeepBeliefNetwork __label__ClassifyIntroduction  deep belief networks and deep learning deep belief network is an algorithm among deep learning it is an effective method of solving the problems from neural network with deep layers such as low velocity and the overfitting phenomenon in learning in this paper we will introduce how to process a deep belief network by using restricted boltzmann machines what is more we will combine the deep belief network together with softmax classifier and use it in the recognition of handwritten numbers 
__label__LearningArtificialIntelligence __label__MachineLearning __label__Electromyography __label__PatternRecognition __label__Sensors __label__Accelerometers __label__Quaternions __label__GestureRecognition __label__HumanComputerInteraction __label__InternetOfThings __label__LearningArtificialIntelligence __label__NeuralNets __label__PatternClassification __label__DeepReinforcementLearningAlgorithm __label__HumanArmMovementPatterns __label__IotSensorDevice __label__SupervisedLearningBasedMethods __label__Cnn __label__HclDevice __label__DeepReinforcementLearningApproach __label__ClassLabels __label__IotDevice __label__DesiredArmMovementPatterns __label__DqnModel __label__LstmBasedModel __label__PatternRecognition __label__HandGestureRecognition __label__HumanComputerInteraction __label__LongShorttermMemoryModels __label__HumancomputerInteractionHcl __label__PatterinRecognition __label__DeepReinforcemnetLearning __label__DeepQnetnrok __label__MyoArmband  pattern recognition of human arm movement using deep reinforcement learning hand gesture recognition is one of the major research areas in the field of human computer interaction hcl  this paper proposes a deep reinforcement learning algorithm to recognize the human arm movement patterns using an iot sensor device recent studies have explored supervised learning based methods such as cnn and rnn to implement the hcl device on the other hand the deep reinforcement learning approach has also been investigated algorithms using this approach learn the patterns from sensors using only the reward feedback with no class labels this allows users to control the iot device and produce the desired arm movement patterns without creating any labels in this paper the performance of convolutional neural network cnn with the dqn model was compared with that of long shortterm memory lstm models with dqn results show that the cnn based dqn model was more stable compared to the lstm based model and yielded a high classification accuracy of 9833 to predict the arm movement patterns 
__label__MachineLearning __label__StabilityAnalysis __label__HeuristicAlgorithms __label__StreamingMedia __label__Surveillance __label__Security __label__MachineLearningAlgorithms __label__ClosedCircuitTelevision __label__FaceRecognition __label__LearningArtificialIntelligence __label__Surveillance __label__VideoStreaming __label__DynamicSecuritylevelMaximization __label__StabilizedParallelDeepLearningArchitectures __label__MultipleParallelDeepLearningFrameworks __label__SecurityApplications __label__SystemStability __label__TimeaverageSecuritylevel __label__MachineLearningAccuracy __label__SurveillancePlatform __label__FaceRecognition __label__VideoStreams __label__CctvCameras __label__DynamicControlAlgorithm __label__DeepLearning __label__Security __label__LyapunovOptimization  dynamic securitylevel maximization for stabilized parallel deep learning architectures in surveillance applications this paper introduces a new surveillance platform which is equipped with multiple parallel deep learning frameworks the deep learning frameworks are used for the face recognition of input image and video streams from cctv cameras in security applications each deep learning framework has its own accuracy related to recognition performance and operation time related to system stability those are in tradeoff relationship based on this system architecture a new dynamic control algorithm which selects one deep learning framework for timeaverage securitylevel ie machine learning accuracy for recognition and classification maximization under the consideration of system stability the performance of the proposed algorithm was evaluated and also verified that it achieves desired performance 
__label__PerformanceEvaluation __label__BigData __label__Computers __label__HardDisks __label__Solids __label__InformationManagement __label__Conferences __label__BigData __label__CacheStorage __label__DataAnalysis __label__DiscDrives __label__HardDiscs __label__ParallelProcessing __label__StorageManagement __label__1aStudy __label__BigDataIoPerformance __label__M2SsdCache __label__BigdataProcessingPerformance __label__ModernComputers __label__HighPerformance __label__Hdds __label__BigdataAnalyses __label__Highperformance __label__ModernStorageSystems __label__BigData __label__M2 __label__HardDiskDrive __label__SolidStateDisk __label__SsdCache __label__SequentialAccess __label__Hadoop  1a study on big data io performance with modern storage systems highperformance io is essential for bigdata analyses modern storage systems utilize hdds and ssds mainly for achieving large capacity and high performance respectively using an ssd as a cache for accesses to hdds is one of the promising methods for improving largescale io performance in modern computers in addition m2 is increasing its importance in highperformance io processing in this paper we investigate the io performance of storage systems including m2 ssd and ssd cache our experimental results show that bigdata processing performance can improve significantly by using an m2 ssd cache
__label__FeatureExtraction __label__BenchmarkTesting __label__Force __label__Trajectory __label__ApproximationAlgorithms __label__Robots __label__LearningArtificialIntelligence __label__LearningArtificialIntelligence __label__CreditAssignmentProblem __label__CaProblem __label__DeepRepresentationLearning __label__RlAgent __label__PushRecoveryLearning __label__PrLearning __label__TaskLearningAcceleration __label__QlearningAlgorithm __label__HighDimensionalStatespace __label__DeepAutoencoders __label__RewardingSystem __label__DeepLearning __label__PushRecovery __label__CreditAssignmentProblem __label__LatentVariable __label__RewardingSystem  alleviating credit assignment problem using deep representation learning with application to push recovery learning we propose two new methods to accelerate the learning of a task using qlearning algorithm we focus specifically on learning of a task which has the credit assignment ca problem a reinforcement algorithm rl agent is performing this task in high dimensional statespace the main idea of this paper is to use latent variables that deep autoencoders provide to make a better rewarding system we show that using these new rewards speeds up learning of the task in the similar circumstances the task chosen for the algorithm is push recovery pr in a simulated environment 
__label__Games __label__LearningArtificialIntelligence __label__Training __label__Visualization __label__MachineLearning __label__Convolution __label__ComputerGames __label__LearningArtificialIntelligence __label__DeepQlearning __label__RedundantOutputs __label__VisualDoomAiGame __label__GameAiDomain __label__DeepReinforcementLearning __label__GameAiCommunities __label__TrainingProgress __label__DeepReinforcementLearning __label__ReinforcementLearning __label__Vizdoom __label__FirstpersonPerspectiveGame  deep qlearning using redundant outputs in visual doom recently there is a growing interest in applying deep learning in game ai domain among them deep reinforcement learning is the most famous in game ai communities in this paper we propose to use redundant outputs in order to adapt training progress in deep reinforcement learning we compare our method with general greedy in vizdoom platform since ai player should select an action only based on visual input in the platform it is suitable for deep reinforcement learning research experimental results show that our proposed method archives competitive performance to greedy without parameter tuning 
__label__NeuralNetworkHardware __label__NeuralNetworks __label__NeuralChips __label__NeuralNetwork __label__NeuralNetworkHardware __label__HardwareImplementation __label__NeuralNetworks __label__NeuralNetworkTechnology  neural network hardware summary form only given as follows the author describes recent advances in the hardware implementation of neural networks and gives his expectations for the future in this aspect of neural network technology 
__label__MagnesiumCompounds __label__NeuralNetworks __label__HopfieldNeuralNetworks __label__AssociativeMemory __label__Stability __label__ArtificialNeuralNetworks __label__Neurofeedback __label__InformationScience __label__AdaptiveSystems __label__IntelligentControl __label__NeuralNets __label__ContentaddressableStorage __label__ExtendedBamNeuralNetworkModel __label__ExtendedBidirectionalAssociativeMemoryNeuralNetwork __label__HeteroassociativeMemory __label__AutoassociativeMemory __label__ModelStability __label__MpModel __label__DiscreteHopfieldNeuralNetwork __label__ContinuousHopfieldNeuralNetwork __label__DiscreteBidirectionalAssociativeMemoryNeuralNetwork __label__BackpropagationNeuralNetwork __label__OptimalDesignedNonlinearContinuousNeuralNetwork  an extended bam neural network model proposes an extended bidirectional associative memory bam neural network model which can do auto and heteroassociative memory the theoretical proof for this neural network model s stability is given experiments show that this neural network model is much more powerful than the mp model discrete hopfield neural network continuous hopfield neural network discrete bidirectional associative memory neural network continuous and adaptive bidirectional associative memory neural network backpropagation neural network and optimal designed nonlinear continuous neural network experimental results also show that when it does autoassociative memory the power of this model is the same as the loop neural network model which can only do autoassociative memory 
__label__MachineLearning __label__SupportVectorMachines __label__ExtraterrestrialMeasurements __label__Cybernetics __label__HilbertSpace __label__LearningSystems __label__FuzzySetTheory __label__HilbertSpaces __label__LearningArtificialIntelligence __label__PatternClassification __label__SupportVectorMachines __label__SeparationTheorem __label__MaximalMarginClassification __label__FuzzyNumberSpaces __label__MachineLearningTheory __label__MetricSpace __label__SupportVectorMachine __label__HyperPlanes __label__HilbertSpace __label__Classification __label__SeparatingHyperplane __label__ConvexHull __label__ExtremePoint __label__FuzzyNumbers  separation theorem and maximal margin classification for fuzzy number spaces the theory of machine learning in metric space is a new research topic and has drawn much attention in recent years the theoretical foundation of this topic is the question under which conditions two sample sets can be separated in this space in this paper motivated by developing a new support vector machine svm in fuzzy number space we present a necessary and sufficient condition of separating two finite classes of samples by a hyperplane in ndimensional fuzzy number space we also present an attainable expression of maximal margin of the separating hyperplanes which includes some cases of the classes of infinite samples in ndimensional fuzzy number space these results generalize and improve the corresponding conclusions for the theory of svm in hilbert space to fuzzy number space 
__label__FuzzyNeuralNetworks __label__NeuralNetworks __label__NetworkTopology __label__FeedforwardNeuralNetworks __label__ArtificialNeuralNetworks __label__Feeds __label__FuzzySystems __label__ErrorCorrection __label__ComputerArchitecture __label__FuzzyControl __label__FuzzyNeuralNets __label__FeedforwardNeuralNets __label__Backpropagation __label__Modelling __label__FuzzyExpertNetworkLearningArchitecture __label__SelfAdjustingFuzzyModeling __label__EventdrivenAcyclicNeuralNetworks __label__BackpropagationLearning __label__NeuralObjects __label__FeedforwardNeuralNetwork __label__LayeredTopology __label__DynamicBackwardErrorAssignment __label__NonsmoothMembershipFunctions __label__LearningPerformance __label__Adaptability  fen fuzzy expert network learning architecture generalization of self adjusting fuzzy modeling on eventdriven acyclic neural networks using expert networks backpropagation learning fen fuzzy expert network is a new network architecture of neural objects for fuzzy modeling the neural objects process information through node functions that are different from a typical sigmoidal node processor for an analog perceptron by connecting a few types of node processors on an event driven acyclic feedforward neural network fen represents the fuzzy modeling with self adjustment weights on this network imply fuzzy parameters to be adjusted with no restriction of layered topology by learning fen offers automated tuning from inputoutput data for membership functions on which the performance of fuzzy modeling depends and especially using the enhanced idea of a dynamic backward error assignment for learning fen is effective for tuning parameters for nonsmooth membership functions for example symmetric triangular functions of an antecedent part results of testing fen are presented to demonstrate learning performance and adaptability 
__label__Backpropagation __label__IntelligentNetworks __label__NeuralNetworkHardware __label__NeuralNetworks __label__CountingCircuits __label__DigitalControl __label__VeryLargeScaleIntegration __label__LogicCircuits __label__ErrorCorrection __label__HopfieldNeuralNetworks __label__NeuralNets __label__Backpropagation __label__AnalogueIntegratedCircuits __label__Vlsi __label__LadderNetworks __label__ModifiedBackpropagationLearning __label__AnalogTmodelNeuralNetworkHardware __label__VlsiImplementation __label__DigitallycontrolledSynapseCircuit __label__AdaptationRuleCircuit __label__R2rLadderNetwork __label__ControlLogicCircuit __label__UpdownCounter __label__ModifiedErrorBackpropagationTechnique __label__DigitallycontrolledSynapse  backpropagation learning in analog tmodel neural network hardware in this paper we describe vlsi implementation of a modified backpropagation learning in the tmodel neural networks a digitallycontrolled synapse circuit and an adaptation rule circuit with a r2r ladder network a simple control logic circuit and an updown counter are implemented to realize the modified backpropagation of error technique we also present the adaptive learning using digitallycontrolled synapse to the tmodel networks for several examples in order to study the learning capabilities of the analog tmodel neural hardware these experiments show that the tmodel adaptive neural networks using the modified backpropagation can perform learning procedure quite well 
__label__NeuralNetworks __label__Networkonachip __label__Backpropagation __label__ArtificialNeuralNetworks __label__Neurons __label__AnalogCircuits __label__MosCapacitors __label__HebbianTheory __label__EnergyConsumption __label__LargescaleSystems __label__NeuralChips __label__CmosAnalogueIntegratedCircuits __label__AnalogueMultipliers __label__MultiplyingCircuits __label__Backpropagation __label__HebbianLearning __label__LargeScaleIntegration __label__SubthresholdMos __label__NeuralNetworks __label__OnchipErrorBackpropagationLearning __label__SynapseCircuits __label__StorageCapacitor __label__AnalogMultiplier __label__SignalFeedforward __label__OuterproductSynapticWeightAdjustments __label__HebbianLearning __label__SigmoidCircuit __label__ModularArchitecture __label__LargeScaleIntegration  subthreshold mos implementation of neural networks with onchip error backpropagation learning subthreshold analog circuits for mos implementation of artificial neural networks are presented with onchip learning capability each synapse circuits consist of a storage capacitor and 3 analog multiplier ie one for signal feedforward one for outerproduct synaptic weight adjustments and one for error backpropagation while all the 3 multipliers are used for error backpropagation learning only the first 2 multipliers are used for hebbian learning each neuron circuits are composed of a sigmoid circuit and a sigmoid derivative circuit which show near ideal sigmoid characteristics and provide external gaincontrol capability all the circuits incorporate modular architecture and are designed to increase the numbers of neurons and layers with multiple chips also the subthreshold operation provides low power consumption and large scale implementation 
__label__NeuralNetworks __label__Routing __label__MultiprocessorInterconnectionNetworks __label__NationalElectricCode __label__ElectronicMail __label__HopfieldNeuralNetworks __label__ComputerNetworks __label__ConcurrentComputing __label__FaultTolerance __label__FaultTolerantComputing __label__MultiprocessorInterconnectionNetworks __label__NeuralNets __label__OpticalInformationProcessing __label__ParallelProcessing __label__MultipleStageInterconnectionNetworks __label__HopfieldModelNeuralNetwork __label__ParallelComputer __label__GeneratingControlBits __label__Selfrouting __label__Faulttolerant __label__BenesInterconnectionNetwork  neural network routing for multiple stage interconnection networks summary form only given as follows a hopfield model neural network can be useful as a form of parallel computer such a neural network may be capable of arriving at a problem solution which much more speed than conventional sequential approaches this concept has been applied to the problem of generating control bits for a multistage interconnection network a hopfield model neural network has been designed that is capable of routing a set of messages this neural network solution is especially useful for interconnection networks that are not selfrouting and interconnection networks that have an irregular structure furthermore the neural network routing scheme is faulttolerant results were obtained on generating routes in a 44 benes interconnection network 
__label__NeuralNetworks __label__ControlSystems __label__OpenLoopSystems __label__MultilayerNeuralNetwork __label__AutomaticControl __label__FeedforwardNeuralNetworks __label__Automation __label__NonlinearDynamicalSystems __label__NeuralNetworkHardware __label__Extrapolation __label__NonlinearControlSystems __label__FeedforwardNeuralNets __label__Neurocontrollers __label__ClosedLoopSystems __label__LearningArtificialIntelligence __label__Compensation __label__Feedforward __label__DisturbancerejectionNeuralNetworkControl __label__UnknownNonlinearPlant __label__MultilayerNeuralNetwork __label__InverseDynamics __label__FeedforwardController __label__Learning __label__ParalleledClosedloopControlSystem __label__Compensator  disturbancerejection neural network control a disturbancerejection neural network control scheme is presented for control of an unknown nonlinear plant in the scheme a multilayer neural network is employed to learn the inverse dynamics of the unknown plant and acts as a feedforward controller to control the plant the effect of disturbances on the output is suppressed by using a paralleled closedloop control system the design technique of the compensator in the closedloop system is discussed simulation results show that the presented control scheme works well in the presence of disturbances 
__label__FlexibleStructures __label__ArtificialNeuralNetworks __label__ControlSystems __label__AdaptiveControl __label__NeuralNetworks __label__VibrationControl __label__Satellites __label__MultilayerNeuralNetwork __label__SignalGenerators __label__Actuators __label__Stability __label__FlexibleStructures __label__AerospaceControl __label__VibrationControl __label__Neurocontrollers __label__FeedforwardNeuralNets __label__MultilayerPerceptrons __label__Stabilization __label__ArtificialNeuralNetworks __label__LargeFlexibleSpaceStructures __label__NeuralController __label__Dynamics __label__ControlSignal __label__StructuralVibrations __label__ThreeLayerFeedforwardNetwork __label__SequentialUpdating __label__NetworkWeights __label__FlexibleBeam  stabilization of flexible structures using artificial neural networks a method of using artificial neural networks to stabilize large flexible space structures is presented the neural controller learns the dynamics of the structure to be controlled and constructs a control signal to stabilize structural vibrations the network consists of a three layer feedforward network the input layer receives the displacement and velocity information from sensors located at various points in the structure and the output layer generates control signals that are applied to the structure through suitable actuators sequential updating of the network weights continues forcing the structure follow a trajectory that eventually leads to complete stabilization simulation results on the stabilization of a flexible beam are presented 
__label__NeuralNetworks __label__ChebyshevApproximation __label__FunctionApproximation __label__RecurrentNeuralNetworks __label__MultilayerNeuralNetwork __label__FeedforwardNeuralNetworks __label__MultilayerPerceptrons __label__Polynomials __label__HInfinityControl __label__TransferFunctions __label__NonlinearControlSystems __label__Identification __label__LearningArtificialIntelligence __label__FunctionApproximation __label__HsupSplInfinControl __label__NeuralNets __label__ApproximateEquivalenceNeuralNetwork __label__WorstcaseIdentification __label__WorstcaseControl __label__NonlinearSystem __label__FastLearning __label__FunctionApproximationCapability __label__ObjectiveFunction __label__HsupSplInfinInducedNorm __label__ApproximateTransformableTechnique __label__SinglelayeredNeuralNetwork __label__MultilayeredPerceptronNeuralNetwork __label__FunctionalLinkNetwork __label__ChebyshevPolynomials __label__InfinityNorm __label__TransferFunction  an approximate equivalence neural network to conventional neural network for the worstcase identification and control of nonlinear system in this paper we propose an approximate equivalence neural network model with a fast learning speed as well as a good function approximation capability and a new objective function which satisfies the hsup spl infin induced norm to solve the worstcase identification and control of nonlinear problems the approximate equivalence neural network not only has the same capability of universal approximator but also has a faster learning speed than the conventional feedforwardrecurrent neural networks based on this approximate transformable technique the relationship between the singlelayered neural network and multilayered perceptrons neural network is derived it is shown that a approximate equivalence neural network can be represented as a functional link network that is based on chebyshev polynomials we also derive a new learning algorithm such that the infinity norm of the transfer function from the input to the output is under a prescribed level it turns out that the approximate equivalence neural network can be extended to do the worstcase problem in the identification and control of nonlinear problems 
__label__NeuralNetworks __label__Neurons __label__MultilayerNeuralNetwork __label__PatternRecognition __label__SignalGenerators __label__PulseModulation __label__Concrete __label__ArtificialNeuralNetworks __label__ImageSampling __label__ArtificialIntelligence __label__FeedforwardNeuralNets __label__Probability __label__PatternRecognition __label__Vectors __label__ContinualNeuralNetworks __label__PatternRecognition __label__OpenloopStructures __label__MultilayerNeuralNetworks __label__MaximumProbability __label__Vector __label__FeatureContinuum  continual neural networks it is necessary to introduce many parameters describing structure and input signal of pattern recognition system during construction of openloop structures of multilayer neural networks in order to provide maximum probability of correcting recognition in practice availability of great number of parameters viz hundreds and thousands rouses some difficulties for learning and technical implementation of such multilayer neural network essence of introduction of continual properties of multilayer neural network characteristics includes the following vector xsub i i1  i replaces by function x i of continued argument ie during transition to continuum of characteristic value transition to attributes continuum and continuum of neurons in layer is considered on the concrete examples of neural networks structures 
__label__MachineLearningAndRadioEmitterThreatDegreeJudgmentBasedOnFuzzyNeuralNetworkModernElectronicWarfareSystemMustJudgeTheThreatDegreeOfComingRadioEmittersCorrectlyInOrderToCounterThemByTheLimitedJammingResourceEffectivelyThisArticlePutsForwardANewStrategyToJudgeTheRadioEmitterThreatDegreeRetdBasedOnMachineLearningItFirstlyGetsTheMembershipDegreesOfTheInputDataThenTheInputDataIsClassifiedATrainedFuzzyNeuralNetworkFnnWithApproachingAbilityGivesTheThreatDegreeTheRetdJudgmentRulesCouldBeMinedFromTheNetworkTheCorrectnessAndEffectivenessAreProvedInTheExperiment  machine learning and radio emitter threat degree judgment based on fuzzy neural network modern electronic warfare system must judge the threat degree of coming radio emitters correctly in order to counter them by the limited jamming resource effectively this article puts forward a new strategy to judge the radio emitter threat degree retd based on machine learning it firstly gets the membership degrees of the input data then the input data is classified a trained fuzzy neural network fnn with approaching ability gives the threat degree the retd judgment rules could be mined from the network the correctness and effectiveness are proved in the experiment 
__label__Kinematics __label__Knee __label__EigenvaluesAndEigenfunctions __label__PrincipalComponentAnalysis __label__BigData __label__FeatureExtraction __label__MachineLearningAlgorithms __label__BigData __label__DataAnalysis __label__FeatureExtraction __label__LearningArtificialIntelligence __label__MeanSquareErrorMethods __label__MedicalComputing __label__PrincipalComponentAnalysis __label__Prosthetics __label__Surgery __label__PostOperativeImplantedKneeFunctionPrediction __label__MachineLearning __label__ClinicalBigDataAnalysis __label__TotalKneeArthroplasty __label__Tka __label__KneeSurgeries __label__PreoperativePlanning __label__FeatureExtraction __label__PrincipalComponentAnalysis __label__MeanCorrelation __label__MeanRootMeanSquaredError __label__PredictiveMedicine __label__TotalKneeArthroplasty __label__ClinicalBigData __label__MachineLearning __label__PrincipalComponentAnalysis  prediction of postoperative implanted knee function using machine learning in clinical big data total knee arthroplasty tka is one of the common knee surgeries because there are some types of tka implant it is hard to select appropriate type of tka implant for individual patient for the sake of preoperative planning this study presents a novel approach which predicts postoperative implanted knee function of individuals it is based on a clinical big data analysis the big data is composed by a set of preoperative knee mobility function and postoperative knee function the method constructs a postoperative knee function prediction model by means of a machine learning approach it extracts features using principal component analysis and constructs a mapping function from preoperative feature space to postoperative feature space the method was validated by applying to prediction of postoperative anteriorposterior translation in 52 tka operated knees leaveoneout cross validation test revealed the prediction performances with a mean correlation coefficients of 079 and a mean rootmeansquarederror of 344 mm 
__label__SupportVectorMachines __label__QuadraticProgramming __label__SupportVectorMachineClassification __label__MachineLearning __label__DifferentialEquations __label__SpaceTechnology __label__LearningSystems __label__RiskManagement __label__NeuralNetworks __label__LagrangianFunctions __label__DifferentialEquations __label__LearningArtificialIntelligence __label__RegressionAnalysis __label__StatisticalAnalysis __label__SupportVectorMachines __label__MultilayerSupportVectorMachine __label__StatisticalLearningTheory __label__MachineLearningMethod __label__ClassificationMethod __label__RegressionMethod __label__HighDimensionalFeatureSpace __label__OrdinaryDifferentialEquation __label__SupportVectorMachine __label__MultilayerSvm __label__MthOrderOrdinaryDifferentialEquations __label__KernelFunction  multilayer support vector machine and its application based on statistical learning theory slt  support vector machine svm  which is a new kind of machine learning method that is used for classification and regression svm is considered as two layers learning machine since it maps the original space into a high dimensional feature space ie input layer and high dimensional feature space layer if the high dimensional feature space layer is considered as a new problem s input layer and the new problem is also solved by svm the new problem can be solved by svms named multilayer svm mlsvm  mlsvm is composed of input layer and at least one layer high dimensional feature space layer in this paper mth order ordinary differential equations are solved by mlsvm for regression experimental results indicate that mlsvm can effectively solve the problem of ordinary differential equations thus mlsvm exhibits its great potential to solve other complex problems
__label__BigData __label__Tools __label__Schedules __label__ResourceManagement __label__FileSystems __label__DynamicScheduling __label__BigData __label__DataAnalysis __label__Middleware __label__ParallelProcessing __label__ResourceAllocation __label__Scheduling __label__BigDataSystems __label__HpcWorkload __label__HpcJobs __label__HpcCollocation __label__HpcIdleResources __label__BigDataAnalytics __label__ExecutingBigDataWorkloads __label__HighPerformanceComputingInfrastractures __label__BigDataScheduling __label__ResourceAndJobManagementSystem __label__ProductionRjmsMiddlewareOar __label__HadoopYarn __label__ClusterUtilization __label__HpcAndBigDataConvergence __label__Infrastructure __label__Scheduling __label__BestEffort __label__ResourceManagement  big data and hpc collocation using hpc idle resources for big data analytics executing big data workloads upon high performance computing hpc infrastractures has become an attractive way to improve their performances however the collocation of hpc and big data workloads is not an easy task mainly because of their core concepts differences this paper focuses on the challenges related to the scheduling of both big data and hpc workloads on the same computing platform in classic hpc workloads the rigidity of jobs tends to create holes in the schedule we can use those idle resources as a dynamic pool for big data workloads we propose a new idea based on resource and job management system s rjms configuration that makes hpc and big data systems to communicate through a simple prologepilog mechanism it leverages the builtin resilience of big data frameworks while minimizing the disturbance on hpc workloads we present the first study of this approach using the production rjms middleware oar and hadoop yarn from the hpc and big data ecosystems respectively our new technique is evaluated with real experiments upon the grid5000 platform our experiments validate our assumptions and show promising results the system is capable of running an hpc workload with 70 cluster utilization with a big data workload that fills the schedule holes to reach a full 100 utilization we observe a penalty on the mean waiting time for hpc jobs of less than 17 and a big data effectiveness of more than 68 in average 
__label__BloodPressure __label__PressureMeasurement __label__BiomedicalMonitoring __label__BigData __label__Hypertension __label__Standards __label__BigData __label__BiomedicalEducation __label__BloodPressureMeasurement __label__ComputerAidedInstruction __label__DataAcquisition __label__EducationalCourses __label__MedicalComputing __label__MedicalInformationSystems __label__ResearchAndDevelopment __label__Mooc __label__MedicalBigDataResearch __label__HypertensionBigDataResearch __label__Physician __label__BloodPressureMeasurement __label__LargescaleHypertensionResearchProject __label__DataAcquisition __label__DataSource __label__MassiveOnlineOpenCourse __label__CitizenScienceParadigm __label__OnlineCourse __label__VideoLecture __label__ComprehensionQuizzes __label__DiscussionForum __label__DataCollector __label__MedicalBigDataResearch __label__MassiveOnlineOpenCourse __label__Hypertension  mooc for medical big data research an important role in hypertension big data research due to limited technical and social resources many physician practices fall short on accurate blood pressure measurement to carry out largescale hypertension research projects the accuracy and standard of data acquisition are very important when data sources are diverse in medical big data research this paper proposes massive online open course mooc is appropriate approach to teach volunteers necessary knowledge and skills of blood pressure measurement for hypertension research it introduces a new citizen science paradigm to support big data research such as hypertension mooc is a new type online course that provides a combination of short video lectures frequent comprehension quizzes and active participation in discussion forum the welltrained data collectors by mooc will be granted to collect and publish data of hypertension research the process of medical big data research based on mooc was introduced 
__label__BigData __label__BiomedicalImaging __label__Personnel __label__ComplexityTheory __label__Bioinformatics __label__ProcessControl __label__Instruments __label__BigData __label__DataAnalysis __label__BigDataQualityProblem __label__Usaf __label__MitreResearchTeam __label__DomainspecificCaseStudies __label__BigDataCollections __label__DataAnalyticsProblems __label__DqIssues __label__ReturnstoscaleEffects __label__BigData __label__DataQuality __label__ReturnsToScale  big data big data quality problem a usaf sponsored mitre research team undertook four separate domainspecific case studies about big data applications those case studies were initial investigations into the question of whether or not data quality issues encountered in big data collections are substantially different in cause manifestation or detection than those data quality issues encountered in more traditionally sized data collections the study addresses several factors affecting big data quality at multiple levels including collection processing and storage though not unexpected the key findings of this study reinforce that the primary factors affecting big data reside in the limitations and complexities involved with handling big data while maintaining its integrity these concerns are of a higher magnitude than the provenance of the data the processing and the tools used to prepare manipulate and store the data data quality is extremely important for all data analytics problems from the study s findings the truth about big data is there are no fundamentally new dq issues in big data analytics projects some dq issues exhibit returnstoscale effects and become more or less pronounced in big data analytics though big data quality varies from one type of big data to another and from one big data technology to another 
__label__BigData __label__Semantics __label__TimeSeriesAnalysis __label__DataModels __label__Databases __label__WindTurbines __label__BigData __label__DataIntegration __label__DesignEngineering __label__GasTurbines __label__GraphicalUserInterfaces __label__ProductionEngineeringComputing __label__ProductionEquipment __label__BigDataAccess __label__BigDataIntegration __label__IndustrialEquipmentDesign __label__DataUsability __label__BigDataTechnologies __label__DataStores __label__DataRepresentation __label__BigDataManagementInfrastructure __label__SemanticTechnologies __label__GraphicalUserInterface __label__DatasetQuery __label__DomainmodelTerms __label__GasTurbine __label__GePowerAndWater __label__PowerGenerationProductsEngineeringDivision __label__Semantics __label__Ontology __label__DomainModel __label__BigData __label__TimeSeriesData __label__DataManagement __label__Engineering  semantics for big data access integration improving industrial equipment design through increased data usability with the advent of big data technologies organizations can efficiently store and analyze more data than ever before however extracting maximal value from this data can be challenging for many reasons for example datasets are often not stored using humanunderstandable terms making it difficult for a large set of users to benefit from them further given that different types of data may be best stored using different technologies datasets that are closely related may be stored separately with no explicit linkage finally even within individual data stores there are often inconsistencies in data representations whether introduced over time or due to different data producers these challenges are further compounded by frequent additions to the data including new raw data as well as results produced by largescale analytics thus even within a single big data environment it is often the case that multiple rich datasets exist without the means to access them in a unified and cohesive way often leading to lost value this paper describes the development of a big data management infrastructure with semantic technologies at its core to provide a unified data access layer and a consistent approach to analytic execution semantic technologies were used to create domain models describing mutually relevant datasets and the relationships between them with a graphical user interface to transparently query across datasets using domainmodel terms this prototype system was built for ge power water s power generation products engineering division which has produced over 50tb of gas turbine and component prototype test data to date the system is expected to result in significant savings in productivity and expenditure 
__label__BigData __label__DataMining __label__CloudComputing __label__DataAnalysis __label__AlgorithmDesignAndAnalysis __label__Correlation __label__FeatureExtraction __label__BigData __label__DataAnalysis __label__DataMining __label__ParallelProcessing __label__SmartEffectiveCrowdManagement __label__Scem __label__BigDataAnalytics __label__KnowledgeExtraction __label__LiveDataMassiveStreaming __label__DistributedStorage __label__DistributedMining __label__SignalCapturing __label__LogitechHdC920 __label__IntelXeonE5540Processors __label__Hadoop __label__Mapreduce __label__BigDataAnalytics __label__Hadoop __label__Mapreduce __label__DynamicDataManagement __label__LiveDataAnalysis  scem smart effective crowd management with a novel scheme of big data analytics the proposed paper presents a novel scheme that can perform a precise extraction of knowledge from the complex and massive streaming of live data of the scene from the crowded place the prime contribution of the proposed system is to perform enough processing over the raw and unstructured distributed data from multiple locations so that processing over distributed storage and mining can be done with lesser processing time and higher degree of accuracy an experimental research methodology has been adopted to capture signal using logitech hd c920 and processed over intel xeon e5540 processors with 2 gpbs connectivity the raw data is subjected to preprocessing segmentation scene profiling in order to get convolved data that are stored in distributive manner using hadoop and mined using mapreduce the comparative study outcome shows lesser processing time and higher accuracy as compared to existing relevant analytics 
__label__DataIntegration __label__Organizations __label__Interoperability __label__BigData __label__Aging __label__StandardsOrganizations __label__DataMining __label__BigData __label__DataIntegration __label__OrganisationalAspects __label__StorageManagement __label__ResearchInstitutions __label__OrganizationWorkFlow __label__DataStorage __label__DataManagement __label__DataMaintenance __label__DataInteroperability __label__OrganizationPerformance __label__BigDataArchitectures __label__DataIntegration __label__DataIntegration __label__DataInteroperability  challenges of data integration and interoperability in big data the enormous volumes of data created and maintained by industries research institutions are on the verge of outgrowing its infrastructure the advancements in the organization s work flow include data storage data management data maintenance data integration and data interoperability among these levels data integration and data interoperability can be the two major focus areas for the organizations which tend to implement advancements in their workflow overall data integration and data interoperability influence the organization s performance the data integration and data interoperability are complex challenges for the organizations deploying big data architectures due to the heterogeneous nature of data used by them therefore it requires a comprehensive approach to negotiate the challenges in integration and interoperability this paper focuses on the challenges of data integration and data interoperability in big data 
__label__BigData __label__TerrestrialAtmosphere __label__Metadata __label__Software __label__Databases __label__Earth __label__DataIntegration __label__BigData __label__DataAnalysis __label__InformationRetrievalSystems __label__ResearchAndDevelopment __label__BigData __label__DataSharing __label__ScholarlyCommunication __label__ConceptualDesign __label__ComprehensiveResearchSupportPlatform __label__DataIntegrationAndAnalysisSystemProgram __label__Dias __label__InteruniversityUpperAtmosphereGlobalObservationNetwork __label__Iugonet __label__SubjectbasedDataArchives __label__Japan __label__ResearchDataManagement __label__ResearchDataManagement __label__DataSharing __label__DataArchive __label__ScholarlyCommunication __label__OpenScience  conceptual design for comprehensive research support platform successful research data management generating big data from little data data sharing which is hot issues in scholarly communication is regarded as generating big data from little data in little science in this article a conceptual framework for research support platform in university is proposed by the survey of two cases of representative and subjectbased data archives in japan data integration and analysis system program dias and interuniversity upper atmosphere global observation network iugonet 
__label__GeospatialAnalysis __label__Heating __label__BigData __label__DataMining __label__Software __label__SpatialDatabases __label__Laboratories __label__BigData __label__GeographicInformationSystems __label__PublicDomainSoftware __label__OpenSourceFramework __label__BigDataGeospatialVisibility __label__BigDataSpatialExtent __label__GeospatialDataPointExtraction __label__GeospatialDataPointNormalization __label__GeospatialDataPointProcessing __label__DataFormat __label__FlexibleVisualizationbasedUserInterface __label__BigData __label__GeospatialSearch __label__BiodiversityData __label__Bison __label__Gbif  an open source framework to add spatial extent and geospatial visibility to big data advancement in the field of computing and remote handheld devices has made the process of collecting geospatial data easy most of the time researchers and scientists have easy access to these data as well however the process of extracting and processing a large volume of data from several sources can be very time consuming and difficult in most cases scientists rely on expensive proprietary software 1  this paper discusses how computational scientists at oak ridge national laboratory are extracting normalizing and processing millions of geospatial data points from multiple data sources and integrating them into a common data format which helps user to find and access these data using a flexible visualizationbased user interface 
__label__DataMining __label__SpatiotemporalPhenomena __label__SocialNetworkServices __label__Correlation __label__BigData __label__Crowdsourcing __label__SpatialDatabases __label__BigData __label__DataAnalysis __label__DataMining __label__PatternClustering __label__SocialNetworkingOnline __label__WeiboCheckinData __label__SpatiotemporalData __label__CrowdSourcingGeographicDataToday __label__CommercialHotspotExploration __label__SocialNetworks __label__FocusedMining __label__SpacetimeLargeData __label__ExploratorySpatialDataAnalysis __label__SocialMediaBigDataAnalysis __label__ClusteringMining __label__Esda __label__Wuhan __label__CommercialHotspotsMining __label__SocialMediaBigData __label__SpatiotemporalData __label__ClusterMining __label__EfficientSelection __label__Esda __label__CommercialHotspots  an effective selecting approach for social media big data analysis  taking commercial hotspot exploration with weibo checkin data as an example according to the problem that efficient datasets can not be quickly obtained from social media big data of social networks in the process of focused mining and analysis an effective selection method for clustering mining with spacetime large data is proposed the effective selection method of clustering mining divides the spatiotemporal large data from the dimension of space time or attribute then do exploratory spatial data analysis esda to the obtained subsets to get the datasets with the potential of clustering mining quickly the proposed method is verified by using the weibo checkin data in wuhan which is between 2011 and 2015 to mine commercial hotspots the experimental results show that the method can quickly and effectively excavate datasets from weibo checkin data that can reflect the distribution of wuhan business circle and the excavate d datasets have the characteristics of high clustering small volume high precision the effective selection method of clustering mining for spatiotemporal data can provide fast and effective methods and ideas for the process of crowd sourcing geographic data today 
__label__BigData __label__Tools __label__Databases __label__ElectronicMail __label__Reliability __label__TemperatureMeasurement __label__NullValue __label__BigData __label__BigDataQuality __label__DataQualityDimension __label__DataValidationProcess __label__BigDataQuality __label__BigDataValidation __label__DataChecklist __label__BigDataTool __label__CaseStudy  big data validation case study with the advent of big data data is being generated collected transformed processed and analyzed at an unprecedented scale since data is created at a fast velocity and with a large variety the quality of big data is far from perfect recent studies have shown that poor quality can bring serious erroneous data costs on the result of big data analysis data validation is an important process to recognize and improve data quality in this paper a case study that is relevant to big data quality is designed to study original big data quality data quality dimension data validation process and tools 
__label__FaultDiagnosis __label__MachineLearning __label__FaultDetection __label__Production __label__SignalDetection __label__ManufacturingSystems __label__ManufacturingProcesses __label__Investments __label__FeatureExtraction __label__ComputerVision __label__FaultDiagnosis __label__LearningArtificialIntelligence __label__SignalDetection __label__ManufacturingSystems __label__MaintenanceEngineering __label__FaultDiagnosisModel __label__MachineLearning __label__FeatureExtraction __label__ManufacturingSystem __label__StatisticalBoundaryVector __label__FaultSamples __label__TheoreticalAnalysis __label__FaultDiagnosis __label__MachineLearning __label__FeatureExtraction  a new method of early fault diagnosis based on machine learning a new method of early fault diagnosis for manufacturing system based on machine learning is presented it is necessary for manufacturing enterprises to detect the states of production process in real time in order to find the early faults in machines so that the losses of production failure and investments of facility maintenance can be minimized this paper proposes a new fault diagnosis model which extracts multidimension features from the detected signal to supervise the different features of the signal simultaneously based on the model the method of inductive learning is adopted to obtain the statistical boundary vectors of the signal automatically and then a normal feature space is built according to which an abnormal signal can be detected and consequently the faults in a complicated system can be found easily furthermore under the condition of without existing fault samples the precise results of fault diagnosis can also be achieved in real time the theoretical analysis and simulation example demonstrate the effectiveness of the method 
__label__DataVisualization __label__Meteorology __label__DistributedDatabases __label__Servers __label__BigData __label__AtmosphericMeasurements __label__DataModels __label__BigData __label__ComputerCentres __label__NextgenTools __label__BigScientificData __label__ArmDataCenter __label__AtmosphericRadiationMeasurementClimateResearchFacility __label__NetcdfFileFormat __label__UserAssessableDataFiles __label__MultidimensionalClimateObservations __label__ArmArchive __label__BigDataDiscovery __label__MetadataManagement  nextgen tools for big scientific data arm data center example the atmospheric radiation measurement arm climate research facility wwwarmgov provides atmospheric observations from diverse climatic regimes around the world currently arm archives over 22 million user assessable data files primarily stored in netcdf file format with total data volumes close to one petabyte in this paper we will discuss how arm is currently storing distributing cataloging and visualizing such large volumes of multidimensional climate observations and model data and also describe their future plan 
__label__BigData __label__DataModels __label__Redundancy __label__Security __label__Companies __label__IsoStandards __label__Taxonomy __label__BigData __label__DataPrivacy __label__InternetOfThings __label__SocialNetworkingOnline __label__BigDataRiskAnalysis __label__SocialNetwork __label__Internetofthings __label__BigDataAnalytics __label__DataRepresentation __label__BigDataLeakThreat __label__DisclosureRisk __label__BigDataSecurity __label__IllprotectedCopies __label__PrivacyProtection __label__BigData __label__ThreatAnalysis __label__Internetofthings __label__BigDataAnalytics  toward big data risk analysis the advent of social networks and internetofthings has resulted in unprecedented capability of collecting sharing and analyzing massive amounts of data from a security perspective big data may seriously weaken confidentiality as techniques for improving big data analytics performanceincluding early fusion of heterogeneous data sources increase the hidden redundancy of data representation generating illprotected copies this gray area of redundancy triggers new disclosure threats that challenge traditional techniques to protect privacy and confidentiality this position paper starts by proposing a definition of the big data leak threat as opposed to the one of data breach and its role as a component of disclosure risk then it discusses how a paradigm of known detect contain and recover could be used to establish big data security practices for containing disclosure risks connected to big data analytics 
__label__Facebook __label__Forecasting __label__Google __label__DataAnalysis __label__Companies __label__DataModels __label__MarketResearch __label__BigData __label__BusinessDataProcessing __label__MarketingDataProcessing __label__RegressionAnalysis __label__SalesManagement __label__SocialNetworkingOnline __label__TextAnalysis __label__NikeSalesForecasting __label__FacebookData __label__AidaSalesFramework __label__Awarenessinterestdesireaction __label__Marketing __label__SocialSetAnalysis __label__ComputationalSocialScience __label__BigSocialData __label__BusinessData __label__SocialSetVisualizer __label__Sosevi __label__RegressionModels __label__TextAnalysis __label__PredictiveAnalytics __label__BigDataAnalytics __label__BigSocialData __label__EventStudy __label__Nike __label__FacebookDataAnalytics  forecasting nike s sales using facebook data this paper tests whether accurate sales forecasts for nike are possible from facebook data and how events related to nike affect the activity on nike s facebook pages the paper draws from the aida sales framework awareness interest desire and action from the domain of marketing and employs the method of social set analysis from the domain of computational social science to model sales from big social data the dataset consists of a selection of nike s facebook pages with the number of likes comments posts etc that have been registered for each page per day and b business data in terms of quarterly global sales figures published in nike s financial reports an event study is also conducted using the social set visualizer sosevi  the findings suggest that facebook data does have informational value some of the simple regression models have a high forecasting accuracy the multiple regressions have a lower forecasting accuracy and cause analysis barriers due to data set characteristics such as perfect multicollinearity the event study found abnormal activity around several nike specific events but inferences about those activity spikes whether they are purely eventrelated or coincidences can only be determined after detailed casebycase text analysis our findings help assess the informational value of big social data for a company s marketing strategy sales operations and supply chain 
__label__DataMining __label__Databases __label__BigData __label__DataModels __label__ComputationalModeling __label__PartitioningAlgorithms __label__ProgramProcessors __label__BigData __label__DataAnalysis __label__DataMining __label__SearchSpaceReduction __label__UncertainBigDataMining __label__UncertainData __label__MapreduceModel __label__FrequentPatterns __label__UserspecifiedConstraints __label__BigDataAnalytics __label__BigDataModelsAndAlgorithms __label__AlgorithmsAndProgrammingTechniquesForBigDataProcessing __label__BigDataAnalytics __label__BigDataSearchAndMining __label__FrequentPatterns __label__Constraints __label__UncertainData  reducing the search space for big data mining for interesting patterns from uncertain data many existing data mining algorithms search interesting patterns from transactional databases of precise data however there are situations in which data are uncertain items in each transaction of these probabilistic databases of uncertain data are usually associated with existential probabilities which express the likelihood of these items to be present in the transaction when compared with mining from precise data the search space for mining from uncertain data is much larger due to the presence of the existential probabilities this problem is worsened as we are moving to the era of big data furthermore in many reallife applications users may be interested in a tiny portion of this large search space for big data mining without providing opportunities for users to express the interesting patterns to be mined many existing data mining algorithms return numerous patterns out of which only some are interesting in this paper we propose an algorithm that i allows users to express their interest in terms of constraints and ii uses the mapreduce model to mine uncertain big data for frequent patterns that satisfy the userspecified constraints by exploiting properties of the constraints our algorithm greatly reduces the search space for big data mining of uncertain data and returns only those patterns that are interesting to the users for big data analytics 
__label__BigData __label__DataMining __label__CloudComputing __label__MachineLearningAlgorithms __label__DataAnalysis __label__Programming __label__BigData __label__DataAnalysis __label__DataMining __label__BigDataAnalytics __label__HybridEra __label__BigDataProcessing __label__SoftwareFrameworks __label__DataMiningAlgorithm __label__BigDataAnalysis __label__BigData __label__BigDataAnalytics __label__CloudComputing __label__DataMining __label__MachineLearning __label__BigDataPlatforms  platforms for big data analytics trend towards hybrid era the primary objective of this paper is to present detailed analysis of various platforms suitable for big data processing in this paper various software frameworks available for big data analytics are surveyed and indetail assessment of their strengths and weaknesses is discussed in addition to this widely used data mining algorithm are discussed for their adaptation for big data analysis wrt their suitability for handling realworld application problems future trends of big data processing and analytics can be predicted with effective implementation of these well established and widely used data mining algorithms by considering the strengths of software frameworks and platforms available hybrid approaches integration of two or more platforms may be more appropriate for a specific data mining algorithm and can be highly adaptable as well as perform realtime processing 
__label__Terrorism __label__Organizations __label__BigData __label__DataMining __label__SocialNetworkServices __label__Monitoring __label__Internet __label__BigData __label__DataAnalysis __label__DataMining __label__Terrorism __label__BigData __label__CounterterrorismResearch __label__CounterterrorismField __label__QuantitativeAnalysisMethod __label__DataCollection __label__DataMining __label__DataAnalysis __label__DataMonitoring __label__DataWarning __label__TheNewYorkTimes __label__TheTimes __label__Wuc __label__BigData __label__Counterterrorism __label__DataMining  research on counterterrorism based on big data in the area of big data people have a new perspective on counterterrorism research in this paper we have carried out a systematic research on the applications of big data in counterterrorism field by using quantitative analysis method and then we have demonstrated effect of big data on counterterrorism research from data collection and preprocessing data mining and analysis monitoring and warning three aspects after that we have used all the data of the times and the new york times about wuc in 2012 for analysis and argument finally it concludes the deficiencies of the research on the territory of counterterrorism by using big data and the problems worth studying in the future 
__label__CloudComputing __label__BigData __label__Libraries __label__ParallelProcessing __label__ComputerScience __label__ComputationalModeling __label__Scalability __label__BigData __label__CloudComputing __label__ParallelProcessing __label__ResourceAllocation __label__Scheduling __label__BigDataApplicationWorkflow __label__DataManagement __label__DataAnalysis __label__GeographicLocation __label__LargescaleDataIntensiveScientificApplication __label__BusinessApplication __label__ParallelDataProcessing __label__DataGeographicalDistribution __label__FederatedIntercloudEnvironment __label__ApplicationAllocation __label__ApplicationExecutionScheduling __label__BigData __label__Cloud __label__Workflow __label__Scheduling __label__Intercloud  scheduling of big data application workflows in cloud and intercloud environments large amount of data is being generated every day and is creating new challenges and opportunities which lead to extraordinary new knowledge and discoveries in many application domains ranging from science and engineering to business one of the main challenges in this era of big data is how to efficiently manage and analyse such scale of data this is challenging not only due to the size of the data but also due to its heterogeneous nature and geographic location in this sense clouds are becoming an increasingly popular infrastructure for enabling largescale data intensive scientific and business applications cloud provides a suitable environment for processing big data applications to process large volumes of data and parallel processing of data due to the geographical distribution of data different varieties of data it is of more advantage to use a federated intercloud environment where moving of large volumes of data can be avoided workflows are used to allocate and schedule execution of big data applications in an optimized manner in this paper we present the need of execution of big data application workflows on cloud and intercloud environments 
__label__Sparks __label__BigData __label__Libraries __label__DataModels __label__ComputerArchitecture __label__MachineLearningAlgorithms __label__BigData __label__DataAnalysis __label__DataMining __label__LearningArtificialIntelligence __label__DiverseDataSources __label__PremierCapabilities __label__MachineLearningStrategies __label__BigDatasets __label__ComplexDatasets __label__LogicalResources __label__PhysicalResources __label__BigDataAnalysis __label__ApacheSparkMllib2 __label__BigDataMachineLearningResearch __label__ArtificialIntelligence __label__ResearchCommunity __label__HeterogeneousDataSources __label__PatternDiscovery __label__BigDataAnalytics __label__ComputationalPerspective __label__PlatformIndependentMachineLearningLibrary __label__QualitativeAttribute __label__QuantitativeAttribute __label__ApacheSparkMllib __label__BigDataMachineLearning __label__BigDataAnalytics __label__MachineLearning  big data machine learning using apache spark mllib artificial intelligence and particularly machine learning has been used in many ways by the research community to turn a variety of diverse and even heterogeneous data sources into high quality facts and knowledge providing premier capabilities to accurate pattern discovery however applying machine learning strategies on big and complex datasets is computationally expensive and it consumes a very large amount of logical and physical resources such as data file space cpu and memory a sophisticated platform for efficient big data analytics is becoming more important these days as the data amount generated in a daily basis exceeds over quintillion bytes apache spark mllib is one of the most prominent platforms for big data analysis which offers a set of excellent functionalities for different machine learning tasks ranging from regression classification and dimension reduction to clustering and rule extraction in this contribution we explore from the computational perspective the expanding body of the apache spark mllib 20 as an opensource distributed scalable and platform independent machine learning library specifically we perform several real world machine learning experiments to examine the qualitative and quantitative attributes of the platform furthermore we highlight current trends in big data machine learning research and provide insights for future work 
__label__ComputerArchitecture __label__BigData __label__DataMining __label__UnifiedModelingLanguage __label__RealtimeSystems __label__Planning __label__Industries __label__BigData __label__DataMining __label__IntelligentBigDataAnalysisArchitecture __label__HeterogeneousSource __label__AutonomousSource __label__DistributedPlatform __label__AnonymousPlatform __label__OnlineAnalyticalProcess __label__CognitiveSolution __label__BigDataIndustry __label__DataAnalyticsProcess __label__NestedAutomaticServiceComposition __label__Nasc __label__CrossIndustryStandardPlatformForDataMining __label__MultidisciplinedProblem __label__DataScienceProcess __label__InnovativeAccumulator __label__MultidimensionalDataSet __label__BigDataAnalyticalProcess __label__CrispdmProcess __label__BigDataAnalytics __label__NestedAutomaticServiceComposition __label__DataMining __label__Architecture __label__Crsipdm  intelligent big data analysis architecture based on automatic service composition big data contains massive information which are generating from heterogeneous autonomous sources with distributed and anonymous platforms since it raises extreme challenge to organizations to store and process these data conventional pathway of store and process is happening as collection of manual steps and it is consuming various resources an automated realtime and online analytical process is the most cognitive solution therefore it needs state of the art approach to overcome barriers and concerns currently facing by the big data industry in this paper we proposed a novel architecture to automate data analytics process using nested automatic service composition nasc and cross industry standard platform for data mining crispdm as main based technologies of the solution nasc is well defined scalable technology to automate multidisciplined problems domains since crispdm also a wellknown data science process which can be used as innovative accumulator of multidimensional data sets crispdm will be mapped with big data analytical process and nasc will automate the crispdm process in an intelligent and innovative way 
__label__Kernel __label__SupportVectorMachines __label__SupportVectorMachineClassification __label__GeneticAlgorithms __label__EducationalInstitutions __label__MachineLearning __label__LagrangianFunctions __label__StatisticalLearning __label__PatternRecognition __label__MachineLearningAlgorithms __label__SupportVectorMachines __label__GeneticAlgorithms __label__LearningArtificialIntelligence __label__PatternClassification __label__SupportVectorMachines __label__StatisticalLearningAlgorithm __label__PatternRecognition __label__PatternClassification __label__CrossbreedGeneticAlgorithm __label__FitnessFunction __label__OptimalKernelFunction __label__SupportVectorMachines __label__KernelFunction __label__GeneticAlgorithm  a method to choose kernel function and its parameters for support vector machines the support vector machines are the new statistical learning algorithm which is developed in recent years they have some advantages in many regions like pattern recognition the kernel function is important to its classification ability this paper presents a crossbreed genetic algorithm based method to choose the kernel function and its parameters the crossbreed genetic algorithm uses two fitness functions which are produced according to the two criterion of svm s performance the experiments proved that this algorithm can find effectively the optimal kernel function and its parameters and it is helpful to increase the support vector machines performance in fact 
