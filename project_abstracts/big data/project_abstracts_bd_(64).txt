My (fair) big data&&&&&Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.&&&&&Big Data,Pipelines,Ontologies,Metadata,Google$$$$$Big Data,data integrity,meta data,ontologies (artificial intelligence)$$$$$processing pipeline,Big Data sources,ontologies,integration systems,formal quality evaluation,integrated data,data confidentiality,Big Data quality dimension,policy making,strict requirement,quantitative quality information,high quality information,data quality issue$$$$$Quality-driven policies,Big Data pipeline,ontology-based quality checking,Big Data confidentiality