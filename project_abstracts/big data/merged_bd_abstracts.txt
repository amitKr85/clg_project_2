Big data analytic empowered grid applications — Is PMU a big data issue?&&&&&With the proliferation of digital measurement devices, such as smart meter on the distribution systems and phasor measurement units on the transmission systems, power companies find themselves inundated with increasingly growing data and long for efficient tools and analytical techniques to identify, digest and utilize critical information to improve the efficiency and reliability of grid operations. Many power researchers believe that the PMU related power system analytics falls under the category of Big Data Science and are keen to apply typical technologies for solution, including machine learning, data mining, cloud based computation and so on. This paper explores the reason behind such presumption, challenges to deal with PMU data, and trends of analytical techniques.&&&&&Phasor measurement units,Big data,Power system stability,Monitoring,Data visualization,Analytical models,Software$$$$$Big Data,cloud computing,data mining,learning (artificial intelligence),phasor measurement,power distribution reliability,power engineering computing,power grids,smart meters,transmission networks$$$$$analytic empowered grid application,digital measurement devices,smart meter,distribution systems,phasor measurement units,PMU data,transmission systems,power companies,grid operations,power system analytics,Big Data science,machine learning,data mining,cloud based computation$$$$$Synchrophasor,measurements,grid operation,big data,power system analysis#####Big data technology and ethics considerations in customer behavior and customer feedback mining&&&&&The rapidly increasing attention to customer behavior and satisfaction by department stores and commercial companies and development in social media as well as online systems has promoted production and research in user engagement pattern recognition, user network analysis, topic detection from customer feedback, text-based sentiment analysis, etc. With the development of Internet of Things and social media network, ethics consideration has also playing an important role in application of big data, especially customer behavior and feedback mining. Our proposed novel system, which extracts the important topics or issues from Skype customer feedback sources and measures the emotion associated with those topics using Vibe metric, can be a good example in this area. Unlike other previous research, which has focused on extracting the user sentiments either globally or in separate topics, our work focuses on tracking the correlated emotional trajectories across all the important issues from Skype customer feedback over time. Moreover, it also provides a platform for both studying customer emotions and tracking how the emotions regarding different important topics correlatively change over time by leveraging unstructured textual customer feedback data and structured user activity telemetry data.&&&&&Data mining,Ethics,Social network services,Big Data,Market research,Computational linguistics$$$$$Big Data,consumer behaviour,customer satisfaction,data mining,ethical aspects,sentiment analysis,social networking (online)$$$$$structured user activity telemetry data,ethics considerations,customer behavior,customer feedback mining,user engagement pattern recognition,user network analysis,topic detection,sentiment analysis,social media network,ethics consideration,user sentiments,Skype customer feedback,unstructured textual customer feedback data,customer emotions,Big data technology,customer satisfaction,Vibe metric$$$$$Big Data,Ethics considerations,customer feedback,user activity,changing emotion#####Big Data as the Big Game Changer&&&&&Big Data is the phenomenon of the Information era. Big Data is a new dimension to explore, collecting Big Data we fix the time. Big Data has some functions, including impact on society, form spatio-temporal structures, change the world and future, and integration society with IT technologies. Most important aspect is risk in Cloud computing. To leverage risks, secure Cloud services and get additional benefits an Integrated Approach should be applied. It is important to separate the various kinds of “Security” needs when considering Cloud computing issues. Also Security Analyst should be included into Data Science Team. Data-driven economy is based on three points: open data, legislation for Big Data, and education. For students is very important practical training that engages students into the culture of Big Data Analytics. This opportunity provides the EMC Academic Alliance Russia & CIS through the establishment of ad-hoc Big Data Analytics Teams among universities. The results of the first stage of launched in 2015 the Big Data Analytics Multicenter Study are presented.&&&&&Terrorism,Big data,Force,Blogs$$$$$Big Data,cloud computing,data analysis,security of data$$$$$big game changer,information era,spatio-temporal structures,IT technologies,cloud computing,risks,secure cloud services,security,data-driven economy,open data,legislation,education,Big Data analytics,Big Data-driven world,Big Data-driven ideology$$$$$Big Data,Data Analytics Multicenter Study,Cloud computing,Security Integrated Approach,Federation Business Data Lake#####SMEs transformation through usage and understanding of big data case study: Spanish restaurant industry&&&&&Big Data is being widely used and it is contributing to an enhancement of business' understanding of their consumers and overall economic context. However, SMEs are not always able to invest in complex data management systems, nor do they have the qualified personnel to understand and manage large quantities of data to obtain valuable information. In order to enable SMEs' usage and transforming their business models, it is necessary to understand which factors can contribute. This paper analyzes the case of Spanish Restaurant SMEs and the socio-economic context of Spain as a promoter for SMEs Big Data usage.&&&&&Big Data,Tools,Industries,Government,Investment,Google$$$$$Big Data,business data processing,catering industry,personnel,small-to-medium enterprises,socio-economic effects$$$$$Spanish restaurant industry,complex data management systems,qualified personnel,business models,socio-economic context,Big Data,SME transformation$$$$$Big Data,SMEs,Data Management,Knowledge-based economy#####Workflow Transformation for Real-Time Big Data Processing&&&&&With the explosion of big data, processing and analyzing large numbers of continuous data streams in real-time, such as social media stream, sensor data streams, log streams, stock exchanges streams, etc., has become a crucial requirement for many scientific and industrial applications in recent years. Increased volume of streaming data as well as the demand for more complex real-time analytics require for execution of processing pipelines among heterogeneous event processing engines as a workflow. In this paper, we propose a workflow transformation for cost minimization in real-time big data processing on the heterogeneous systems. We first give the definition of stream-based workflow, and then we define eight different patterns as rules for workflow transformation, next, we give our workflow transformation algorithm based on our designed rules. Finally, our experiment shows that our proposed workflow transformation method can reduce the communication and computation cost effectively.&&&&&Big data,Real-time systems,Minimization,Computational modeling,Data models,Engines$$$$$Big Data,cost reduction$$$$$workflow transformation,real-time big data processing,processing pipelines,heterogeneous event processing engines,cost minimization,heterogeneous systems,stream-based workflow,communication cost reduction,computation cost reduction$$$$$big data processing,stream data,workflow transformation,geo-distributed data centers#####Enabling scientific data storage and processing on big-data systems&&&&&Big-data systems are increasingly important for solving the data-driven problems in many science domains including geosciences. However, existing big-data systems cannot support the self-describing data formats such as NetCDF which are commonly used by scientific communities for data distribution and sharing. This limitation presents a serious hurdle to the further adoption of big-data systems by science domains and prevents scientific users from leveraging these systems to improve their productivity. This paper presents a solution to this problem by enabling big-data systems to directly store and process scientific data. Specifically, it enables Hadoop to efficiently store NetCDF data on HDFS and process them in MapReduce using convenient APIs. It also enables Hive to support standard queries on NetCDF data, transparently to users. The paper also presents an evaluation of the proposed solution using several representative queries on a typical geoscientific dataset. The results show that the proposed approach achieves substantial speedup (up to 20 times) and space saving (83% reduction), compared to the traditional approach which has to convert NetCDF data to CSV format for Hadoop and Hive to use them.&&&&&Meteorology,Geology,File systems,Big data,Computational modeling,Data models,Distributed databases$$$$$application program interfaces,Big Data,data mining,storage management$$$$$scientific data storage,big-data system,data-driven problem,geosciences,NetCDF,Hadoop,MapReduce,API,Hive$$$$$Scientific data,big data,NetCDF,Hadoop#####Data quality issues in big data&&&&&Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.&&&&&Databases,Big data,Measurement,Context,Cleaning,Biology,Computers$$$$$Big Data,biology computing,data integration,database management systems$$$$$big data applications,bad data quality,data quality issues,data integration issues,biological databases,attendant data quality issues$$$$$Data quality,big data,biological data,information quality#####Confrontation and oppurtunities of big data — A survey&&&&&Data plays a vital role in many frontiers like organizations, industry, business, scientific research, any discipline and it is immeasurable. Data that is stored electronically draws huge attention. Excessive data is making great troubles for the people to think over the necessity to store data which is used to grow an organization further and is also important in decision making too. Major challenge arises in storing and retrieving data, thereby Big Data come into picture. This paper provides a brief survey on variety of data available, preserving high graded data, how the data can be processed and analyzed. On the other hand, this paper also focuses on, the existence, few challenges and technologies used for Big Data.&&&&&Handheld computers,Big Data,Computational intelligence,Conferences$$$$$Big Data,decision making$$$$$decision making,high graded data,retrieving data,excessive data,big data$$$$$Veracity,Hadoop,Big Data,Tableau,Machine,Learning,Skytree#####Hyperbolic tangent activation function on FIMT-DD algorithm analysis for airline big data&&&&&In recent years, big data has become hot and challenging issue. The use of big data term in many areas provided positive impact. In traffic area included road traffic, railway traffic, and airline traffic, there is huge information can be obtained. The needed of big data analytics to process the data quickly and give an accurate prediction about it, became essential. The FIMT-DD with hyperbolic tangent (tanh) algorithm is proposed to predict the airline big data. The simulation time of FIMT-DD-tanh is almost the same with original FIMT-DD. Based on this analysis and evaluation, in data stream mining evaluation, FIMT-DD-tanh could be able to decrease the error value in airline big dataset.&&&&&Big Data,data analysis,data mining,traffic engineering computing$$$$$road traffic,railway traffic,airline traffic,big data analytics,hyperbolic tangent algorithm,airline big data,FIMT-DD-tanh,original FIMT-DD,data stream mining evaluation,airline big dataset,hyperbolic tangent activation function,FIMT-DD algorithm analysis,big data term,traffic area$$$$$Hyperbolic Tangent,Activation Function,FIMT-DD,Airline data,Big Data#####The Reform of Vocational Colleges' Teaching Method in the Age of Big Data -- Based on PHP Programming&&&&&In the big data era, due to the disadvantages of traditional PHP Programming teaching, the author thinks that the teacher's teaching method should be improved. The improvement strategies are as follows: Step 1: collection and analysis of data before teaching -- before teaching, the teacher should firstly know about students' mastery of leading courses, Step 2: analysis and prediction of students' learning behavior during teaching- the teacher can update his teaching schedule and strategies by analyzing the data from teaching platform, Step 3: Evaluation of students' learning after teaching -- the teacher can offer guidance individually so as to teach students according to their aptitude. The kind of data collection, data analysis, experience summary and intensive training process is also a process of improving students' learning efficiency and teachers' professional competence. According to the feedback from students, the new teaching method produces a remarkable effect and those graduates who go in for website development have been highly thought of by employing units.&&&&&Big data,Programming profession,Training,Databases,Data analysis$$$$$Big Data,computer science education,data analysis,educational institutions,programming,teaching,vocational training$$$$$vocational college teaching method reform,Big Data,PHP programming,student learning behavior,data analysis$$$$$PHP Programming,Big Data,language points,teaching platform#####Modeling and analysis of material supply network based on big data packed with traffic&&&&&Big data is an important tool for intelligent supply, which can effectively improve the efficiency of the existing installed security. This paper uses complex network analysis and modeling method, establish the whole supply network model is put forward in the future is based on business data, some optimization targets on the basis of this, to provide a reference for further improving our army is security efficiency.&&&&&Business,Complex networks,Big Data,Data models,Analytical models,Optimization$$$$$Big Data,business data processing,complex networks,optimisation,security of data$$$$$business data,security efficiency,big data,intelligent supply,complex network analysis,material supply network model,optimization$$$$$big data,material supply network,modeling and analysis#####Data quality in big data processing: Issues, solutions and open problems&&&&&With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.&&&&&Big Data,Data integrity,Data analysis,Social network services,Data preprocessing,Internet$$$$$Big Data,data analysis,recommender systems$$$$$data collection,data preprocessing,data storage,big data processing,data quality,data analysis,recommendation systems,prediction systems$$$$$Big Data,Big data processing,Data Quality,Recommendation system,Prediction system#####TV ratings vs. social media engagement: Big social data analytics of the Scandinavian TV talk show Skavlan&&&&&This paper explores the relationship between TV viewership ratings for Scandinavian's most popular talk show, Skavlan and public opinions expressed on its Facebook page. The research aim is to examine whether the activity on social media affects the number of viewers per episode of Skavlan, how the viewers are affected by discussions on the Talk Show, and whether this creates debate on social media afterwards. By analyzing TV viewer ratings of Skavlan talk show, Facebook activity and text classification of Facebook posts and comments with respect to type of emotions and brand sentiment, this paper identifes patterns in the users' real-world and digital world behaviour.&&&&&TV,Facebook,Organizations,Big data,Data analysis$$$$$Big Data,data analysis,pattern classification,sentiment analysis,social networking (online)$$$$$social media engagement,big social data analytics,Scandinavian TV talk show,TV viewership ratings,public opinions,Facebook page,Skavlan talk show,Facebook activity,text classification,Facebook posts,Facebook comments,emotions,brand sentiment,digital world behaviour$$$$$Television Viewer Behaviour,Skavlan Talk Show,Big Social Data,Facebook,Big Data Analytics,Text Classification,Social Set Analysis#####Managing data lakes in big data era: What's a data lake and why has it became popular in data management ecosystem&&&&&The concept of a data lake is emerging as a popular way to organize and build the next generation of systems to master new big data challenges, but there are lots of concerns and questions for large enterprises to implement data lakes. The paper discusses the concept of data lakes and shares the author's thoughts and practices of data lakes.&&&&&Lakes,Data warehouses,Big data,Companies,Ecosystems$$$$$Big Data,data handling$$$$$data lake,Big Data era,data management ecosystem,Apache Hadoop$$$$$Big Data,Data Lake,Hadoop,data management ecosystem,eneterpirse architecture#####Impacts of public transportation fare reduction policy on urban public transport sharing rate based on big data analysis&&&&&Urban transport is an important support system to the city. With the city's development, traffic congestion has become a major traffic problem nowadays and it is badly in need of solutions. Big data analysis has been widely used in the domain of transportation in recent years and it does great help to find solutions to different kinds of problems from historical data. In order to solve the urban traffic problems fundamentally, developing the public traffic is one of the major effective ways and bus priority policies and strategies are such important measures that they have contributions to the increase of public transport sharing rate. This paper mainly studied the influences of two bus fare adjustment policies in Beijing on urban public transport sharing rate based on big data analysis through the computer software SPSS, and then put forward corresponding recommendations to the reform of bus fares in Beijing.&&&&&Urban areas,Transportation,Data models,Analytical models,Mathematical model,Rails,Big Data$$$$$Big Data,data analysis,public transport,road traffic,traffic engineering computing$$$$$traffic congestion,big data analysis,urban traffic problems,bus priority policies,urban public transport sharing rate,public transportation fare reduction policy,Beijing,computer software SPSS$$$$$big data analysis,urban public transport,fare reduction policy,data fitting model,SPSS#####Research on reliability evaluation of big data system&&&&&The application of big data system is now more pervasive. The reliability of the large data system is crucial to both the academic and the industry. However, to date there are few studies on the reliability of the big data system, and lack of evaluation model. This paper uses the fault tree to model the reliability of the big data system on the cloud. The type of faults is summarized and the cause of fault is analyzed by experiments. The fault tree analysis (FTA) is used to evaluate the reliability of the big data system, which can provide reference for the fault processing and quality assurance of big data system.&&&&&Big Data,Fault trees,Software,Software reliability,Data models,Hardware$$$$$Big Data,cloud computing,fault trees,quality assurance$$$$$big data system,reliability evaluation,large data system reliability,evaluation model,fault tree analysis,FTA,fault processing,quality assurance$$$$$big data system,reliability,fault tree,evaluation#####Study on the application of big data in accurate marketing of cross-border e-commerce in China&&&&&With the development of strategic goals of “The Belt and Road”, cross-border e-commerce has developed prosperously in China. The application of big data has been a vital strategic measure for cross-border e-commerce in “Internet+” era. Based on the motivation of many cross-border e-commerce enterprises to apply big data in marketing, this paper researches the procedures and fruits of applying big data into the accurate marketing of cross-border e-commerce combining the qualitative methods and literature review. It concluded that applying big data to cross-border marketing not only provides consumers' convenience but also enterprises' benefits of various aspects. Therefore, the research on the application of big data in accurate marketing of cross-border e-commerce bears practical significance.&&&&&Conferences,Big Data$$$$$Big Data,electronic commerce,industrial economics,Internet,marketing data processing$$$$$Big Data,cross-border e-commerce enterprise marketing,China,qualitative methods,consumer convenience$$$$$big data,accurate marketing,cross-border e-commerce#####A Data Science Solution for Mining Interesting Patterns from Uncertain Big Data&&&&&Nowadays, high volumes of valuable uncertain data can be easily collected or generated at high velocity in many real-life applications. Mining these uncertain Big data is computationally intensive due to the presence of existential probability values associated with items in every transaction in the uncertain data. Each existential probability value expresses the likelihood of that item to be present in a particular transaction in the Big data. In some situations, users may be interested in mining all frequent patterns from these uncertain Big data, in other situations, users may be interested in only a tiny portion of these mined patterns. To reduce the computation and to focus the mining for the latter situations, we propose a tree-based algorithm that (i) allows users to express the patterns to be mined according to their intention via the use of constraints and (ii) uses MapReduce to mine uncertain Big data for only those frequent patterns that satisfy user-specified constraints. Experimental results show the effectiveness of our algorithm in mining interesting patterns from uncertain Big data.&&&&&Big data,Databases,Association rules,Data models,Program processors,Computational modeling$$$$$Big Data,data mining,parallel processing,probability$$$$$data science solution,uncertain Big Data,existential probability values,tree-based algorithm,MapReduce,frequent pattern mining$$$$$Big data and cloud computing,Big data analytics,Big data applications,Big data mining,MapReduce#####A big data architecture for learning analytics in higher education&&&&&Data with high volume, velocity, variety and veracity brings the new experience curve of analytics. Big data in higher education comes from different sources that include blogs, social networks, student information systems, learning management systems, research, and other machine-generated data. Once the data is analysed it promises better student placement processes; more accurate enrolment forecasts, and early warning systems that identify and assist students at-risk of failing or dropping out. Big data is becoming a key to creating competitive advantages in higher education. Like with any organization, traditional data processing and analysis of structured and unstructured data using RDBMS and data warehousing no longer satisfy big data challenges. The lack of adequate conceptual architectures for big data tailored for institutions of higher education has led to many failures to produce meaningful, accessible, and timely information for decision making. Therefore, this calls for the development of conceptual architectures for big data in higher education. This paper presents an architecture for big data analytics in higher education.&&&&&Education,Data mining,Big Data,Computer architecture,Decision making,Data visualization,Data analysis$$$$$Big Data,data analysis,data mining,data warehouses,educational administrative data processing,educational institutions,further education,information systems,learning (artificial intelligence)$$$$$higher education,student information systems,data warehousing,big data analytics,big data architecture,data processing,data structure,educational data mining$$$$$learning analytics,big data,educational data mining#####Framework of integrated big data: A review&&&&&Currently, how to deeply distill potential attributes of big data has become a great challenge for structured, semi-structured and unstructured data (SSU data) with a unified model. Structured data refers to any data that resides in a fixed field within a record or file including data contained in relational databases and spreadsheets. Unstructured data refers to data from text, pictures, audio, video, and other sources that do not fit into a relational database. Semi-structured data is information that doesn't reside in a relational database but that does have some organizational properties that make it easier to analyze, such as XML, and HTML documents. In this paper, we present a literature survey and a framework, namely integrated big data (IBD), which aims at exploring the approaches for constructing a universal IBD model, including representation, storage and management, computation, and visual analysis. Firstly, we present a systematic framework to decompose big data analytics into four modules. Next, we present a detailed survey of numerous approaches for these four modules. The main contributions of this paper are summarized in two dimensions. First, we propose a novel integrated big data framework for unified big data representation, storage, computation, and visual analysis. Second, we present the possible future methods in realizing the framework by reviewing methods. Through this paper, we would like to point out a promising research direction in unified investigation and application of big data.&&&&&Big data,Data models,Tensile stress,Databases,Computational modeling,Visualization,Data visualization$$$$$Big Data,data analysis,data structures,relational databases$$$$$integrated big data,structured data,semistructured data,unstructured data,SSU data,unified model,relational databases,spreadsheets,universal IBD model,visual analysis,big data analytics,integrated big data framework,unified big data representation$$$$$big data,data analytics,data storage,unified representation#####A Big Data Framework for Electric Power Data Quality Assessment&&&&&Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.&&&&&Big Data,Data integrity,Power grids,History,Real-time systems,Sensors$$$$$Big Data,data analysis,electricity supply industry,power engineering computing,power grids$$$$$big data framework,electric power data quality assessment,low-quality data,data related activities,electric power industry,electric power applications,special characteristic data assessment frameworks,big data techniques,real-time data,history data,electric power big data assessment,power grid$$$$$data quality,electric power data,data quality assessment,big data,framework#####Research on high-risk student prediction based on big data&&&&&In recent years, the development of smart phones, Internet of things and cloud computing provides massive data for the study of education big data. However, the current research on education big data is not enough, as the data is small, the processing method is simple and the scope of application is narrow. In this paper, big data technology is adopted and the massive data of multiple data sources are utilized, the model features are extracted by well-designed data processing methods, and a more general high-risk student prediction model is constructed. The research of this paper can broaden the research methods and application scope of education big data, so it has strong theoretical value and important practical value.&&&&&Feature extraction,Predictive models,Data models,Big Data,Web sites,Training$$$$$Big Data,cloud computing,computer aided instruction,feature extraction,Internet of Things,smart phones$$$$$education Big Data,high-risk student prediction model,feature extraction,smart phone development,Internet of Things development,cloud computing development$$$$$high-risk,prediction,big data,model#####Bad big data science&&&&&As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.&&&&&Big data,Metadata,Distributed databases,Stakeholders,Data science$$$$$Big Data,data analysis,data warehouses,distributed databases$$$$$bad big data science,hardware technologies,software technologies,data sets,domain-specific knowledge,technical knowledge,statistical knowledge,data warehousing,data quality,distributed database projects$$$$$Big Data,Data Quality,Data Warehousing,Distributed Database,Metadata#####Multimedia Big Data Computing for In-Depth Event Analysis&&&&&While the most part of "big data" systems target text-based analytics, multimedia data, which makes up about2/3 of internet traffic, provide unprecedented opportunities for understanding and responding to real world situations and challenges. Multimedia Big Data Computing is the new topic that focus on all aspects of distributed computing systems that enable massive scale image and video analytics. During the course of this paper we describe BPEM (Big Picture Event Monitor), a Multimedia Big Data Computing framework that operates over streams of digital photos generated by online communities, and enables monitoring the relationship between real world events and social media user reaction in real-time. As a case example, the paper examines publicly available social media data that relate to the Mobile World Congress 2014 that has been harvested and analyzed using the described system.&&&&&Streaming media,Multimedia communication,Big data,Mobile communication,Multimedia computing,Real-time systems,Sparks$$$$$Big Data,Internet,multimedia computing,text analysis$$$$$multimedia big data computing,in-depth event analysis,big data systems,text-based analytics,multimedia data,Internet traffic,multimedia Big Data computing,distributed computing systems,massive scale image,video analytics,big picture event monitor,digital photos,online communities,social media user reaction,social media data,Mobile World Congress$$$$$big data,multimedia,spark,movile congress,barcelona,multimodal,image,analysis#####R-tool: Data analytic framework for big data&&&&&This research suggests a framework of R tool. R tool frame work is used for analyzing big data in cloud computing. The aim is to identify the challenges for analyzing big data. R is a statistical programming language which is behind statistics, analytics and visualization. Today's data scientist and business leader uses R to make power business decisions. The RFramework is open source and flexible. R includes different packages which are useful in analysis of data. R Framework contains Deploy R server, Deploy R repository and Deploy R API's which are used to upload and verily data. In R we can write our scripts and we can also upload different format files. We can also link RFramework with other languages such as java, .NET. R is a flexible framework and it is capable for analyzing various types of data which is available on cloud.&&&&&Data analysis,Big data,Computer languages,Business,Facebook,Data visualization$$$$$application program interfaces,Big Data,cloud computing,data analysis,data visualisation,public domain software,statistical analysis$$$$$data analytic framework,R tool framework,Big data analysis,cloud computing,statistical programming language,open source,R-API$$$$$Big Data,Data Analytics,Rframework,Statistical computing,data processing#####Challenges of Software Testing for Astronomical Big Data&&&&&Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.&&&&&Big Data,Software testing,Software,Astronomy,Data analysis,Distributed databases$$$$$astronomy computing,Big Data,program testing$$$$$software testing,astronomical big data,data quality,scientific findings,software testers$$$$$software testing; big data; astronomical software#####Challenges and opportunities of complex equipment operational reliability technology in industrial big data age&&&&&Large and complex equipment reliability evaluation is extremely dependent on equipment reliability experiment data, maintenance records, and failure data. With the informationalization and intellectualization of equipment (such as CNC machine tools, shield machines, and weaponry), large amounts of data (big data) will be produced during the equipment's operation. Abundant data provide a strong support for equipment operational reliability analysis in the industrial big data age, but also pose a huge challenge for reliability analysis. This paper first explores the opportunities provided by big data to promote the reliability analysis and assessment of complex equipment. Then, we mainly focus on the remaining challenges of equipment operational reliability assessment using the industrial big data method, such as the fact that most of the data reflect an intermediate state (incomplete failure state) of the equipment. We also consider a way to analyze the multiple-states of the equipment operation and correlate the multiple failure modes of the equipment operation using the big data. Moreover, a big data analysis method for calculating the reliability and predicting the residual life of gradual systems is discussed, along with a method for combining the traditional reliability calculation theory with the big data theory. All of these issues provide a significant challenge for the reliability analysis of complex equipment in the big data age.&&&&&Big Data,Reliability theory,Reliability engineering,Degradation,Manufacturing,Industries$$$$$Big Data,data analysis,failure (mechanical),machine tools,maintenance engineering,production engineering computing,reliability$$$$$equipment reliability experiment data,equipment operational reliability analysis,equipment operational reliability assessment,industrial big data method,big data analysis method,big data theory,Big Data age,equipment operational reliability technology,equipment reliability evaluation,reliability calculation theory$$$$$reliability,big data,complex equipment,residual life,gradual systems#####Technical aspects and case study of big data based condition monitoring of power apparatuses&&&&&This paper presents the key technologies of big data based condition monitoring of power apparatuses. Firstly, the characteristics of big data and big data of power system are discussed and the application prospects of big data based condition monitoring of power apparatuses is presented and the key technologies of the system is discussed, in terms of big data analyzing technologies, big data management technologies, big data processing technologies and big data visualization technologies. Thirdly, big data based condition assessment techniques of power apparatuses are discussed, including data fusion of signal collected from different sensors, historical trending analysis and association analysis of combined equipments. Finally, to further introduce big data techniques, an integrated condition monitoring system of transformer, GIS and power cable is presented, including system hardware structures and big data based condition assessments.&&&&&Big data,Monitoring,Condition monitoring,Partial discharges,Data visualization,Data integration,Power systems$$$$$Big Data,condition monitoring,gas insulated switchgear,power apparatus,power cables,power transformers,sensor fusion$$$$$big data based condition monitoring,power apparatuses,power system,big data analyzing technologies,big data management technologies,big data processing technologies,big data visualization technologies,data fusion,historical trending analysis,association analysis,integrated condition monitoring system,transformer,GIS,power cable,system hardware structures,big data based condition assessments$$$$$Big data,condition monitoring,power apparatuses,key Technologies,data fusion#####Comparative study of tools for big data analytics: An analytical study&&&&&Data sets grow rapidly in different forms due to digitalization. When the data or information sets which are too large and are complex in nature in which traditional data processing techniques are not able to deal with those complex data, then that data is called B ig data. Researchers, scientists, business organizations, government agencies, advertising agencies, medical researchers often come across more difficulty in dealing with data for any decision making. The data available for research has to be processed by using various techniques of data analytics which is called Big Data Analytics. These techniques helps in getting benefits in dealing with massive volume of either unstructured, structured or semi-structured data content that is fast changing nature, also not possible to process using conventional database techniques. This paper discusses the major utilization of big data analytics by comparing different tools available for big data validation. Furthermore, this paper discusses the case study conducted to overcome the big data challenges and needs.&&&&&Tools,Big Data,Data visualization,Data analysis,Data mining,Automation$$$$$Big Data,data analysis,decision making$$$$$data content,Big Data analytics,data processing techniques,Big Data validation,decision making$$$$$Big Data Analytics,Data Sets,Challenges,big data validation,unstructured data,multiple techniques,semi-structured data#####Big data application in job trend analysis&&&&&Big data technologies are now being largely applied to several industries including the recruiting industry. In this research, big data techniques such as Hadoop and Tableau are applied to identify job trend analysis in New York. Results indicate time-analysis and patterns in specific locations of job vacancies in New York.&&&&&Big data,Data visualization,Market research,Industries,Portals,Data analysis$$$$$Big Data,parallel processing,recruitment$$$$$big data application,job trend analysis,recruiting industry,Hadoop,Tableau,New York,job vacancies$$$$$data,Hadoop,Tableau,visualization,recruiting#####A Workflow Model for Adaptive Analytics on Big Data&&&&&The analysis of Big Data needs to be performed on a range of data stores, both traditional and modern, on data sources that are heterogeneous in their schemas and formats, and on a diversity of query engines. The users that need to perform such data analysis may have several roles, like, business analysts, engineers, end-users etc. Therefore Big Data analytics should be expressed and executed in a manner that is adaptive to the user and the system. We discuss the principles of adaptive analytics and we summarise ongoing work on the creation of a workflow model and, furthermore, a workflow management system that enables the creation and the execution of adaptive analytics. The model focuses on the separation of task dependencies from task functionality and the decoupling of application logic from implementation. Our motivation and applications derive from real use cases of the telecommunication domain.&&&&&Adaptation models,Optimization,Analytical models,Engines,Data models,Adaptive systems,Big data$$$$$Big Data,data analysis,workflow management software$$$$$workflow model,adaptive analytics,Big Data analytics,query engines,workflow management system$$$$$big data analytics,adaptive analytics,online analytics#####Storing and manipulating environmental big data with JASMIN&&&&&JASMIN is a super-data-cluster designed to provide a high-performance high-volume data analysis environment for the UK environmental science community. Thus far JASMIN has been used primarily by the atmospheric science and earth observation communities, both to support their direct scientific workflow, and the curation of data products in the STFC Centre for Environmental Data Archival (CEDA). Initial JASMIN configuration and first experiences are reported here. Useful improvements in scientific workflow are presented. It is clear from the explosive growth in stored data and use that there was a pent up demand for a suitable big-data analysis environment. This demand is not yet satisfied, in part because JASMIN does not yet have enough compute, the storage is fully allocated, and not all software needs are met. Plans to address these constraints are introduced.&&&&&Communities,Meteorology,Earth,Data models,Data handling,Virtual machining,Data analysis$$$$$Big Data,data analysis,environmental science computing,pattern clustering,storage management$$$$$environmental big data,big data storage,big data manipulation,JASMIN super-data-cluster,high-performance high-volume data analysis,UK environmental science community,United Kingdom,scientific workflow,data products,STFC Centre for Environmental Data Archival,big data analysis,software needs$$$$$Curation,Climate,Earth Observation,Big Data,Cloud Computing#####Towards a requirements engineering artefact model in the context of big data software development projects: Research in progress&&&&&There is ample literature that suggests that the field of Big Data is growing rapidly. Also, there is emerging literature on the need to create end-user Big Data applications, as distinct from “data analytics” that typically employs machine learning algorithms to find value in large datasets for the stakeholder. A solid foundation for creating sound applications is a thorough understanding of domain and artefact models that embody artefact types and activities involved in a software project. This paper focuses on the Requirements Engineering (RE) aspect of a Big Data software project. Currently, there are no known RE artefact models to support RE process design and project understanding. To fill this void, this paper proposes a RE artefact model for Big Data end-user applications (BD-REAM). The paper also describes a method for creating the artefact model, including the basic elements and inter-relationships involved in the model.&&&&&Big Data,Software,Requirements engineering,Data models,Unified modeling language,Software engineering,Stakeholders$$$$$Big Data,formal specification$$$$$end-user Big Data applications,data analytics,process design,requirements engineering artefact model,Big Data software development projects,RE artefact models,BD-REAM$$$$$Big Data,Requirements Engineering,Artefact Model,Big Data Requirements Engineering Artfect Model#####Genetic Algorithm Based Data-Aware Group Scheduling for Big Data Clouds&&&&&Cloud computing is a promising cost efficient service oriented computing platform in the fields of science, engineering, business and social networking for delivering the resources on demand. Big Data Clouds is a new generation data analytics platform using Cloud computing as a back end technologies, for information mining, knowledge discovery and decision making based on statistical and empirical tools. MapReduce scheduling models for Big Data computing operate in the cluster mode, where the data nodes are pre-configured with the computing facility for processing. These MapReduce models are based on compute push model-pushing the logic to the data node for analysis, which is primarily for minimizing or eliminating data migration overheads between computing resources and data nodes. Such models, however, substantially perform well in the cluster setups, but are infelicitous for the platforms having the decoupled data storage and computing resources. In this paper, we propose a Genetic Algorithm based scheduler for such Big Data Cloud where decoupled computational and data services are offered as services. The approach is based on evolutionary methods focussed on data dependencies, computational resources and effective utilization of bandwidth thus achieving higher throughputs.&&&&&Biological cells,Genetic algorithms,Big data,Processor scheduling,Scheduling,Data models,Cloud computing$$$$$Big Data,cloud computing,data analysis,data mining,decision making,genetic algorithms,parallel processing,pattern clustering,processor scheduling,service-oriented architecture,statistical analysis$$$$$genetic algorithm based data aware group scheduling,Big data cloud computing,cost efficient service oriented computing,data analytics,information mining,knowledge discovery,decision making,statistical tool,empirical tool,MapReduce scheduling model,cluster mode,data node preconfiguration,push model-pushing,decoupled computational service,data service,evolutionary method,data dependency,computational resource,effective bandwidth utilisation,throughput$$$$$Big Data,Cloud computing,Data Intensive Scheduling,Genetic algorithms,Big Data Clouds#####Big data challenges in railway engineering&&&&&As Big Data becomes part of railroad data analysis, there are many challenges which need to be addressed by the railway industry. This extended abstract highlights some of the challenges from specific examples in railway engineering. This work does not present the challenges of dealing with Big Data in general which is beyond the scope of this paper. The examples provided in this extended abstract cover both the engineering and the management of railroad applications.&&&&&Big data,Rails,Rail transportation,Databases,Data analysis,Railway engineering,Geometry$$$$$Big Data,data analysis,railway engineering,railway industry$$$$$railway engineering,Big Data,railroad data analysis,railway industry$$$$$Big Data,Railway,Rail Defects,Geometry Defects,Privacy#####The Management of Application of Big Data in Internet of Thing in Environmental Protection in China&&&&&The internet of thing in environmental protection provide a full range of monitor way to control environment from source. It becomes an important big data source. And how to manage and apply big data is a crucial problem. In this paper, we focus on the features, framework of management of big data of IoT in environmental protection. And outline the characteristics, areas and challenges of application of big data of IoT in environmental protection in China.&&&&&Big data,Pollution,Monitoring,Sensors,Cloud computing,Data mining$$$$$Big Data,environmental management,environmental science computing,Internet of Things$$$$$Big Data,Internet of Things,IoT,environmental protection,China$$$$$internet of things in environmental protection,big data,cloud computing#####An Analysis of College Students' Deep Entrepreneurship Patterns Based on Spatio-Temporal Big Data Flow&&&&&Students' complicated behaviors often lead to fluctuations in the status of the job market and destroys the stable operation of the job market. This paper introduces the theory of spatio-temporal big data flow, constructs an adaptive student entrepreneurial behavior evaluation method for the status of job market, uses association rules reduction and association rules importance degree method to excavate the data of college students' entrepreneurial behaviors and job market status, analyzes the association degree of college students' entrepreneurial behaviors on job market status changes so to adaptively build the evaluation indicator and weight and accurately quantify the influence degree of students' entrepreneurial behaviors on job market status changes. The experimental results show that the evaluation method can help to accurately find out the students who caused the job market status changes and their behaviors, and effectively support the management and control of students' entrepreneurial behaviors.&&&&&Conferences,Transportation,Big Data,Smart cities$$$$$Big Data,business data processing,data mining,innovation management$$$$$college students,spatio-temporal big data flow,adaptive student entrepreneurial behavior evaluation method,association rules reduction,association rules importance degree method,association degree,evaluation indicator,evaluation weight,job market status$$$$$college students’ deep entrepreneurship,job market,pattern analysis,spatio-temporal big data flow#####Advanced Query Answering Techniques over Big Mobile Data&&&&&Mobile environments are classical settings where big data arise, mostly raised up by emerging (mobile) environmentssuch as social networks, sensor networks, IoT infrastructures, and so forth. This phenomenon introduces a novel class of big data, the so-called big mobile data. Big mobile data demand for novel models, techniques and algorithms devoted to the annoying problem of effectively and efficiently querying large-scale, enormous, highly-heterogeneous amounts of data, which is now living a renewed season precisely due to the advent of the big data era. Indeed, classical approaches developed during decades of database research activities demand for novel adaptations and optimizations explicitly tailored to deal with the (many) V-requirements of big data management in mobile environments. In line with this emerging research trends, this panel will focus the attention on state-of-the-art proposals in the area of advanced query answering techniques over big mobile data, and will propose critical comments about pros and cons of actual research efforts along with future research directions to be considered in future years.&&&&&Mobile communication,Big data,Query processing,Context,Distributed databases,Proposals,Trajectory$$$$$Big Data,mobile computing,query processing$$$$$big mobile data,large-scale-enormous-highly-heterogeneous data query,database research activities,Big Data management,mobile environments,advanced-query answering techniques$$$$$Big Data,Big Mobile Data,Querying Big Mobile#####A Study on Association Algorithm of Smart Campus Mining Platform Based on Big Data&&&&&In this thesis, jumping out of the traditional data statistic analysis method, the author constructs smart campus data platform and uses Apriori algorithm to make the association analysis on the learning and living data of students in campus in order to excavate their marks, basic information, attendance and states of internet use. In this way, the university can better guide the students to study based on the results of association analysis.&&&&&Transportation,Big Data,Smart cities$$$$$Big Data,data mining,educational administrative data processing,statistical analysis$$$$$association analysis,association algorithm,smart campus mining platform,big data,Apriori algorithm,student learning,student attendance,data statistic analysis method,smart campus data platform,student living data,internet use$$$$$Big Data,Smart Campus,Apriori Association Algorithm#####Predicting outcomes for big data projects: Big Data Project Dynamics (BDPD): Research in progress&&&&&The number and importance of Big Data projects is increasing, but unfortunately, a large proportion of Big Data projects are failing. The ability of organizations to manage these projects has so far not kept pace - they need better ways to analyze the behavior of their Big Data projects and positively affect outcomes. The objective of this paper is to identify the important dynamic characteristics of Big Data projects, and explore how a modeling and simulation technique called system dynamics (SD) can be applied these characteristics. The approach draws from applicable concepts in the domains of traditional project management, Agile software development and Lean product development, and proposes to develop a model called Big Data Project Dynamics (BDPD) incorporating these insights. The BDPD model is organized into sectors: Core Rework Cycle, Iterative & Incremental, Exploration & Learning, Economic Value, and Policy Actions & Consequences. Given ADPD, practitioners can simulate Big Data project behavior from initial conditions, and probabilistically predict project outcomes.&&&&&Big Data,Data models,Project management,Organizations,Tools,Predictive models,Complexity theory$$$$$Big Data,DP industry,DP management,product development,project management$$$$$outcome prediction,system dynamics,project management,agile software development,lean product development,BDPD,SD,Big Data project behavior,Big Data Project Dynamics,big data projects$$$$$Big Data Project,System Dynamics,Prediction,Modeling & Simulation,Project Management#####Towards the Design of a System and a Workflow Model for Medical Big Data Processing in the Hybrid Cloud&&&&&Globally, a big data analytics technology is being issued in various business areas including the medical field. Because the technologies demand a large number of resources, studies on a distributed cloud including a hybrid cloud technology are needed. However, there are limitations of performance and cost since the most of hospitals use the private cloud of low computing resource. Thus, a research on the hybrid cloud is required to resolve the limitations. In this paper, we propose and evaluate a system called BigPros for medical big data processing with the proposed workflow model in the hybrid cloud resolving above issues. Based on this study, the BigPros can provide an efficient data processing method for a medical big data analytics in the hybrid cloud environment.&&&&&Batch production systems,Big Data,Task analysis,Cloud computing,Data models,Ontologies$$$$$Big Data,cloud computing,data analysis,hospitals,medical administrative data processing$$$$$workflow model,medical big data processing,big data analytics technology,medical field,distributed cloud,hybrid cloud technology,private cloud,low computing resource,medical big data analytics,hybrid cloud environment,hospitals,BigPros$$$$$Big data Computing; Hybrid Cloud; Dynamic Workflow; Directed Acyclic Graph; MapReduce#####KDD meets Big Data&&&&&Cross-Industry Standard process model (CRISPDM) was developed in the late 90s by a consortium of industry participants to facilitate the end-to-end data mining process for Knowledge Discovery in Databases (KDD). While there have been efforts to better integrate with management and software development practices, there are no extension to handle the new activities involved in using big data technologies. Data Science Edge (DSE) is an enhanced process model to accommodate big data technologies and data science activities. In recognition of the changes, the author promotes the use of a new term, Knowledge Discovery in Data Science (KDDS) as a call for the community to develop a new industry standard data science process model.&&&&&Big data,Data science,Organizations,Data models,Standards organizations,Data mining,Capability maturity model$$$$$Big Data,data mining,software engineering$$$$$knowledge discovery in databases,KDD,knowledge discovery in data science,KDDS,data science edge,DSE,cross-industry standard process model,CRISPDM,data mining process,software development practices,management practices,big data technologies,industry standard data science process model$$$$$data science,big data,knowledge discovery,KDD,KDDM,KDDS,data mining,analytics,analytics lifecycle,data lifecycle#####Linked 'Big' Data: Towards a Manifold Increase in Big Data Value and Veracity&&&&&The Web of Data is an increasingly rich source of information, which makes it useful for Big Data analysis. However, there is no guarantee that this Web of Data will provide the consumer with truthful and valuable information. Most research has focused on Big Data's Volume, Velocity, and Variety dimensions. Unfortunately, Veracity and Value, often regarded as the fourth and fifth dimensions, have been largely overlooked. In this paper we discuss the potential of Linked Data methods to tackle all five V's, and particularly propose methods for addressing the last two dimensions. We draw parallels between Linked and Big Data methods, and propose the application of existing methods to improve and maintain quality and address Big Data's veracity challenge.&&&&&Big data,Semantics,Resource description framework,Data models,Encyclopedias,Vocabulary$$$$$Big Data,data analysis,Internet$$$$$Big Data value,Web of Data,Big Data analysis,linked Big Data methods,Big Data veracity challenge$$$$$linked data,Web of Data,Veracity,Value,Big Data dimension#####Discovering the interdisciplinary nature of big data research&&&&&The study presented in this poster aims to address the paucity of studies examining the interdisciplinary nature of Big Data research. Using bibliometric records of articles downloaded from the Web of Science (WoS), this study utilizes the co-occurrence data between Subject Categories (SCs) related to Big Data research to discover the structure and pattern of the interdisciplinary network; its distribution and evolution over time; and the structural communities of interdisciplinary collaboration. The study also provides visualizations of these interdisciplinary networks. Additionally, this study measures the degree of interdisciplinary collaboration in Big Data research based on the co-occurrences of SCs of the related research publications using Stirling's Diversity Index and Specialization Index.&&&&&Big Data,Collaboration,STEM,Indexes,Bibliometrics,Computer science,Data visualization$$$$$Big Data,citation analysis,social sciences$$$$$interdisciplinary collaboration,big data research,related research publications,co-occurrence data,bibliometric records,Web of Science articles,WoS articles,subject categories$$$$$Big Data,interdisciplinary collaboration,network structure and patterns,visualization,interdisciplinarity,measures,network analysis#####An exploratory study on big data processing: A case study from a biomedical informatics&&&&&Recent studies confirm that big data processing will be a leading trend in the next several years in the biomedical field. Technical advancements in the field of medicine and life science demand for high storage and processing power. This paper presents an exploratory study on big data processing from a biomedical informatics perspective. In an attempt to perform this task, two case studies are considered from medical imaging and bioinformatics specifically proteomics aspect. It has been identified that there is scope for the growing medical imaging data and proteomic data be efficiently stored and processed through the use of big data technology. This study will also benefit the researchers working on big data tools in conjunction with data mining tools.&&&&&Big data,Proteins,Bioinformatics,Diseases,Medical diagnostic imaging$$$$$Big Data,bioinformatics,biomedical imaging,data mining,medical computing,proteomics$$$$$Big data processing,biomedical informatics,biomedical field,medicine field,life science demand,medical imaging data,Big data tools,data mining tools$$$$$Big data,Big data processing,Bioinformatics,Medical imaging,Neurodegenerative diseases#####Agile big data analytics: AnalyticsOps for data science&&&&&Big data analytic (BDA) systems leverage data distribution and parallel processing across a cluster of resources. This introduces a number of new challenges specifically for analytics. The analytics portion of the complete lifecycle has typically followed a waterfall process - completing one step before beginning the next. While efforts have been made to map different types of analytics to an agile methodology, the steps are often described as breaking activities into smaller tasks while the overall process is still consistent with step-by-step waterfall. BDA changes a number of the activities in the analytics lifecycle, as well as their ordering. The goal of agile analytics - to reach a point of optimality between generating value from data and the time spent getting there. This paper discusses the implications of an agile process for BDA in cleansing, transformation, and analytics.&&&&&Software,Data science,Analytical models,Big Data,Data models,Testing,Computational modeling$$$$$Big Data,data analysis,parallel processing,software prototyping$$$$$BDA,analytics lifecycle,agile process,agile big data analytics,data science,waterfall process,agile methodology,step-by-step waterfall,AnalyticsOps,big data analytic systems,data distribution,parallel processing$$$$$advanced analytics,agile development,analytics lifecycle,AnalyticsOps,big data analytics,data science,data science process models,Deep Learning,DevOps,Knowledge Discovery in Data Science,machine learning#####Research and application on occupational hazards regulation big data&&&&&Big date application soars in commercial and data industries. By analyzing overall framework of occupational hazards regulation big date platform, process from the source to the end-user and the occupational hazards regulation big data application architecture was illustrated in this issue. The data integration, analysis, processing and display technologies were researched to meet the requirements of occupational hazards regulation. Key technologies of big data will bring a new development opportunity for the construction of occupational hazards regulation platform.&&&&&Big Data,Hazards,Data mining,Databases,Computer architecture,Data integration,Distributed computing$$$$$Big Data,data integration,occupational health,occupational stress$$$$$data integration,data analysis,data processing,data display technologies,occupational hazards regulation Big Data application architecture$$$$$big data,occupational hazards regulation,intelligent,platform#####Shade: A differentially-private wrapper for enterprise big data&&&&&Enterprises usually provide strong controls to prevent cyberattacks and inadvertent leakage of data to external entities. However, in the case where employees and data scientists have legitimate access to analyze and derive insights from the data, there are insufficient controls and employees are usually permitted access to all information about the customers of the enterprise including sensitive and private information. Though it is important to be able to identify useful patterns of one's customers for better customization and service, customers' privacy must not be sacrificed to do so. We propose an alternative - a framework that will allow privacy preserving data analytics over big data. In this paper, we present an efficient and scalable framework for Apache Spark, a cluster computing framework, that provides strong privacy guarantees for users even in the presence of an informed adversary, while still providing high utility for analysts. The framework, titled Shade, includes two mechanisms - SparkLAP, which provides Laplacian perturbation based on a user's query and SparkSAM, which uses the contents of the database itself in order to calculate the perturbation. We show that the performance of Shade is substantially better than earlier differential privacy systems without loss of accuracy, particularly when run on datasets small enough to fit in memory, and find that SparkSAM can even exceed performance of an identical nonprivate Spark query.&&&&&Privacy,Data privacy,Sparks,Sensitivity,Databases,Big Data,Laplace equations$$$$$Big Data,business data processing,cluster computing,data analysis,data privacy,database management systems,query processing$$$$$differentially-private wrapper,enterprise big data,strong controls,cyberattacks,external entities,employees,legitimate access,sensitive information,private information,customization,service,efficient framework,scalable framework,Apache Spark,cluster computing framework,strong privacy guarantees,informed adversary,user,customer privacy,pattern identification,privacy preserving data analytics,Shade,data inadvertent leakage,data scientists,big data,SparkLAP,Laplacian perturbation,user query,SparkSAM,database,differential privacy systems,identical nonprivate Spark query$$$$$enterprise big data,differential privacy,big data analytics#####Big Data Security and Privacy Issues in Healthcare&&&&&With the ever-increasing cost for healthcare and increased health insurance premiums, there is a need for proactive healthcare and wellness. In addition, the new wave of digitizing medical records has seen a paradigm shift in the healthcare industry. As a result, the healthcare industry is witnessing an increase in sheer volume of data in terms of complexity, diversity and timeliness. As healthcare experts look for every possible way to lower costs while improving care process, delivery and management, big data emerges as a plausible solution with the promise to transform the healthcare industry. This paradigm shift from reactive to proactive healthcare can result in an overall decrease in healthcare costs and eventually lead to economic growth. While the healthcare industry harnesses the power of big data, security and privacy issues are at the focal point as emerging threats and vulnerabilities continue to grow. In this paper, we present the state-of-the-art security and privacy issues in big data as applied to healthcare industry.&&&&&Medical services,Big data,Security,Industries,Data privacy,Real-time systems,Privacy$$$$$Big Data,data privacy,health care,medical information systems$$$$$Big data security issues,Big data privacy issues,health insurance premiums,medical record digitization,health care industry,data volume,data complexity,data diversity,data timeliness,health care process improvement,health care delivery improvement,health care management improvement,proactive healthcare,reactive healthcare,healthcare cost reduction,economic growth$$$$$healthcare,big data security,privacy,security analytics#####bigNN: An open-source big data toolkit focused on biomedical sentence classification&&&&&Every single day, a massive amount of text data is generated by different medical data sources, such as scientific literature, medical web pages, health-related social media, clinical notes, and drug reviews. Processing this wealth of data is indeed a daunting task, and it forces us to adopt smart and scalable computational strategies, including machine intelligence, big data analytics, and distributed architecture. In this contribution, we designed and developed an open-source big data neural network toolkit, namely bigNN which tackles the problem of large-scale biomedical text classification in an efficient fashion, facilitating fast prototyping and reproducible text analytics researches. bigNN scales up a word2vec-based neural network model over Apache Spark 2.10 and Hadoop Distributed File System (HDFS) 2.7.3, allowing for more efficient big data sentence classification. The toolkit supports big data computing, and simplifies rapid application development in sentence analysis by allowing users to configure and examine different internal parameters of both Apache Spark and the neural network model. bigNN is fully documented, and it is publicly and freely available at https://github.com/bircatmcri/bigNN.&&&&&Big Data,Neural networks,Sparks,Conferences,Open source software,Biological system modeling$$$$$Big Data,medical information systems,neural nets,pattern classification,public domain software,text analysis$$$$$text analytics,medical data sources,bigNN,big data sentence classification,sentence analysis,big data computing,Hadoop Distributed File System 2,Apache Spark 2,neural network model,word2vec,large-scale biomedical text classification,open-source big data neural network toolkit,distributed architecture,big data analytics,smart strategies,clinical notes,social media,medical web pages,scientific literature,text data,biomedical sentence classification$$$$$Big Data Computing,Big Data Biomedical Text Classification,Open-Source Big Data Neural Network#####Big social data analytics of changes in consumer behaviour and opinion of a TV broadcaster&&&&&This paper examines the changes in consumer behaviour and opinions due to the transition from a public to a commercial broadcaster in the context of broadcasting international media events. By analyzing TV viewer ratings, Facebook activity and its sentiment, we aim to provide answers to how the transition from airing Winter Olympic Games on NRK to TV2 in Norway affected consumer behaviour and opinion. We used text classification and visual analytics methods on the business and social datasets. Our main finding is a clear link between negative sentiment and commercials. Despite positive change in customer behaviour, there was a negative change in customer opinion. Based on media events and broadcaster theories, we identify generalisable findings for all such transitions.&&&&&Facebook,TV,Broadcasting,Games,Data analysis,Media,Companies$$$$$Big Data,consumer behaviour,data analysis,data visualisation,pattern classification,social networking (online),television broadcasting,text analysis$$$$$big social data analytics,consumer behaviour,consumer opinion,TV broadcaster,international media event broadcasting,TV viewer ratings,Facebook activity,Winter Olympic Games,NRK,TV2,Norway,text classification,visual analytics$$$$$Television Viewer behaviour,Big Social Data,Facebook,Big Data Analytics,Social Set Analysis,Social Set Visualiser,Text Classification#####Research on big data application in intelligent safety supervision&&&&&Big date technology plays an important role in intelligent safety supervision. Based on overall framework of safety supervision big date platform, this issue demonstrates the process from the source to the end-user and the safety supervision big data application architecture. The data integration, analysis, processing and display technologies were analyzed to meet the requirements of intelligent safety supervision. Key technologies of big data will bring a new development opportunity for the construction of intelligent safety supervision platform.&&&&&Safety,Data mining,Real-time systems,Big Data applications,Distributed computing$$$$$Big Data,business data processing$$$$$display technologies,intelligent safety supervision platform,Big Data technology,safety supervision Big Data application architecture,safety supervision Big Data platform$$$$$big data,safety supervision,intelligent,platform#####Fragmenting Big Data to Boost the Performance of MapReduce in Geographical Computing Contexts&&&&&The last few years have seen a growing demand of distributed Cloud infrastructures able to process big data generated by geographically scattered sources. A key challenge of this environment is how to manage big data across multiple heterogeneous datacenters interconnected through imbalanced network links. We designed a Hierarchical Hadoop Framework (H2F) where a top-level business logic smartly schedules bottom-level computing tasks capable of exploiting the potential of the MapReduce within each datacenter.In this work we discuss on the opportunity of fragmenting the big data into small pieces so that better workload configurations may be devised for the bottom-level tasks. Several case study experiments were run on a testbed where a software prototype of the designed framework was deployed. The test results are reported and discussed in the last part of the paper.&&&&&Big Data,Task analysis,Computational modeling,Data models,Distributed databases,Processor scheduling$$$$$Big Data,cloud computing,computer centres,data analysis,geographic information systems,parallel processing,scheduling$$$$$geographically scattered sources,multiple heterogeneous datacenters,imbalanced network links,Hierarchical Hadoop Framework,MapReduce,geographical computing,bottom-level computing task scheduling,top-level business logic,Big Data fragmentation,distributed cloud infrastructure,Big Data management,H2F,workload configurations,software prototype,Big Data processing$$$$$Big Data,MapReduce,Data fragmentation,Geographical computing environment,Hierarchical Hadoop#####A simple analysis of revolution and innovation of marketing mix theory from big data perspective&&&&&Using big data technology to integrate date, transform these data into fundamental and strategic resources in enterprise marketing management and formulate and implement data-driven marketing mix strategy has become a research hotspot in enterprise management circles, and it has generated far-reaching influence on traditional marketing theory and especially marketing mix theory. From perspective of big data, revolution idea and implementation form of traditional marketing mix theory is analyzed in this paper, and approach to establishing innovative strategies of marketing mix pattern under big data background is studied.&&&&&Big Data,Pricing,Forecasting,Marketing management,Internet,Data mining,Databases$$$$$Big Data,marketing data processing$$$$$big data technology,enterprise marketing management,data-driven marketing mix strategy,enterprise management circles$$$$$Big Data,Marketing Mix Theory,Big Data Marketing,Innovation#####Big data machine learning and graph analytics: Current state and future challenges&&&&&Big data machine learning and graph analytics have been widely used in industry, academia and government. Continuous advance in this area is critical to business success, scientific discovery, as well as cybersecurity. In this paper, we present some current projects and propose that next-generation computing systems for big data machine learning and graph analytics need innovative designs in both hardware and software that provide a good match between big data algorithms and the underlying computing and storage resources.&&&&&Big data,Computer architecture,Conferences,Machine learning algorithms,Graphics processing units,Hardware,Nonvolatile memory$$$$$Big Data,graph theory,learning (artificial intelligence)$$$$$Big Data machine learning,graph analytics,business success,scientific discovery,cybersecurity,next-generation computing systems,hardware innovative designs,software innovative designs,Big Data algorithms,computing resources,storage resources$$$$$Big Data,Lambda Architecture,Hardware and Software Co-Design,Graphics Processing Unit,Non-Volatile Memory,Solid-State Drive#####ORANGE: Spatial big data analysis platform&&&&&Information which is related to the geographic area is being produced continuously. However, there is currently no technique that can handle large spatial data. For this reason, we developed a spatial big data platform, ORANGE, based on the Apache Hadoop. ORANGE can load the vector and raster data based on HDFS and manages metadata and creates index data using the Apache HIVE. These improvements made the platform faster process of large scale spatial data. In case of comparing the nine-Intersection Model.&&&&&Spatial databases,Big data,Libraries,Metadata,Load modeling,Spatial indexes,Conferences$$$$$Big Data,data analysis,meta data,visual databases$$$$$ORANGE,spatial big data analysis platform,Apache Hadoop,HDFS,meta data,nine-intersection model$$$$$Spatial Big Data,Spatial Hadoop,Spatial Analysis,Visualization#####Enrichment Patterns for Big Data&&&&&Importance of "Big Data" in terms of business value is very well understood across different sectors such as telecom, banking, insurance etc for targeted campaigns or real time performance actions. "Big Data" emphasizes the following characteristics, Velocity, Volume, Variety, and Veracity. Business adopts one or more of the above properties to cater to the requirements of the clients. Data being crucial in this case has different facets. The sources of data being different and consumption across different businesses makes the data modeling a tougher problem. Data schema evolves with new sources of data, changes due to change in data sources, etc Thus enrichment of data constantly triggers the needs to device methods to adopt the models to the new patterns. When the enrichment patterns are understood, modeling the Big Data and Management becomes easy. We highlight the list of such identified patterns based upon our real world implementations. In this work, we propose a method to evolve the data models from its initially defined schema such that data models can easily adapt to changes. We show through cases studies from real world example that our model can adopt to evolve data from different sources.&&&&&Big data,Data models,Organizations,Abstracts,Context,Real-time systems$$$$$Big Data,data models$$$$$enrichment patterns,Big Data,business value,velocity,volume,variety,veracity,data schema,data sources,data models$$$$$Data Architecture patterns for Big Data#####A research on enterprise crisis management innovation based on big data technology&&&&&Big data is cutting-edge technology extensively applied to enterprise management based on cloud computing, and it brings a brand-new perspective for enterprise crisis management. Using big data technology to collect, screen, analyze and apply data generated by enterprise crisis incidents and formulating and implementing data-driven enterprise crisis management strategies help to elevate level of enterprise crisis management. Enterprise crisis management system model was established on basis of big data technology, main problems existing in enterprise crisis management in China were analyzed, and innovative application of big data under the new situation in enterprise crisis management was explored.&&&&&Crisis management,Big Data,Decision making,Data mining,Analytical models,Data models,Cloud computing$$$$$Big Data,business data processing,cloud computing,innovation management$$$$$enterprise crisis management innovation,enterprise crisis incidents,enterprise crisis management system model,Big Data technology,data-driven enterprise crisis management strategies,cloud computing$$$$$Big Data,Enterprise Crisis Management,Data Mining#####Using big data to enhance crisis response and disaster resilience for a smart city&&&&&High population growth, urbanization, and global climate change drive up the frequency of disasters, causing grave losses of people's lives and property worldwide. Additionally, globalization, technological development, and the changing roles of individuals in society will require entirely new approaches, tools, and capabilities to help inform decision making under uncertain conditions. However, the mismatch between the high disaster vulnerability and the low crisis response and disaster resilience becomes a critical problem for emergency management. Recent years, key advances in computing - smartphones, worldwide mobile internet access, social media and industrial big data have all contributed to break through barriers of information exchange which help disaster managers working on data-driven solutions to disaster management problems. Based on big data, the emergency managers can identify and assesse risk through critical infrastructure operating data or sensor data, and then predict the affected population through smartphone data or social media data, and finally provide an operation plan for establishing the target capabilities for a community to response to the crisis. Also, big data plays a part in providing real-time clues of on-site disaster information through data mining. Based on the analyzed real-time disaster information, the research creates real-time feedback loops on nature disaster to help the decision-makers make the real-time update, precision and dynamic rescue plan. In combination, big data can help in all four phases of disaster management: prevention, protection, mitigation, response, and recovery, and can also help us take actions to improve our city's resilience to disasters.&&&&&Big Data,Resilience,Real-time systems,Emergency services,Engines,Social network services,Disaster management$$$$$Big Data,critical infrastructures,data mining,disasters,emergency management,Internet,mobile computing,smart phones$$$$$data mining,real-time disaster information,nature disaster,disaster resilience,emergency management,information exchange,data-driven solutions,disaster management problems,critical infrastructure operating data,sensor data,smartphone data,social media data,crisis response,industrial Big Data,smart city,worldwide mobile Internet access,real-time feedback loops,onsite disaster information$$$$$big data,crisis response,disaster resilience,emergence management,smart city#####Big data challenges in China centre for resources satellite data and application&&&&&China Centre for Resources Satellite Data and Application (abbreviate as CRESDA) is a core platform to store, process, analyze, and distribute land observing satellite data in China. It can provide high quality and effective services for the State Council and the relevant departments of government and local authorities. In the era of big data, the data center benefits from big data opportunities as well as suffering from big data challenges. In the paper, the big data challenges of the CRESDA are summarized. In particular, four major challenges are comprised of the 3V dimensions of big data (i.e. Volume, Variety, and Velocity) and one specific challenge (i.e., extensibility) in the data center.&&&&&Satellites,Remote sensing,Big Data,Monitoring$$$$$artificial satellites,Big Data,geophysics computing,remote sensing$$$$$resources satellite data,land observing satellite data distribution,big data 3V dimensions,land observing satellite data processing,land observing satellite data analysis,land observing satellite data storage,CRESDA,China Centre for Resources Satellite Data and Application,data center$$$$$Land observing satellites,data center,big data,data challenges#####Semantic HMC for big data analysis&&&&&Analyzing Big Data can help corporations to improve their efficiency. In this work we present a new vision to derive Value from Big Data using a Semantic Hierarchical Multi-label Classification called Semantic HMC based in a non-supervised Ontology learning process. We also propose a Semantic HMC process, using scalable Machine-Learning techniques and Rule-based reasoning.&&&&&Big data,Semantics,Ontologies,Cognition,Data mining,Taxonomy,Vectors$$$$$Big Data,data analysis,inference mechanisms,knowledge based systems,learning (artificial intelligence),ontologies (artificial intelligence),pattern classification$$$$$semantic HMC,Big Data analysis,semantic hierarchical multilabel classification,nonsupervised ontology learning process,machine-learning techniques,rule-based reasoning$$$$$classification,multi-classify,Big-Data,ontology,semantic technologies,machine learning#####Big Data Model of Security Sharing Based on Blockchain&&&&&The rise of big data age in the Internet has led to the explosive growth of data size. However, trust issue has become the biggest problem of big data, leading to the difficulty in data safe circulation and industry development. The blockchain technology provides a new solution to this problem by combining non-tampering, traceable features with smart contracts that automatically execute default instructions. In this paper, we present a credible big data sharing model based on blockchain technology and smart contract to ensure the safe circulation of data resources.&&&&&Contracts,Big Data,Data models,Security,Government,Protocols,Industries$$$$$Big Data,security of data$$$$$industry development,blockchain technology,smart contract,data resources,security sharing,data size,trust issue,data safe circulation,Big Data sharing model$$$$$big data,blockchain,smart contract,data sharing#####Philosophy of Big Data: Expanding the Human-Data Relation with Big Data Science Services&&&&&Big data is growing as an area of information technology, service, and science, and so too is the need for its intellectual understanding and interpretation from a theoretical, philosophical, and societal perspective. The Philosophy of Big Data is the branch of philosophy concerned with the foundations, methods, and implications of big data, the definitions, meaning, conceptualization, knowledge possibilities, truth standards, and practices in situations involving very-large data sets that are big in volume, velocity, variety, veracity, and variability. The Philosophy of Big Data is evolving into a discipline at two levels, one internal to the field as a generalized articulation of the concepts, theory, and systems that comprise the overall conduct of big data science. The other is external to the field, as a consideration of the impact of big data science more broadly on individuals, society, and the world. Methods, tools, and concepts are evaluated at both the level of industry practice theory and social impact. Three aspects are considered: what might constitute a Philosophy of Big Data, how the disciplines of the Philosophy of Information and the Philosophy of Big Data are developing, and an example of the Philosophy of Big Data in application in the data-intensive science field of Synthetic Biology. Overall a Philosophy of Big Data might helpful in conceptualizing and realizing big data science as a service practice, and also in transitioning to data-rich futures with human and data entities more productively co-existing in mutual growth and collaboration.&&&&&Big data,Standards,Context,Data models,Cognition,Ontologies,Ethics$$$$$Big Data,philosophical aspects,very large databases$$$$$human-data relation,Big Data science services,very-large data sets,Big Data philosophy,generalized articulation,internal levels,external levels,industry practice theory,social impact,information philosophy,data-intensive science field,synthetic biology,information technology,information service,information science,theoretical analysis,philosophical analysis,societal analysis,data volume,data velocity,data variety,data veracity,data variability$$$$$big data,philosophy,information,scientific method,research,methodology,human-data relations#####Big Data Collection and Analysis Framework Research for Public Digital Culture Sharing Service&&&&&The big data collection and analysis of public digital culture sharing service is researched in this paper. Big data includes three types of data, namely: ancillary service data, public digital culture sharing service platform operation data and user data. The aim is to build a data analysis platform for the three classes of data. Through the analysis of the three types of data collected, the use of resources and the operation of the platform can be mastered for providing better service for resource organization and scheduling of platform. Through the analysis of three types of the collected data, it can realize all kinds of statistics and analysis services in multidimensional. This paper presents a personalized recommender system of public digital cultural resources.&&&&&Conferences,Multimedia communication,Big data$$$$$Big Data,cultural aspects,data analysis,peer-to-peer computing,social sciences computing$$$$$Big Data analysis,ancillary service data,public digital culture sharing service platform operation data,user data,resource organization,platform scheduling,personalized recommender system,Big Data collection$$$$$Public Culture,big data,Cloud Computing,Framework#####Experimental teaching design and practice on big data course&&&&&With the rapid development of big data technology and the rapid growth of big data industry market, big data talent demand is also a substantial increase in China. In order to cultivate more talented people satisfying the needs of the community, we have designed the big data course for undergraduates. The big data course stresses not only on many theories but also lots of practice. The project of “big data talent development trend analysis” is designed in the experimental teaching on big data. By doing this project, students can master all the technologies of big data processing lifecycle, including data collection, data preprocessing, data mining and data visualization. We evaluate students who master big data core technology with a multi-evaluation method and design the experiment evaluation system on big data. Through our two years' practice, the results show that all these designs have achieved the good effect and improved the teaching quality.&&&&&Big Data,Education,Data mining,Clustering algorithms,Data visualization,Classification algorithms,Industries$$$$$Big Data,computer science education,data analysis,data mining,data visualisation,educational courses,further education,teaching$$$$$data visualization,big data core technology,big data course,big data technology,big data industry market,big data talent demand,big data talent development trend analysis,big data processing lifecycle,data collection,data preprocessing,data mining,China,multievaluation method,teaching quality$$$$$Experimental teaching,Big data course,Big data processing lifecycle,Big data project,Big data#####The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology&&&&&This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.&&&&&Transportation,Big data,Smart cities$$$$$Big Data,data analysis,data mining,economics,geographic information systems$$$$$geographic environment analysis,regional economic development,Yunnan province,China,Big Data technology,data mining analysis methods,data quality,data management,semantic engine,Big Data prediction,geographic elements,geographic factors$$$$$analysis of big data,Yunnan province of China,regional economic development,geographic environment#####Hybrid big data warehouse for on-demand decision needs&&&&&Every day trillions of data are generated across the world and put the information systems facing the emergence of big data phenomenon. This vertiginous evolution makes the enterprise confronting the challenge to build its own big data. To achieve the challenge, the enterprise is supposed to embark on big investments in terms of resource and material to process petabytes of diverse data, this last are sometimes useful and sometimes useless. The problem here is how to optimize data relevancy to extract value from the big data sources. From this Reasons, we propose in this paper an ETL and MapReduce Hybrid Approach based on Data Filtering and Processing to build an effective on-demand Dimensional Big Data, enabling enterprises to process relevant data in efficient and effective way according to the stakeholder's needs.&&&&&Big Data,Data warehouses,Filtering,Distributed databases,Data models,Data mining$$$$$Big Data,business data processing,data handling,data warehouses,information systems,parallel processing$$$$$Hybrid big data warehouse,on-demand decision needs,information systems,big data phenomenon,enterprise,big investments,data relevancy,big data sources,Data Filtering,on-demand Dimensional Big Data,data processing,MapReduce$$$$$Big Data,ETL,Data Warehouse,MapReduce,Data Filtering and Processing,on-demand Dimensional Big Data Warehouse#####Research on Warship Communication Operation and Maintenance Management Based on Big Data&&&&&Big data as another revolution of information technology is happening and has a huge impact on many fields of the society. In the military communication field, massive and multi-source data with explosive growth of operational command, operation and maintenance, meteorological environment provides the information source for data analysis in "information war" era. In order to improve the operational command efficiency in actual combat and enable to real-time predict and analyze battlefield situation and network performance and fault needs the support of intelligent and automatic communication operation and maintenance. In this paper, on a basis of related technologies of big data, focusing on the requirement of big data in the warship communication operation and maintenance management, and proposing a management architecture of warship communication operation and maintenance management based on big data, and the advantages of big data in the application of warship communication operation and maintenance management system are analyzed. Finally several issues of big data hold in the application of warship communication operation and maintenance management are elaborated.&&&&&Big data,Maintenance engineering,Business,Real-time systems,Data analysis,Servers,Computer architecture$$$$$Big Data,data analysis,maintenance engineering,military vehicles,ships$$$$$warship communication operation,maintenance management,big data,information technology,military communication field,multisource data,meteorological environment,information source,data analysis,information war era,operational command efficiency$$$$$big data,warship communication,operation and maintenance management#####System and architecture level characterization of big data applications on big and little core server architectures&&&&&Emerging Big Data applications require a significant amount of server computational power. Big data analytics applications rely heavily on specific deep machine learning and data mining algorithms, and exhibit high computational intensity, memory intensity, I/O intensity and control intensity. Big data applications require computing resources that can efficiently scale to manage massive amounts of diverse data. However, the rapid growth in the data yields challenges to process data efficiently using current server architectures such as big Xeon cores. Furthermore, physical design constraints, such as power and density, have become the dominant limiting factor for scaling out servers. Therefore recent work advocates the use of low-power embedded cores in servers such as little Atom to address these challenges. In this work, through methodical investigation of power and performance measurements, and comprehensive system level and micro-architectural analysis, we characterize emerging big data applications on big Xeon and little Atom-based server architecture. The characterization results across a wide range of real-world big data applications and various software stacks demonstrate how the choice of big vs little core-based server for energy-efficiency is significantly influenced by the size of data, performance constraints, and presence of accelerator. Furthermore, the microarchitecture-level analysis highlights where improvement is needed in big and little cores microarchitecture.&&&&&Big data,Servers,Data mining,Computer architecture,Microarchitecture,Atomic measurements,Real-time systems$$$$$Big Data,data mining,file servers,microcomputers,microprocessor chips$$$$$little cores microarchitecture,microarchitecture-level analysis highlights,energy efficiency,software stacks,Atom-based server architecture,big Xeon,micro-architectural analysis,low-power embedded cores,Xeon cores,diverse data,control intensity,I/O intensity,memory intensity,high computational intensity,data mining algorithms,machine learning,Big data analytics applications,server computational power,core server architectures,architecture level characterization$$$$$Performance,Power,Characterization,Big Data,High-Performance server,Low-Power server,Accelerator#####In Light of the Legal Debate over Personal Data Privacy at a Time of Globalized Big Data: Making Big Data Researchers Cooperating with Lawmakers to Find Solutions for the Future&&&&&At the same time as Big Data technologies are being constantly refined, the legislation relating to data privacy is changing. The invalidation by the Court of Justice of the European Union on October 6, 2015, of the agreement known as "Safe Harbor", negotiated by the European Commission on behalf of the European Union with the United States has two consequences. The first is to announce its replacement by a new, still fragile, program, the "Privacy Shield", which isn't yet definitive, which could also later be repealed by the Court of Justice of the European Union. For example, we are expecting to hear the opinion in mid-April 2016 of the group of data protection authorities for the various states of the European Union, known as G29. The second is to mobilize the Big Data community to take control of the question of data privacy management, to put in place an adequate internal program.&&&&&Europe,Law,Data privacy,Big data,Companies,Facebook$$$$$Big Data,data privacy,legislation$$$$$personal data privacy,globalized big data,big data researchers,legal debate,lawmakers,legislation,court of justice,European Union,safe harbor,European commission,United States,privacy shield,data protection authorities,G29$$$$$Big Data,Data Mining,Machine Learning,privacy,Data-Driven Intelligent Predictive Systems,personal data preservation,Privacy By Design,Safe Harbor,Privacy Shield#####Research on the Application of Big Data in Academic Libraries&&&&&This paper proposes a big data application model system of academic library. From the perspective of big data collection and integration, big data analysis methods, knowledge services, and the shortage of resources, this paper analyzes the plight of library big data application and then constructs a library big data application model system based on large scale network analysis method. This library big data application model based on knowledge management theory and open source cloud computing platform based on large scale network analysis method can face the challenge of big data service in academic library and effectively promote the further development of big data service in academic library.&&&&&Conferences,Transportation,Big Data,Smart cities$$$$$academic libraries,Big Data,cloud computing,data analysis,knowledge management,public domain software$$$$$big data service,academic library,integration,scale network analysis method,big data analysis,knowledge management theory,open source cloud computing platform$$$$$Keywords - Knowledge Management,Cloud Computing,Application Mode,Big Data of Academic Library#####The research of policy big data retrieval and analysis based on elastic search&&&&&As big data technologies develop, data retrieval and analysis has become an important research topic recently. This paper designs and implements a data retrieval and analysis platform based on elastic search. On the one hand, the system can store and search policy big data efficiently and reliably, and index data with real-time performance. On the other hand, policy big data has its own characteristics, for example, policy data have the properties of time, region, category, source, hot words, so this paper proposes the multidimensional analysis methods. In detail, by analysing from the aspects of the time, region, category, and keywords co-occurrence, this paper defines the characteristics of policy data, and displays them in a visual way.&&&&&Urban areas,Indexes,Data mining,Big Data,Local government,Data models$$$$$Big Data,data analysis,information retrieval$$$$$policy big data retrieval,elastic search,big data technologies,multidimensional analysis methods$$$$$elastic search,multidimensional analysis,policy big data,visualization#####Dynamic data transformation for low latency querying in big data systems&&&&&Big data storage technologies inherently entail high latency characteristics, preventing users from performing efficient ad-hoc querying and interactive visualization on large and distributed datasets. Most of the existing approaches addressing this issue thrive on de-normalization of the static data schema and creation of application specific (i.e. hard-coded) materialized views, which certainly reduce data access latency but at the expense of flexibility. In this regard, this paper proposes an approach that relies on an iterative process of data transformation intended to generate read-optimized data schemas. The transformation process is able to automatically identify optimization opportunities (e.g. materialized views, missing indexes), by analyzing the original data schema and the record of queries issued by users and client applications against the data set. An experimental evaluation of the proposed approach evidences a significant reduction in the query latency, ranging from 81.60% to 99.99%.&&&&&Data visualization,Tools,Big Data,Optimization,Amplitude modulation,Benchmark testing$$$$$Big Data,data visualisation,query processing$$$$$big data systems,big data storage technologies,high latency characteristics,interactive visualization,distributed datasets,data access,iterative process,read-optimized data schemas,transformation process,optimization opportunities,materialized views,original data schema,client applications,dynamic data transformation,low latency querying,ad-hoc querying,static data schema denormalization,application specific materialized view creation,data access latency reduction$$$$$Low latency querying,Dynamic data transformation,Polyglot persistence,Big Data,Denormalization#####Mathematical Models on the Hadoop Runtimes on Big Data&&&&&The problem of understanding runtime on big data processing has become key to solving the ever increasing volumes of data generated on machines. Nowadays big data is accessed through a searching system called Hadoop which uses the MapReduce algorithm. The effect of increasing machine clusters through which data is processed, the effect of machine failures on steady runtime, the effect of optimising runtime and machine cluster on the workflow process is analysed. The case in which the runtime and hours of data being processed differ is considered and the effect of the accumulation of data on runtime is analysed in detail. Mathematical models to analyse runtimes are proposed. The mathematical models proposed are borrowed from systems that process data in parallel processes. A simple runtime formula is adopted and numerical method is used to predict runtimes in the case where data is allowed to accumulate. Increasing the machine cluster reduce processing time. Increasing the overhead result in the increase in runtimes, A 15% machine failure result in the 261% increase on runtimes. The time to process one hour of data should be kept small. If one hour of data is processed in more than one hour the Hadoop system significantly slows down.&&&&&Runtime,Big Data,Mathematical model,Data models,Software,Data mining,Clustering algorithms$$$$$Big Data,data handling,parallel processing,pattern clustering$$$$$machine cluster,workflow process,mathematical models,Hadoop runtimes,big data processing,machine failures,parallel process,machine failure,MapReduce algorithm$$$$$Big Data,runtimes,Hadoop#####Bridging high velocity and high volume industrial big data through distributed in-memory storage & analytics&&&&&With an exponential increase in time series sensor data generated by an ever-growing number of sensors on industrial equipment, new systems are required to efficiently store and analyze this “Industrial Big Data.” To actively monitor industrial equipment there is a need to process large streams of high velocity time series sensor data as it arrives, and then store that data for subsequent analysis. Historically, separate systems would meet these needs, with neither system having the ability to perform fast analytics incorporating both just-arrived and historical data. In-memory data grids are a promising technology that can support both near real-time analysis and mid-term storage of big datasets, bridging the gap between high velocity and high volume big time series sensor data. This paper describes the development of a prototype infrastructure with an in-memory data grid at its core to analyze high velocity (>100,000 points per second), high volume (TB's) time series data produced by a fleet of gas turbines monitored at GE Power & Water's Remote Monitoring & Diagnostics Center.&&&&&Time series analysis,Real-time systems,Big data,Distributed databases,Hardware,Memory management,Data structures$$$$$Big Data,distributed memory systems,gas turbines,production engineering computing,sensors,time series$$$$$high velocity industrial big data,high volume industrial big data,distributed in-memory storage,distributed in-memory analytics,sensors,industrial equipment,high velocity time series sensor data,in-memory data grids,near real-time analysis,mid-term storage,big datasets,high velocity big time series sensor data,high volume big time series sensor data,high velocity analysis,time series data,gas turbines,GE Power and Water's Remote Monitoring and Diagnostics Center$$$$$time series data,remote monitoring and diagnostics,distributed computing,in-memory data grids,big data#####Weather data analysis and sensor fault detection using an extended IoT framework with semantics, big data, and machine learning&&&&&In recent years, big data and Internet of Things (IoT) implementations started getting more attention. Researchers focused on developing big data analytics solutions using machine learning models. Machine learning is a rising trend in this field due to its ability to extract hidden features and patterns even in highly complex datasets. In this study, we used our Big Data IoT Framework in a weather data analysis use case. We implemented weather clustering and sensor anomaly detection using a publicly available dataset. We provided the implementation details of each framework layer (acquisition, ETL, data processing, learning and decision) for this particular use case. Our chosen learning model within the library is Scikit-Learn based k-means clustering. The data analysis results indicate that it is possible to extract meaningful information from a relatively complex dataset using our framework.&&&&&Meteorology,Big Data,Semantics,Resource description framework,Machine learning algorithms,Data analysis,Feature extraction$$$$$Big Data,data analysis,fault diagnosis,feature extraction,geophysics computing,Internet of Things,learning (artificial intelligence),meteorology,pattern clustering,sensors$$$$$machine learning models,highly complex datasets,Big Data IoT Framework,weather data analysis use case,weather clustering,sensor anomaly detection,publicly available dataset,framework layer,data processing,relatively complex dataset,sensor fault detection,extended IoT framework,big data analytics solutions,hidden feature extraction,Internet of Things implementation,k-means clustering,Scikit-Learn,information extraction$$$$$Internet of things,machine learning,framework,big data analytics,weather data analysis,anomaly detection,fault detection,clustering#####Privacy-protected place of activity mining on big location data&&&&&People always spend their time at a few important locations for various activities in groups during specific time slots, called place of activity (POA), e.g., resting at home among family members during night and working at office among colleagues during work time. Inferring such places is significant for not only the precise advertising on the commercial aspect but the identifying rallies or meetings among a group of people and tracking of the target individuals on the aspect of public security, e.g., locating and tracking suspected terrorists for anti-terrorist work. However, it is a challenge to map from big location data to places of activity due to the volume and complexity whilst giving rise to privacy concerns, e.g., personally important place mining. In the paper, a method for POA mining on big location data is proposed, named P-PAM, aiming at big data analytics and privacy concerns. We use a clustering algorithm to discover the place of activity, then adopt location entropy as reference of user diversity and take into account temporal variation, to infer place of activity. Further, robust privacy-preserving mechanisms under differential privacy are embedded into clustering results and location entropy evaluation that accesses to raw location data. We demonstrate the utility of our proposed approach with large-scale location datasets derived from geo-referenced social media. The experimental results suggest that the POA mining approach can successfully scale to big data scenarios whilst preserving individual user privacy.&&&&&Data privacy,Privacy,Clustering algorithms,Entropy,Big Data,Perturbation methods$$$$$Big Data,data analysis,data mining,data privacy,entropy,mobile computing$$$$$anti-terrorist work,big location data,places,privacy concerns,personally important place mining,robust privacy-preserving mechanisms,location entropy evaluation,raw location data,large-scale location datasets,POA mining approach,individual user privacy,important locations,specific time slots,work time,Big Data analytics,Big Data scenarios,suspected terrorists,user privacy,geo-referenced social media,place of activity$$$$$Location privacy,clustering,big data,spatiotemporal data,differential privacy#####Cyber crime investigations in the era of big data&&&&&The amount of data seized in Crime Investigations has increased enormously. Investigators are more than ever confronted with vast amount of heterogeneous data, highly-diverse data formats, increased complexity in distributed stored information. With constantly increasing network bandwidth it makes extremely challenging to process or even store part of the network traffic. Nevertheless, criminal investigations need to solve crimes in a timely manners. New computational methods, infrastructure and algorithmic approaches are required. Although Big Data is a challenge for criminal investigators, it can also help them make to source an detect patterns to prevent and solve crimes. This paper aims to raise attention to current challenges in Cyber Crime Investigations - related to Big Data - and possible ways to approach combating cybercrimes.&&&&&Computer crime,Big Data,Digital forensics,Tools,Computers,Data analysis$$$$$Big Data,computer crime,security of data,storage management$$$$$distributed stored information,network bandwidth,network traffic,criminal investigations,criminal investigators,heterogeneous data,highly-diverse data formats,Big Data,cyber crime investigations$$$$$Digital Forensics,Big Data,Cyber Crime,Criminal Investigation,Computational Intelligence#####DD-Rtree: A dynamic distributed data structure for efficient data distribution among cluster nodes for spatial data mining algorithms&&&&&Parallelizing data mining algorithms has become a necessity as we try to mine ever increasing volumes of data. Spatial data mining algorithms like Dbscan, Optics, Slink, etc. have been parallelized to exploit a cluster infrastructure. The efficiency achieved by existing algorithms can be attributed to spatial locality preservation using spatial indexing structures like k-d-tree, quad-tree, grid files, etc. for distributing data among cluster nodes. However, these indexing structures are static in nature, i.e., they need to scan the entire dataset to determine the partitioning coordinates. This results in high data distribution cost when the data size is large. In this paper, we propose a dynamic distributed data structure, DD-Rtree, which preserves spatial locality while distributing data across compute nodes in a shared nothing environment. Moreover, DD-Rtree is dynamic, i.e., it can be constructed incrementally making it useful for handling big data. We compare the quality of data distribution achieved by DD-Rtree with one of the recent distributed indexing structure, SD-Rtree. We also compare the efficiency of queries supported by these indexing structures along with the overall efficiency of DBSCAN algorithm. Our experimental results show that DD-Rtree achieves better data distribution and thereby resulting in improved overall efficiency.&&&&&Data structures,Clustering algorithms,Data mining,Indexing,Distributed databases,Algorithm design and analysis$$$$$Big Data,data mining,data structures,indexing,pattern clustering,tree data structures$$$$$DD-RTREE,dynamic distributed data structure,cluster nodes,spatial data mining,DBSCAN,OPTICS,SLINK,cluster infrastructure,spatial locality preservation,spatial indexing structures,k-d-tree,quadtree,grid files,Big Data handling,data distribution quality,distributed indexing structure,SD-RTREE$$$$$Data mining,data distribution,spatial locality,neighborhood queries,k-NN queries,density based clustering#####A Study on Discovery Method of Hot Topics Based on Smart Campus Big Data Platform&&&&&To study and pose an modified discovery method for campus hot topics. The method is to extract word segmentation and keywords from the campus news collected on information big data platform through ICTCLAS word segmentation system, construct the text knowledge representation model based on vector space modal and finally add up the word frequency of headlines to determine k initial clustering centers and improve K-Means algorithm so as to obtain the campus hot topics.&&&&&Transportation,Big Data,Smart cities$$$$$Big Data,data mining,educational computing,knowledge representation,pattern clustering,text analysis$$$$$smart campus big data platform,modified discovery method,campus news,information big data platform,ICTCLAS word segmentation system,text knowledge representation model,campus hot topics discovery,headline word frequency,clustering centers,K-means algorithm$$$$$Smart Campus,Big Data,K-Means algorithm,Discovery Method of Hot Topics#####The application of big data using MongoDB: Case study with SCeLE Fasilkom UI forum data&&&&&Big Data is a condition in which data size in a database is very large so it is difficult to be managed. An e-Learning application, like SCeLE Fasilkom UI (scele.cs.ui.ac.id), also has a very large data. SCeLE has hundreds of forum data, and each forum has at least 4000 threads of discussion. In addition, one thread can have at least dozens or hundreds posts. Therefore, it may further experience data growth problem, which will be difficult to be handled by RDBMS, such as MySQL that is currently used. In order to solve this problem, a research been conducted to apply Big Data in SCeLE Fasilkom UI, which implementation is aimed to increase SCeLE's data management performance. The implementation of Big Data in the research used MongoDB as the system's DBMS. The research result showed that MongoDB obtain better results than MySQL in SCeLE Fasilkom UI forum data case in terms of speed.&&&&&Big data,Data models,Scholarships,Structured Query Language,Analytical models,Relational databases$$$$$Big Data,computer aided instruction,document handling,relational databases,SQL$$$$$big data,MongoDB,SCeLE Fasilkom UI forum data,e-Learning application,data growth problem,RDBMS,MySQL$$$$$Big Data,Database,SCeLE,MySQL,MongoDB,DBMS#####A Study on Key Techniques of Wisdom Campus Information Recommendation Platform Based on Big Data&&&&&This thesis employs the mainstream Hadoop framework to store and analyze the large-scale data generated in the campus environment. The system will first collect, process and save data by using the HDFS distributed file system. And then data mining and intelligent analysis of the data will be carried out. Last, the recommendation system framework which deals with large dataset will be put forward based on the above two steps. Therefore, a modified collaborative filtering recommendation algorithm based on the students' similarity will be achieved and the accuracy as well as the efficiency of recommendation will be improved.&&&&&Distributed databases,Filtering,Collaboration,Big data,Algorithm design and analysis,File systems$$$$$Big Data,collaborative filtering,computer aided instruction,data analysis,data mining,educational institutions,recommender systems$$$$$wisdom campus information recommendation platform,Big Data,Hadoop framework,data storage,data analysis,HDFS distributed file system,data mining,recommendation system framework,modified collaborative filtering recommendation algorithm,student similarity$$$$$Big Data,Wisdom Campus,Hadoop,HDFS#####A Discriminant Framework for Detecting Similar Scientific Research Projects Based on Big Data Mining&&&&&Scientific research projects play an important role in promoting the science and technology competitiveness of a country. Due to lack of information open and sharing, it is possible to approve similar or duplicated projects by different government departments. In some way, these similar projects are a waste of scientific resources. To avoid such a problem, this paper proposes a discriminant framework for detecting similar projects based on big data mining technologies, providing evidence-based decision making for government departments during the project approval process. Firstly, we construct a big data file associated with officially approved projects, including project titles, principal investigators, research organizations, keywords, and bibliographies of published scholar papers. Secondly, a discriminant framework is proposed to detect similar projects by mining information from the above big data file. Finally, we adopt the Hadoop architecture to speed up the data mining algorithm. To demonstrate the effectiveness and feasibility of the framework, we implement a prototype system for similar projects detection.&&&&&Big data,Data mining,Proposals,Prototypes,Distributed computing,Government$$$$$Big Data,data mining,file organisation,government data processing,natural sciences computing,public domain software$$$$$science and technology competitiveness,big data mining technologies,evidence-based decision making,government departments,project approval process,big data file,project titles,principal investigators,research organizations,keywords,bibliographies,published scholar papers,information mining,Hadoop architecture$$$$$big data mining,discriminant framework,similar scientific project detection,Hadoop architecture#####Addressing data veracity in big data applications&&&&&Big data applications such as in smart electric grids, transportation, and remote environment monitoring involve geographically dispersed sensors that periodically send back information to central nodes. In many cases, data from sensors is not available at central nodes at a frequency that is required for real-time modeling and decision-making. This may be due to physical limitations of the transmission networks, or due to consumers limiting frequent transmission of data from sensors located at their premises for security and privacy concerns. Such scenarios lead to partial data problem and raise the issue of data veracity in big data applications. We describe a novel solution to the problem of making short term predictions (up to a few hours ahead) in absence of real-time data from sensors in Smart Grid. A key implication of our work is that by using real-time data from only a small subset of influential sensors, we are able to make predictions for all sensors. We thus reduce the communication complexity involved in transmitting sensory data in Smart Grids. We use real-world electricity consumption data from smart meters to empirically demonstrate the usefulness of our method. Our dataset consists of data collected at 15-min intervals from 170 smart meters in the USC Microgrid for 7 years, totaling 41,697,600 data points.&&&&&Real-time systems,Intelligent sensors,Predictive models,Smart meters,Big data,Data models$$$$$Big Data,power engineering computing,smart power grids$$$$$data veracity,Big Data applications,smart electric grids,transportation,remote environment monitoring,communication complexity,sensory data transmission,electricity consumption data,USC Microgrid$$$$$data veracity,prediction model,smart grid#####Online anomaly detection over Big Data streams&&&&&Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.&&&&&Entropy,Measurement,Correlation,Big data,Data structures,Yttrium,Sparks$$$$$Big Data,security of data,telecommunication computing$$$$$mobile devices,large-scale telecommunication networks,user-driven anomalies,hardware misconfiguration,infrastructure failures,Spark Streaming,Kafka queues,Pearson correlation,relative entropy,data distributions,categorical data,numerical data,scalability condition,generality condition,online anomaly detection pipeline,data quality,Big Data streams#####Understanding Quality Requirements in the Context of Big Data Systems&&&&&While the domain of big data is anticipated to affect many aspects of human endeavour, there are numerous challenges in building big data applications among which is how to address big data characteristics in quality requirements. In this paper, we propose a novel, unified, approach for specifying big data characteristics (e.g., velocity of data arrival) in quality requirements (i.e., those requirements specifying attributes such as performance, reliability, availability, security, etc.). Several examples are given to illustrate the integrated specifications. As this is early work, further experimentation is needed in different big data situations and quality requirements and, beyond that, in a variety of project settings.&&&&&Big data,Security,Context,Requirements engineering,Data privacy,Reliability,Software$$$$$Big Data,software quality$$$$$quality requirements,Big Data systems,human endeavour,Big Data characteristics,data arrival$$$$$Quality requirements; big data; specification,requirements engineering,software engineering#####Saving costs with a big data strategy framework&&&&&Disruptive innovations such as Big Data offer enormous benefits to organizations however they usually come with a cost. Decision makers instinctively seek to adopt such innovative technologies for competitive advantage, however, they also explore ways in reducing implementation costs to a reasonable level. This research was focused on developing and empirically testing a Big Data Strategy framework that helps organizations in aligning their business strategy with the Big Data project in order to identify the potential value before embarking on a full Big Data implementation. The SAVI-BIGD framework was tested in two different organizations and findings reveals that the framework can help organizations identify the potential value of the Big Data project and also to save costs. This paper reports on a second case study testing and suggests how any organization taking ownership and championing the implementation of this Big Data strategy framework could save cost and generate their Strategic Big Data Goals.&&&&&Big Data,Organizations,Satellite broadcasting,Interviews,Data science,Standards organizations$$$$$Big Data,business data processing,innovation management,organisational aspects,strategic planning$$$$$big data strategy framework,organizations,Big Data project,Big Data implementation,Strategic Big Data Goals,implementation cost reduction,cost saving,decision makers,innovative technologies,business strategy,SAVI-BIGD framework,case study testing$$$$$Big Data,Strategy,Methodology,Method,Framework,Business Strategy,Digital Strategy,IS Alignment,Data Science#####Campus Management Strategy Research under the Environment of Big Data&&&&&With the development of modern network technology and the increasing information, society begins to gradually into the era of big data. Under the environment of Big data, the campuses how to implement the management strategy with the help of big data, and how to strengthen the utilization of big data to make construction of "digital campus" and "smart campus" become the focus of current topic which people should think. In view of the foregoing, in this paper, starting from the definition of big data, the meaning of big data in campus management and the specific informational campus management application are analyzed, so as to provide reference for the application of big data campus management strategy in the future.&&&&&Transportation,Big Data,Smart cities$$$$$Big Data,educational administrative data processing,educational institutions$$$$$campus management strategy research,specific informational campus management application,big data campus management strategy,modern network technology development,big data utilization,digital campus,smart campus$$$$$Big Data,Smart Campus,Campus Management Strategy,Informational Level#####Privacy-Preserving Big Data Stream Mining: Opportunities, Challenges, Directions&&&&&This paper explores recent achievements and novel challenges of the annoying privacy-preserving big data stream mining problem, which consists in applying mining algorithms to big data streams while ensuring the privacy of data. Recently, the emerging big data analytics context has conferred a new light to this exciting research area. This paper follows the so-depicted research trend.&&&&&Big Data,Data privacy,Privacy,Publishing,Trajectory,Data models$$$$$Big Data,data analysis,data mining,data privacy$$$$$mining algorithms,big data streams,privacy-preserving big data stream mining,big data analytics context$$$$$Privacy-preserving big data stream mining,mining big data streams,privacy of big data processing#####Big data analytics as-a-service: Issues and challenges&&&&&Big Data domain is one of the most promising ICT sectors with substantial expectations both on the side of market growing and design shift in the area of data storage managment and analytics. However, today, the level of complexity achieved and the lack of standardisation of Big Data management architectures represent a huge barrier towards the adoption and execution of analytics especially for those organizations and SMEs not including a sufficient amount of competences and knowledge. The full potential of Big Data Analytics (BDA) can be unleashed only through the definition of approaches that accomplish Big Data users' expectations and requirements, also when the latter are fuzzy and ambiguous. Under these premises, we propose Big Data Analytics-as-a-Service (BDAaaS) as the next-generation Big Data Analytics paradigm and we discuss issues and challenges from the BDAaaS design and development perspective.&&&&&Big data,Organizations,Distributed databases,Data models,Pipelines,Complexity theory$$$$$Big Data,cloud computing,data analysis,small-to-medium enterprises$$$$$Big Data analytics as-a-service,BDAaaS,ICT,data storage managment,Big Data management architectures,organizations,SME$$$$$Big Data,Big Data analytics,Issue and challenges#####Gateway-based access interface management in big data platform&&&&&Nowadays, there has been a massive data explosion coming from various devices sensors, social networks and IoT services. Due to big data analytics platforms, users can store, organize, and process these large sets of data to solve different issues in different domains. However, the current big data platforms still have many drawbacks. Among the limitations, managing access interfaces, an important process of analytic service development, needs to be improved significantly. The main reason is that the emergence of too many systems recently has been making the process become more and more complicated and costly. Therefore, we propose here a system related to the field of big data management, in particular to interface management to allow end-users to use easily their desired functions including metadata and data accessing. It also helps platform managers to extend and modify effortlessly the access interfaces. A case study on log analytic service is conducted to verify the validation and practice use of our system.&&&&&Big Data,Logic gates,Servers,Authentication,Metadata,Interface management,Authorization$$$$$authorisation,Big Data,data analysis,Internet of Things,internetworking,meta data$$$$$Big Data platform,gateway-based access interface management,IoT services,social networks,metadata,data accessing,log analytic service$$$$$Analytic,Big data,Gateway,Interface Management,Metadata#####A novel initialization method for particle swarm optimization-based FCM in big biomedical data&&&&&Based on empirical studies, the feature of random initialization in Particle Swarm Optimization (PSO) based Fuzzy c-means (FCM) methods affects the computational performance especially in big data. As the data points in high-density areas are more likely near the cluster centroids, we design a new algorithm to guide the initialization according to the data density patterns. Our algorithm is initialized by fusing the data characteristics near the cluster centers. Our evaluation results from real data show that our approach can significantly improve the computational performance of PSO-based Fuzzy clustering methods, while preserving comparable clustering performance.&&&&&Big data,Particle swarm optimization,Clustering algorithms,Algorithm design and analysis,Electronic mail,Bioinformatics,Solids$$$$$Big Data,fuzzy set theory,medical information systems,particle swarm optimisation,pattern clustering$$$$$initialization method,particle swarm optimization,big biomedical data,PSO,fuzzy c-means methods,FCM methods,cluster centroids,data density patterns,fuzzy clustering methods$$$$$Initialization,FCM,Patterns,Particle Swarm Optimization,Big Data#####Risk adjustment of patient expenditures: A big data analytics approach&&&&&For healthcare applications, voluminous patient data contain rich and meaningful insights that can be revealed using advanced machine learning algorithms. However, the volume and velocity of such high dimensional data requires new big data analytics framework where traditional machine learning tools cannot be applied directly. In this paper, we introduce our proof-of-concept big data analytics framework for developing risk adjustment model of patient expenditures, which uses the “divide and conquer” strategy to exploit the big-yet-rich data to improve the model accuracy. We leverage the distributed computing platform, e.g., MapReduce, to implement advanced machine learning algorithms on our data set. In specific, random forest regression algorithm, which is suitable for high dimensional healthcare data, is applied to improve the accuracy of our predictive model. Our proof-of-concept framework demonstrates the effectiveness of predictive analytics using random forest algorithm as well as the efficiency of the distributed computing platform.&&&&&Data models,Computational modeling,Information management,Data handling,Data storage systems,Predictive models,Linear regression$$$$$Big Data,data analysis,distributed processing,divide and conquer methods,health care,learning (artificial intelligence),medical information systems,regression analysis$$$$$risk adjustment,patient expenditures,big data analytics approach,healthcare applications,patient data,machine learning algorithms,high dimensional data,machine learning tools,proof-of-concept big data analytics framework,divide and conquer strategy,big-yet-rich data,model accuracy,distributed computing platform,MapReduce,random forest regression algorithm,high dimensional healthcare data,predictive model,proof-of-concept framework,predictive analytics$$$$$Healthcare Big Data,Risk Adjustment,Distributed Computing,Random Forest,Patient Expenditure#####My (fair) big data&&&&&Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.&&&&&Big Data,Pipelines,Ontologies,Metadata,Google$$$$$Big Data,data integrity,meta data,ontologies (artificial intelligence)$$$$$processing pipeline,Big Data sources,ontologies,integration systems,formal quality evaluation,integrated data,data confidentiality,Big Data quality dimension,policy making,strict requirement,quantitative quality information,high quality information,data quality issue$$$$$Quality-driven policies,Big Data pipeline,ontology-based quality checking,Big Data confidentiality#####Big Data Analytics Architecture for Internet-of-Vehicles Based on the Spark&&&&&Internet-of-Vehicles (IoV) technology is the development trend of the intelligent traffic management system, it is also an effective means to ease traffic and improve traffic efficiency. This article used big data analysis technology to build a big data analysis platform of intelligent transportation, the platform is decomposed into infrastructure layer, data analysis layer and application layer; All vehicle information are acquired by several in-vehicle sensors on ECUs or roadside, and the acquired sensor data and received information are for information fusion, processing, trajectory prediction and risk assessment. The platform can solve the problem of storage, analysis and multi terminal distribution of mass data, provide traffic information services to traffic management departments and the public, it is a useful attempt to apply advanced information technology to the transportation industry.&&&&&Conferences,Transportation,Big Data,Smart cities$$$$$Big Data,data analysis,Internet of Things,sensor fusion,traffic information systems,transportation$$$$$data analysis layer,vehicle information,Spark,intelligent transportation,big data analysis technology,traffic efficiency,intelligent traffic management system,internet-of-Vehicles technology,big data analytics architecture,traffic management departments,traffic information services,risk assessment,information fusion,acquired sensor data$$$$$Sensors,Big-data,Internet-of-Vehicles,Fusion#####Impact of big data on Electric-power industry&&&&&Mobile Internet technology is more and more into people's lives. Big Data for the commercial, economic, service, and other areas has brought the earth-shaking change. Over the next 10 years will be a Big Data to lead the age of wisdom, both opportunities and challenges. The data in a certain sense has become a new economic asset class. How to use Big Data to create more value will be a new task faced by all industries, especially the power industry. In this paper, Big Data platform model of the power industry and nuclear power survey and design industry is designed. Author focuses on the impact of Big Data on the Electric-power industry.&&&&&Big Data,Industries,Data mining,Data models,Decision making,Power generation,Power systems$$$$$Big Data,electricity supply industry,Internet,mobile computing,nuclear power,power engineering computing$$$$$Big Data platform model,electric-power industry,mobile Internet technology,economic asset,nuclear power survey$$$$$nuclear power survey and design,Big Data foundational models,Data mining,decision making#####Towards Big Data Bayesian Network Learning - An Ensemble Learning Based Approach&&&&&Recently, we are entering the Big Data era[[1]]. The Bayesian Network (BN), as a directed probabilistic graph model, is providing intuitive knowledge presentation and accurate prediction for many mission critical areas. However, the current algorithms do not scale well for Big Data Bayesian network learning. This paper proposes a novel parallel BN learning algorithm called PENBays (Parallel ENsemble based Bayesian Networks Learning), which integrates the best BN learning algorithms MMHC, TPDA and REC. It has three phases: Data Preprocess (DP), Individual Ensemble Learning (IEL) and Central Ensemble Learning (CNL). Through these phases, PENBays effectively learns a BN rapidly from large datasets. Experiments reveal that PENBays learns BNs with better accuracy than base line learning algorithms like MMHC, TPDA and REC, showing promising application potential in the big data mining area.&&&&&Bayes methods,Big data,IEL,Prediction algorithms,Data models,Computational modeling,Algorithm design and analysis$$$$$Bayes methods,Big Data,data mining,directed graphs,learning (artificial intelligence),parallel algorithms$$$$$Big Data Bayesian network learning,ensemble learning based approach,directed probabilistic graph model,knowledge presentation,parallel BN learning algorithm,PENBays,parallel ensemble based Bayesian network learning,MMHC,TPDA,REC,data preprocess,DP,individual ensemble learning,IEL,central ensemble learning,CNL,Big Data mining$$$$$Big Data,Bayesian network,Ensemble learning,Distributed computing#####Big Data representation for grade analysis through Hadoop framework&&&&&Big Data is a large dataset displaying the features of volume, velocity and variety in an OR relationship. Big Data as a large dataset is of no significance if it cannot be exposed to strategic analysis and utilization. There are many software and hardware solutions available in the technological landscape that enable capturing, storing and subsequently analysis of Big Data. Hadoop and its associated technological solution is one of them. Hadoop is the software framework for computing large amount of data. It is made up of four main modules. These modules are Hadoop Common, Hadoop Distributed File System (HDFS), Hadoop YARN, and Hadoop MapReduce. Hadoop MapReduce divides large problem into smaller sub problems under the control of JobTracker. This paper suggests a Big Data representation for grade analytics in an educational context. The study and the experiments can be implemented on R or AWS the cloud infrastructure provided by Amazon.&&&&&Big data,Cloud computing,Hardware,File systems,Computer architecture,Data models$$$$$Big Data,cloud computing,data structures,distributed databases,educational administrative data processing,parallel processing$$$$$Amazon,cloud infrastructure,educational context,JobTracker,Hadoop MapReduce,Hadoop YARN,HDFS,Hadoop distributed file system,Hadoop common,large data amount computation,software framework,OR relationship,grade analysis,big data representation$$$$$Big Data,Hadoop,MapReduce,AWS,HDFS#####Big Data Application in Education: Dropout Prediction in Edx MOOCs&&&&&Educational Data Mining and Learning Analytics are two growing fields of study, trying to make sense of education data and to improve teaching and learning experience. We study dropout prediction in Massively Open Online Courses (MOOCS), where the goal is given student's learning behavior log data in one month, to predict whether students would drop out in next ten days. We collect 39 courses data from XuetangX platform, which is based on the open source Edx platform. We describe our complete approach to cope with drop out prediction task, including data extraction from Edx platform, data preprocessing, feature engineering and performance test on several supervised classification model such as SVM, Logistics Regression, Random Forest and Gradient Boosting Decision Tree. We achieve 88% accuracy in dropout prediction task with GBDT model.&&&&&Data models,Data mining,Predictive models,Feature extraction,Education,Data preprocessing,Big data$$$$$Big Data,computer aided instruction,data mining,distance learning,educational courses$$$$$big data,dropout prediction,Edx MOOC,educational data mining,learning analytics,massively open online courses,student learning behavior log data,XuetangX platform,open source Edx platform,supervised classification model$$$$$MOOC,Big Data,Dropout prediction,Supervised learning#####Data optimised computing for heterogeneous big data computing applications&&&&&The rise of big science techniques is reshaping the provisioning of computing resources and scientific software in large science facilities. As facilities are gearing up for data intensive computing infrastructure, a wave of facility-based big science computing platforms is emerging. This paper presents a new computing paradigm towards designing HPC data analysis platform, named Data Optimised Computing (DOC). The DOC paradigm leverages the characteristics of science data to optimize HPC resource utilization and to improve users' ability to harness a variety of scientific analysis software frameworks. We present a preliminary architectural design of a software platform that implements this approach and also discuss the future directions of this work.&&&&&Software,Data analysis,Tomography,Algorithm design and analysis,Neutrons,Software algorithms,Big data$$$$$Big Data,data analysis,parallel processing,resource allocation$$$$$scientific analysis software frameworks,HPC resource utilization,science data,DOC,HPC data analysis platform,heterogeneous big data computing,data optimised computing$$$$$Data Intensive Science,Data Intensive Computing Platform,Facility Science,HPC resource optimisation,Data Analysis Platform#####Visualization of a Scale Free Network in a Smartphone-Based Multimedia Big Data Environment&&&&&Smartphones equipped with varieties of sensors enable them in participatory and opportunistic crowd-sourced sensing. The built-in as well as external sensors paired with modern smartphones provide an ideal multimedia big data source, where a very large crowd can share audio, video, text, SMS, location, etc. In this paper, we will illustrate our proposed big data framework that has been storing multimedia data from a very large crowd since September 2014. Our framework uses Scale Free Network (SFN) to represent the dynamics of large crowd and produces visualization metrics by running spatio-temporal queries over the proposed multimedia big data framework.&&&&&Multimedia communication,Big data,Data visualization,Sensors,Streaming media,Computer architecture,Smart phones$$$$$Big Data,data visualisation,multimedia computing,query processing,smart phones$$$$$scale free network visualization,smartphone- based multimedia big data environment,opportunistic crowd-sourced sensing,external sensors,multimedia big data source,SMS,SFN,visualization metrics,spatio-temporal queries$$$$$Crowd sourcing,multimedia,big data,spatio-temporal queries#####A performance evaluation of Apache Kafka in support of big data streaming applications&&&&&Stream computing is becoming a more and more popular paradigm as it enables the real-time promise of data analytics. Apache Kafka is currently the most popular framework used to ingest the data streams into the processing platforms. However, how to tune Kafka and how much resources to allocate for it remains a challenge for most users, who now rely mainly on empirical approaches to determine the best parameter settings for their deployments. In this poster, we make a through evaluation of several configurations and performance metrics of Kafka in order to allow users avoid bottlenecks, reach its full potential and avoid bottlenecks and eventually leverage some good practice for efficient stream processing.&&&&&Throughput,Big Data,Real-time systems,Benchmark testing,Measurement,Internet of Things,Sparks$$$$$Big Data,data analysis,fault tolerant computing,resource allocation$$$$$stream processing,big data streaming applications,resource allocation,parameter settings,empirical approaches,processing platforms,data analytics,stream computing,Apache Kafka,performance evaluation,performance metrics$$$$$Stream computing,Apache Kafka,Big Data#####Knowledge Discovery from Big Data for Intrusion Detection Using LDA&&&&&This paper explores a hybrid approach of intrusion detection through knowledge discovery from big data using Latent Dirichlet Allocation (LDA). We identify the "hidden" patterns of operations conducted by both normal users and malicious users from a large volume of network/systems logs, by mapping this problem to the topic modeling problem and leveraging the well established LDA models and learning algorithms. This new approach potentially completes the strength of signature-based and anomaly-based methods.&&&&&Intrusion detection,Monitoring,Big data,Vocabulary,Knowledge discovery,Data models$$$$$Big Data,data mining,learning (artificial intelligence),security of data$$$$$knowledge discovery,Big Data,intrusion detection,LDA,latent Dirichlet allocation,network logs,topic modeling problem,LDA models,learning algorithms,anomaly-based methods,signature-based methods,system logs$$$$$intrusion detection,big data,data mining,LDA#####Research and implementation of big data preprocessing system based on Hadoop&&&&&With the rising growth trend of data size in the Internet era, storage, analysis, and processing of big data arebecomingamong the strongtopics in academia and industry. Typical big data processing platforms adopt the MapReduce programming model to perform application processing. For example, the deployment and calculation method of Hadoop are as follows: Hadoop first collects data and stores them in distributed storage systems, which are storage nodes in clusters. Then, the compute nodes read data from the storage nodes and perform map operations. Lastly, the compute nodes communicate with each other and obtain computation results by performing reduction operations. In the process of collecting and storing data, the storage nodes mainly perform IO operations; hence, the computing resources of these nodes are not fully utilized. This paper proposes a big data preprocessing system based on Hadoop platforms. The main idea of this system is that the data collection and storage phase starts computation operations earlier by utilizing idle computing resources on the basis that IO performance is not affected. This idea can reduce the data size of disk transfer and network communication, and the runtime of applications. Experiments conducted with WordCount, a typical big data processing application, indicate that the system can improve the performance of Hadoop applications.&&&&&Big data,Data preprocessing,Monitoring,Computational modeling,Resource management,Data models,Bandwidth$$$$$Big Data,storage management$$$$$WordCount,disk transfer,storage phase,data collection,MapReduce programming,Hadoop,big data preprocessing system$$$$$big data,preprocessing,Hadoop#####Optimized Multiple Platforms for Big Data Analysis&&&&&The objective of this study is to optimize a multiple big data processing platform with high performance and high availability. The optimization to the integration of Apache Hive, Cloudera Impala and BDAS Spark SQL enables the platform to support SQL queries in a big data environment, automatically select the best performing big data warehouse platform for computing, and receive the same result far more rapidly from the high-performance cache system. The proposed approach significantly improves overall performance, especially in terms of the application of multiple repeated SQL commands in multi-user mode, thus dramatically reducing the query/response time in such scenarios.&&&&&Sparks,Big data,File systems,Databases,Data warehouses,Scalability,Google$$$$$Big Data,cache storage,cloud computing,optimisation,query processing,SQL$$$$$optimized multiple platforms,Big Data analysis,multiple big data processing,optimization,Apache Hive,Cloudera Impala,BDAS Spark SQL,SQL queries,high-performance cache system$$$$$multiple big data processing platform,data warehouse,Distributed memory storage,distributed file system,SQL-like query#####A multimedia big data retrieval framework to detect dyslexia among children&&&&&In this paper we present a tablet-based and big data-based multimedia environment, which uses text, audio, video, and gaze movement to detect a set of symptoms of having dyslexia among children of age both less than 10 and greater than 10. Multi-modal screening test modules have been developed, which gives indications of further dyslexia diagnosis. The multimedia framework is envisioned to accelerate and ease the process of testing dyslexia at the global level, and to identify and auto assess potential Dyslexic patterns and to accumulate huge collection of multimedia test data for in-depth dyslexia pattern analysis.&&&&&Multimedia communication,Streaming media,Medical services,Big Data,Writing,Servers,Media$$$$$Big Data,handicapped aids,medical disorders,multimedia computing,multimedia systems,neurophysiology$$$$$multimedia framework,multimedia test data,in-depth dyslexia pattern analysis,multimedia environment,gaze movement,multimodal screening test modules,dyslexia diagnosis,multimedia Big Data retrieval framework$$$$$Dyslexia,Gaze Tracking,Accessibility,Big Data#####Hadoop as Big Data Operating System -- The Emerging Approach for Managing Challenges of Enterprise Big Data Platform&&&&&Over last few years, innovation in Hadoop and other related Big Data technologies in last few years brings on to the table a lot of promises around better management of enterprise data at much lesser cost and with high value business benefits. In this paper, we propose to delve into details of these challenges from practitioners' perspective based on lessons learnt from various Big Data implementation scenarios. We also aim to discuss the emerging concept of Hadoop as Big Data Operating System to address these challenges with a holistic proposition. Finally, we also plan to provide a prescriptive approach based on best practices which can help moving towards the vision of Enterprise Big Data Platform using Hadoop as Data Operating System balancing between short term objectives and long term goals of managing and maintaining Enterprise Big Data Platform.&&&&&Big data,Operating systems,Industries,Production,Organizations,Engines$$$$$Big Data,business data processing,data mining,database management systems,operating systems (computers)$$$$$Hadoop,Big Data operating system,enterprise Big Data platform,Big Data technologies,enterprise data management,high value business benefits,data mining$$$$$Business analytics,Big Data,Data Mining,Map Reduce,Hadoop,NoSQL#####Utilizing Big Data Analysis for Diseases Prevention and Control During Hajj&&&&&This is a conceptual paper that aims to make use of annually accumulated big data collected from more than two million pilgrims gathered in one place at one time in the Kingdom of Saudi Arabia during the Hajj. This paper proposes an approach to Hajj Health Control (HHC) to handle big data issues and avoid data loss, which will lead to improved healthcare and assist with disease prevention systems during the Hajj period. The implementation of big data methods of management and analysis using Structured Query Language (SQL) is suggested to assist with the epidemiological surveillance of diseases and plans for prevention and control, as well as to help with future health planning and decision making.&&&&&Big data,Ports (Computers),Surveillance,Hospitals$$$$$Big Data,data analysis,diseases,epidemics,health care,SQL$$$$$big data analysis,diseases prevention,Kingdom of Saudi Arabia,Hajj health control,data loss,healthcare improvement,disease prevention system assistance,big data management methods,structured query language,SQL,epidemiological disease surveillance$$$$$Hajj,big data model,health strea,International Classification of Disease,Saudi Arabia#####B-dids: Mining anomalies in a Big-distributed Intrusion Detection System&&&&&The focus of this paper is to present the architecture of a Big-distributed Intrusion Detection System (B-dIDS) to discover multi-pronged attacks which are anomalies existing across multiple subnets in a distributed network. The B-dIDS is composed of two key components: a big data processing engine and an analytics engine. The big data processing is done through HAMR, which is a next generation in-memory MapReduce engine. HAMR has reported high speedups over existing big data solutions across several analytics algorithms. The analytics engine comprises a novel ensemble algorithm, which extracts training data from clusters of the multiple IDS alarms. The clustering is utilized as a preprocessing step to re-label the datasets based on their high similarity to known potential attacks. The overall aim is to predict multi-pronged attacks that are spread across multiple subnets but can be missed if not evaluated in an integrated manner.&&&&&Heat-assisted magnetic recording,Training,Big data,Engines,Intrusion detection,Organizations,Data mining$$$$$Big Data,data mining,security of data$$$$$big-distributed intrusion detection system,B-dIDS,multipronged attacks,big data processing engine,analytics engine,HAMR,MapReduce engine,multiple IDS alarms$$$$$big data,distributed Intrusion Detection System,Ensemble learning#####An outlier detection algorithm based on the degree of sharpness and its applications on traffic big data preprocessing&&&&&Outlier detection is one important research area of data mining, which plays key roles in data preprocessing, equipment fault diagnosis, credit fraud detection, traffic incident detection etc. This paper is devoted to a new outlier detection algorithm based on the degree of sharpness. The proposed algorithm takes a new way to solve the outlier detection problem, which employs a measure in image processing, degree of sharpness, to detect the outliers. Compared to the classical outlier detection methods with statistical learning, the proposed algorithm has no iterative processes. It generates a smooth curve to describe the overall distribution of the data firstly, and then computes the sharpness of degree for each data point. Finally, the outliers are recognized as they have larger values of the degree of sharpness. Also, some practical applications on traffic big data are presented to prove the effectiveness of the proposed algorithm.&&&&&Anomaly detection,Data visualization,Statistical learning,Measurement,Big Data,Roads,Learning systems$$$$$Big Data,data mining,learning (artificial intelligence),traffic engineering computing$$$$$outlier detection algorithm,data mining,data point,traffic Big Data preprocessing,image processing,degree-of-sharpness,data distribution$$$$$outlier detection,degree of sharpness,big traffic data#####Privacy-Aware Big Data Warehouse Architecture&&&&&Along with the ever increasing growth in data collection and its mining, there is an increasing fear of compromising individual and population privacy. Several techniques have been proposed in literature to preserve privacy of collected data while storing and processing. In this paper, we propose a privacy-aware architecture for storing and processing data in a Big Data warehouse. In particular, we propose a flexible, extendable, and adaptable architecture that enforces user specified privacy requirements in the form of Embedded Privacy Agreements. The paper discusses the details of the architecture with some implementation details.&&&&&Data privacy,Big data,Metadata,Servers,Privacy,Computer architecture$$$$$Big Data,data mining,data privacy,data warehouses$$$$$privacy-aware Big Data warehouse architecture,data collection,data mining,privacy preservation,data storing,data processing,flexible-extendable-adaptable architecture,embedded privacy agreements$$$$$architecture,big data,data warehouse,privacy enforcement,security#####Cloud Accounting: The Transition of Accounting Information Model in the Big Data Background&&&&&Cloud Accounting is an online accounting information system based on cloud computing and customer use the computer or other devices to achieve accounting and financial analysis functions. The development and popularization of "cloud" computing may have a great impact on corporate accounting information application and building model, which will provide favorable conditions for the development of SMEs. Based on the author's work and learning experiences, this paper first analyzed the accounting data processing issues under the background of big data, Then, on this basis, the paper proposed the cloud model building strategy under the background of big data, where the author made a profound analysis for the cloud model, Finally, the article proposed the cloud accounting application countermeasures in the big data background.&&&&&Transportation,Big data,Smart cities$$$$$accounts data processing,Big Data,cloud computing,small-to-medium enterprises$$$$$online accounting information system,cloud accounting,cloud computing,financial analysis functions,accounting analysis functions,corporate accounting information application,corporate accounting information building model,SME,accounting data processing,Big Data background$$$$$big data,cloud accounting,cloud model,accounting data#####Study on information recommendation of scientific and technological achievements based on user behavior modeling and big data mining&&&&&This paper brief introduces the source, the concept and characteristics of big data of scientific and technological achievements. The methods and techniques of big data analysis are reviewed. The process of providing personalized service based on user behavior modeling and big data mining is analyzed. The information recommendation service of scientific and technological achievements based on big data analysis is discussed. Combined with the characteristics of personalized service in big data environment, the construction strategy of user behavior model is proposed. The model building method and the personalized service scheme are given at the end.&&&&&Ontologies,Data mining,Big Data,Databases,Correlation,Data models,Libraries$$$$$behavioural sciences computing,Big Data,data mining,recommender systems$$$$$user behavior model,model building method,personalized service scheme,scientific achievements,technological achievements,user behavior modeling,information recommendation service,Big Data mining,construction strategy$$$$$big data,scientific and technological achievements,information recommendation,ontology#####Application of big data in electronic bidding&&&&&With the integration of information technology into social life, data acquisition, transmission, application scale reached an unprecedented level. The quantitative change has accumulated to a certain extent resulting in some industries caused a qualitative change. At present, big data has been applied to many fields of modern science. In China, the large amount of data generated in the process of electronic bidding transaction is a valuable resource, but how to use large data analysis to explore its potential value is still in its infancy. According to the characteristics of large data and electronic bidding, this paper analyzes the application of big data in the electronic bidding, and looks forward to the application prospect of electronic bidding data.&&&&&Big Data,Consumer electronics,Government,Economics,Data mining,Market research$$$$$Big Data,data analysis,electronic commerce,tendering,transaction processing$$$$$quantitative change,qualitative change,big data,electronic bidding transaction,electronic bidding data,information technology,large data analysis$$$$$electronic bidding,big data analysis,application,tendering,bidding#####Weatherman: Exposing weather-based privacy threats in big energy data&&&&&Smart energy meters record electricity consumption and generation at fine-grained intervals, and are among the most widely deployed sensors in the world. Energy data embeds detailed information about a building's energy-efficiency, as well as the behavior of its occupants, which academia and industry are actively working to extract. In many cases, either inadvertently or by design, these third-parties only have access to anonymous energy data without an associated location. The location of energy data is highly useful and highly sensitive information: it can provide important contextual information to improve big data analytics or interpret their results, but it can also enable third-parties to link private behavior derived from energy data with a particular location. In this paper, we present Weatherman, which leverages a suite of analytics techniques to localize the source of anonymous energy data. Our key insight is that energy consumption data, as well as wind and solar generation data, largely correlates with weather, e.g., temperature, wind speed, and cloud cover, and that every location on Earth has a distinct weather signature that uniquely identifies it. Weatherman represents a serious privacy threat, but also a potentially useful tool for researchers working with anonymous smart meter data. We evaluate Weatherman's potential in both areas by localizing data from over one hundred smart meters using a weather database that includes data from over 35,000 locations. Our results show that Weatherman localizes coarse (one-hour resolution) energy consumption, wind, and solar data to within 16.68km, 9.84km, and 5.12km, respectively, on average, which is more accurate using much coarser resolution data than prior work on localizing only anonymous solar data using solar signatures.&&&&&Portable document format$$$$$Big Data,data analysis,data privacy,energy conservation,energy consumption,power consumption,power engineering computing,power meters,power system measurement,smart meters,smart power grids,wireless sensor networks$$$$$Weatherman,weather-based privacy threats,big energy data,building,anonymous energy data,energy consumption data,solar generation data,anonymous smart meter data,localizing data,coarser resolution data,anonymous solar data,Big Data analytics,smart energy meters,electricity consumption,energy efficiency,size 5.12 km,size 16.68 km,size 9.84 km#####Tile based visual analytics for Twitter big data exploratory analysis&&&&&New tools for raw data exploration and characterization of “big data” sets are required to suggest initial hypotheses for testing. The widespread use and adoption of web-based geo maps have provided a familiar set of interactions for exploring extremely large geo data spaces and can be applied to similarly large abstract data spaces. Building on these techniques, a tile based visual analytics system (TBVA) was developed that demonstrates interactive visualization for a one billion point Twitter dataset. TBVA enables John Tukey-inspired exploratory data analysis to be performed on massive data sets of effectively unlimited size.&&&&&Tiles,Twitter,Visual analytics,Heating,Strips,Information management,Data handling$$$$$Big Data,cartography,data analysis,data visualisation,interactive systems,social networking (online)$$$$$tile based visual analytics system,Twitter big data exploratory analysis,Big data set characterization,Web-based geo maps,geo data spaces,abstract data spaces,TBVA,John Tukey-inspired exploratory data analysis,interactive Twitter dataset visualization$$$$$big data,visual analytics,exploratory data analysis#####Big Data, Big Challenges&&&&&Summary form only given. Big data analytics is the process of examining large amounts of data of a variety of types (big data) to uncover hidden patterns, unknown correlations and other useful information. Its revolutionary potential is now universally recognized. Data complexity, heterogeneity, scale, and timeliness make data analysis a clear bottleneck in many biomedical applications, due to the complexity of the patterns and lack of scalability of the underlying algorithms. Advanced machine learning and data mining algorithms are being developed to address one or more challenges listed above. It is typical that the complexity of potential patterns may grow exponentially with respect to the data complexity, and so is the size of the pattern space. To avoid an exhaustive search through the pattern space, machine learning and data mining algorithms usually employ a greedy approach to search for a local optimum in the solution space, or use a branch-and-bound approach to seek optimal solutions, and consequently, are often implemented as iterative or recursive procedures. To improve efficiency, these algorithms often exploit the dependencies between potential patterns to maximize in-memory computation and/or leverage special hardware (such as GPU and FPGA) for acceleration. These lead to strong data dependency, operation dependency, and hardware dependency, and sometimes ad hoc solutions that cannot be generalized to a broader scope. In this talk, I will present some open challenges faced by data scientist in biomedical fields and the current approaches taken to tackle these challenges.&&&&&Complexity theory,Machine learning algorithms,Algorithm design and analysis,Big data,Data mining,Hardware,Conferences$$$$$Big Data,data analysis,data mining,learning (artificial intelligence),medical computing,pattern classification$$$$$data scientist,hardware dependency,operation dependency,data dependency,field programmable gate array,graphics processing unit,FPGA,GPU,recursive procedure,iterative procedure,branch-and-bound approach,greedy approach,pattern space,data mining algorithms,machine learning,biomedical applications,data timeliness,data scale,data heterogeneity,data complexity,data examination,Big Data analytics$$$$$big dataanalytics,data mining,machine learning,BD2K#####A summarization paradigm for big data&&&&&We have developed an efficient summarization paradigm for data drawn from hierarchical domain to construct a succinct view of important large-valued regions (“heavy hitters”). It requires one pass over the data with moderate number of updates per element of the data and requires lesser amount of memory space as compared to existing approaches for approximating hierarchically discounted frequency counts of heavy hitters with provable guarantees. The proposed technique is generic that can make use of existing state-of-the-art sketch-based or count-based frequency estimation approaches. Any algorithm from both of these families can be coupled as a subroutine in the proposed framework without any substantial modifications. Experimental as well as theoretical justifications have been provided for its significance.&&&&&IP networks,Lattices,Frequency estimation,Big data,Approximation algorithms,Accuracy$$$$$Big Data$$$$$summarization paradigm,big data,hierarchical domain,memory space,hierarchically discounted frequency counts,heavy hitters,provable guarantees,state-of-the-art sketch-based frequency estimation approaches,count-based frequency estimation approaches$$$$$Hierarchical Heavy Hitters,Data Summarization,Big Data#####Near Real-Time Big-Data Processing for Data Driven Applications&&&&&This paper addresses the context data integration and processing problem for design of data driven application by introducing ASAPCS (Auto-scaling and Adjustment Platform for Cloud-based Systems) platform. Conceptual model, technical architecture and data integration process are described. The ASAPCS platform supports model-driven configuration, separation of context acquisition and application, utilization of various context processing algorithms and scalability. It is based on technologies that have proven to work well with big data and each part of it is horizontally scalable. ASAPCs integrates data from heterogeneous sources and aggregates raw context data and uses it to perform real-time adjustments in the data-driven application. Its application is illustrated with example of providing data store resilience.&&&&&Context modeling,Real-time systems,Cognition,Adaptation models,Sparks,Data integration,Data models$$$$$Big Data,cloud computing,data integration,ubiquitous computing$$$$$data store resilience,context data integration,conceptual model,technical architecture,ASAPCS platform,context acquisition,context processing algorithms,heterogeneous sources,Big-data processing,raw context data aggregation,autoscaling and adjustment platform for cloud-based systems,model-driven configuration$$$$$stream processing,big data,cloud computing,data-driven systems#####Work in Progress - In-Memory Analysis for Healthcare Big Data&&&&&Advances in healthcare data management and analytics have opened new horizons for healthcare providers such as cost effective treatments, ability to detect medical fraud, and diagnose diseases at an early stage. Central to these abilities is the need for fast ad-hoc query processing of large volumes of complex healthcare datasets. End users who work with healthcare databases spend enormous effort in data exploration since exploration is the first step to any subsequent predictive modeling to generate actionable insights for patients, providers and physicians. Unfortunately, unlike other domains the complexity and volumes of claims (ICD9 or 10) as well as clinical (HL7) healthcare datasets results in data exploration solutions being extremely slow and cumbersome when attempted using traditional disk resident data warehousing approaches. In this paper we describe the first ever attempt of real-time data exploration for healthcare datasets using in-memory databases. We benchmark and compare two such in-memory database systems to study responsiveness and ability to handle complexity of typical health data exploration tasks. We share our work in progress results and outline key issues that need to be addressed for forthcoming advances in this very important big data vertical.&&&&&Medical services,Real-time systems,Medical diagnostic imaging,Big data,Database systems,Relational databases$$$$$Big Data,data analysis,database management systems,health care$$$$$in-memory analysis,healthcare big data,healthcare data management,healthcare data analytics,real-time data exploration,in-memory database systems,health data exploration$$$$$Healthcare,Big Data,In-Memory databases,Real-time prediction#####The AutoMat CVIM - A Scalable Data Model for Automotive Big Data Marketplaces&&&&&In the past years, connectivity has been introduced in automotive production series, enabling vehicles as highly mobile Internet of Things (IoT) sensors and participants. The Horizon 2020 research project AutoMat addressed this scenario by building a vehicle big data marketplace in order to leverage and exploit crowd-sourced sensor data, a so far unexcavated treasure. As part of this project the Common Vehicle Information Model (CVIM) as harmonized data model has been developed. The CVIM allows a common understanding and generic representation, brand-independent throughout the whole data value and processing chain. The demonstrator consists of CVIM vehicle sensor data, which runs through the different components of the whole AutoMat vehicle big data processing pipeline. Finally, at the example of a traffic measurement service the data of a whole vehicle fleet is aggregated and evaluated.&&&&&Big Data,Automobiles,Sensors,Histograms,Cloud computing,Metadata$$$$$automobiles,Big Data,Internet of Things$$$$$AutoMat CVIM,scalable data Model,automotive production series,Horizon 2020 research project AutoMat,crowd-sourced sensor data,harmonized data model,CVIM vehicle sensor data,vehicle fleet,automotive Big Data marketplaces,common vehicle information model,vehicle Big Data marketplace,AutoMat vehicle Big Data processing pipeline,mobile Internet of Things sensors,IoT sensors$$$$$data model,vehicle big data,car to cloud,marketplace#####Defining architecture components of the Big Data Ecosystem&&&&&Big Data are becoming a new technology focus both in science and in industry and motivate technology shift to data centric architecture and operational models. There is a vital need to define the basic information/semantic models, architecture components and operational models that together comprise a so-called Big Data Ecosystem. This paper discusses a nature of Big Data that may originate from different scientific, industry and social activity domains and proposes improved Big Data definition that includes the following parts: Big Data properties (also called Big Data 5V: Volume, Velocity, Variety, Value and Veracity), data models and structures, data analytics, infrastructure and security. The paper discusses paradigm change from traditional host or service based to data centric architecture and operational models in Big Data. The Big Data Architecture Framework (BDAF) is proposed to address all aspects of the Big Data Ecosystem and includes the following components: Big Data Infrastructure, Big Data Analytics, Data structures and models, Big Data Lifecycle Management, Big Data Security. The paper analyses requirements to and provides suggestions how the mentioned above components can address the main Big Data challenges. The presented work intends to provide a consolidated view of the Big Data phenomena and related challenges to modern technologies, and initiate wide discussion.&&&&&Big data,Data models,Computer architecture,Security,Biological system modeling,Ecosystems,Industries$$$$$Big Data,data analysis,security of data$$$$$Big Data ecosystem,data centric architecture,data operational models,information-semantic models,Big Data properties,volume property,velocity property,variety property,value property,veracity property,data models,data structures,data analytics,data infrastructure,data security,Big Data architecture framework,BDAF,Big Data infrastructure,Big Data analytics,Big Data lifecycle management,Big Data security$$$$$Big Data Technology,Big Data Ecosystem,Big Data Architecture Framework (BDAF),Big Data Infrastructure (BDI),Big Data Lifecycle Management (BDLM),Cloud based Big Data Infrastructure Services#####A strategic approach for visualizing the value of big data (SAVV-BIGD) framework&&&&&An organization and its environment are mutually related. By implication, as the business environment constantly evolves, business policies and strategies should be dynamic enough to adapt to and align with the changes impacting the organization. The term Big Data is fast becoming a buzz word and it is believed to carry huge amounts of benefits for both industries and academia. Big Data was initially defined by `3V's (Volume, Velocity & Variety) and most recently with additional `V's. While Big Data can greatly influence the operation of an Organisation, there is a need to first visualize the potential value that can be derived from a Big Data project. Therefore, the purpose of this research is to present a Big data Strategy expressed as the SAVV-BIGD framework that can be used to visualize the value of Big Data. Using Design Science Research Methodology, the framework is coined from the Co-evolutionary IS alignment framework by Benbya & McKelvey.&&&&&Big data,Organizations,Data visualization,Satellite broadcasting,Roads,Industries$$$$$Big Data,data visualisation$$$$$strategic approach for visualizing the value of Big Data,SAVV-BIGD framework,design science research methodology,co-evolutionary IS alignment framework$$$$$Big Data Strategy,SAVV-BIGD Framework,Big Data,Digital Business Strategy,Big Data Implementation,IS Alignment#####Knowledge Management in Big Data Times&&&&&In this era filled with big data, a crucial issue of knowledge management is to create an effective platform to integrate knowledge with new product development. Because of the 4V (Volumes, Variety, Velocity, Value) features of big data, traditional knowledge lifecycle management methods should be improved so as to better control and manage knowledge related activities. This paper thus discusses relevant issues on knowledge management in big data environment and builds a maturity model characterize general knowledge management activities in new product development in big data times.&&&&&Big data,Knowledge management,Context,Product development,Data models,Coordinate measuring machines,Companies$$$$$Big Data,knowledge management,product development$$$$$big data time,knowledge lifecycle management method,knowledge related activity,big data environment,maturity model,general knowledge management activity,new product development$$$$$new product development,knowledge management,big data#####Big data: A new challenge for tourism&&&&&This paper proposes the foundations for a definition of digital tourist survey field based on the study of social networks. Several social network provide essential information on the perceptions of tourist destination. Nevertheless to operate correctly, several points should be discussed. This article provides multiple ideas for reflections on the challenges and opportunities offered by these new media.&&&&&Communities,Photography,Social network services,Data mining,Europe,Data visualization,Big data$$$$$Big Data,social networking (online),travel industry$$$$$big data,tourism,digital tourist survey field,social network,tourist destination$$$$$social network,tourist behavior,big data#####Communication Rules Learning Strategy in Big Data Network Based on SVN Neural Network&&&&&Big data system has great effect on improving the informational level of the application and demand. The Internet of things and wireless sensor network (WSN) is the typical manifestation of big data. Communication process of big data system can be done adopting various wireless communication rules which include 4G, WLAN and WAVE. As each wireless communication rule has different application scene applicability and different communication effects, this paper proposed a communication rules learning strategy based on SVN neural network. This strategy obtained learning samples which indicate the communication mode with best communication performance through simulation of various application scenes in OPENT Modeler. Through the study of neural network algorithm, this designed strategy can output the predicted result mode of wireless communication adaptively under unknown data application scenes. The outstanding results show that the learning strategy based on neural network can accurately choose the optimal communication rule in big data system.&&&&&Transportation,Big Data,Smart cities$$$$$Big Data,learning (artificial intelligence),neural nets,wireless sensor networks$$$$$communication effects,SVN neural network algorithm,data application scenes,WSN,WLAN,WAVE,OPENT Modeler,communication performance,communication mode,wireless communication rule,communication process,wireless sensor network,big data network,communication rules learning strategy,big data system,optimal communication rule$$$$$Big data,wireless Communication Rules,Neural Network,Communication Performance,OPENT Modeler#####Business information modeling: A methodology for data-intensive projects, data science and big data governance&&&&&This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.&&&&&Business,Data models,Terminology,Big data,Data warehouses,Wheels,Standards organizations$$$$$Big Data,business data processing,data warehouses$$$$$business information modeling,data science,Big Data governance,large data-intensive projects,data stewards,data warehouse environments,business requirements analysis,software requirements,accurity glossary tool,banking data warehouse project$$$$$Data Modeling,Project Methodology,Data Governance,Metadata,Information Catalog#####Internet of things and big data analytics for smart oil field malfunction diagnosis&&&&&With the rapid development of information technology and digital communication, the data types are more abundant by integration of various technologies. In this paper, based on the analysis of a large number of historical data of oil and water wells, the changes of some important parameters of the wells can be monitored and then used in the trend prediction and the early warning system. Subsequently, we use 6 Sigma algorithm to process the historical data, and by the big data trend analysis combining with various parameters, we can diagnose six operating conditions, such as sand production, abnormal of moisture content etc. Through experiments, the algorithm is stable and reliable in practical application, and it has great significance to ensure the normal production of oil field and improve the management ability for oil field.&&&&&Oils,Production,Standards,Fluids,Moisture,Big Data,Algorithm design and analysis$$$$$Big Data,data analysis,fault diagnosis,Internet of Things,petroleum industry,production engineering computing,six sigma (quality)$$$$$big data analytics,smart oil field malfunction diagnosis,information technology,digital communication,data types,water wells,early warning system,6 Sigma algorithm,big data trend analysis,oil wells,Internet of things$$$$$internet of thing,warning thresholds scheme,oil field fault diagnosis,big data,6 sigma#####iSkin specialist — A big data based expert system for dermatology&&&&&ISkin Specialist is a work in progress project - leveraging the increased popularity of smartphones and advancements in big data and image recognition technologies to address the shortage of dermatologists worldwide. The idea behind iSkin Specialist is simple - patients from anywhere in the world can take pictures of the affected skin area and upload to the iSkin Specialist Expert System for quick diagnosis for skin conditions like rash, bug bite, etc. The diagnosis in many cases are simple but without proper diagnosis and treatment, some of these skin conditions can have serious health impact and can be fatal.&&&&&Skin,Diseases,Big Data,Lesions,Image recognition,Expert systems$$$$$Big Data,biomedical imaging,expert systems,image recognition,patient diagnosis,skin$$$$$big data,progress project - leveraging,image recognition technologies,iSkin Specialist Expert System,skin conditions,smartphones$$$$$healthcare,dermatology,big data,expert systems#####Software Metrics for Green Parallel Computing of Big Data Systems&&&&&Big Data is typically organized around a distributed file system on top of which the parallel algorithms can be executed for realizing the Big Data analytics. In general, the parallel algorithms can be mapped in different alternative ways to the computing platform. Hereby each alternative will perform differently with respect to the environmentally relevant parameters such as energy and power consumption. Existing studies on deployment of parallel computing algorithms have mainly focused on addressing general computing metrics such as speedup with respect to serial computing and efficiency of the use of the computing nodes. In this paper, we report on the elicitation of green metrics for big data systems that are required when analyzing deployment alternatives. To this end we use the existing systematic literature reviews and identify, and discuss the important green computing metrics for big data systems.&&&&&Measurement,Green products,Big data,Green computing,Bibliographies,Parallel processing,Systematics$$$$$Big Data,data analysis,green computing,parallel processing,software metrics$$$$$software metrics,green parallel computing,big data systems,parallel algorithms,big data analytics,parallel computing algorithms,serial computing$$$$$big data,green computing,parallel computing component,metrics#####An infrastructure and application of computational archival science to enrich and integrate big digital archival data: Using Taiwan Indigenous Peoples Open Research Data (TIPD) as an example&&&&&This paper highlights research on constructing a big archival data called Taiwan Indigenous Peoples Open Research Data (TIPD, see https://osf.io/e4rvz/) based on contemporary census and household registration data sets in 2013-2017 (see http://TIPD.sinica.edu.tw). TIPD utilizes record linkage, geocoding, and high-performance in-memory computing technology to construct various dimensions of Taiwan Indigenous Peoples (TIPs) demographics and developments. Embedded in collecting, cleaning, cleansing, processing, exploring, and enriching individual digital records are archival computational science and data science. TIPD consists of three categories of archival open data: (1) categorical data, (2) household structure and characteristics data, and (3) population dynamics data, including cross-sectional time-series categorical data, longitudinally linked population dynamics data, life tables, household statistics, micro genealogy data, marriage practice and ethnic identity data, internal migration data, geocoded data, etc. TIPD big archival data not only help unveil contemporary TIPs demographics and various developments, but also help overcome research barriers and unleash creativity for TIPs studies.&&&&&Couplings,Distributed databases,Big Data,Sociology,Statistics,Data science,Probabilistic logic$$$$$Big Data,data mining,demography,information retrieval systems,public domain software,records management,time series$$$$$Taiwan Indigenous Peoples demographics,archival computational science,data science,archival open data,characteristics data,cross-sectional time-series categorical data,microgenealogy data,categorical data,record linkage,geocoding,high-performance in-memory computing technology,individual digital record collection,individual digital record cleaning,individual digital record cleansing,individual digital record processing,individual digital record exploration,individual digital record enrichment,household structure,population dynamics data,life tables,household statistics,marriage practice,household registration data sets,contemporary census,Taiwan Indigenous Peoples Open Research Data,big digital archival data,computational archival science,TIPD big archival data,geocoded data,internal migration data,ethnic identity data$$$$$identity,genealogy,in-memory computing,open data,record linkage,TIPD#####Industrial Big Data Analytics: Lessons from the Trenches&&&&&Big Data Analytics in particular and Data Science in general have become key disciplines in the last decade. The convergence of Information Technology, Statistics and Mathematics, to explore and extract information from Big Data have challenged the way many industries used to operate, shifting the decision making process in many organizations. A new breed of Big Data platforms has appeared, to fulfill the needs to process data that is large, complex, variable and rapidly generated. The author describes the experience in this field from a company that provides Big Data analytics as its core business.&&&&&Big data,Joining processes,Distributed databases,Computer architecture,Data mining,Companies,Programming$$$$$Big Data,information retrieval,information technology$$$$$industrial Big Data analytics,data science,information technology,statistics,mathematics,information extraction$$$$$Big Data,Distributed Algorithms,Declarative Programming,Dataflow Programming,Abstraction Models#####The need for new processes, methodologies and tools to support big data teams and improve big data project effectiveness&&&&&As data continues to be produced in massive amounts, with increasing volume, velocity and variety, big data projects are growing in frequency and importance. However, the growth in the use of big data has outstripped the knowledge of how to support teams that need to do big data projects. In fact, while much has been written in terms of the use of algorithms that can help generate insightful analysis, much less has been written about methodologies, tools and frameworks that could enable teams to more effectively and efficiently "do" big data projects. Hence, this paper discusses the key research questions relating methodologies, tools and frameworks to improve big data team effectiveness as well as the potential goals for a big data process methodology. Finally, the paper also discusses related domains, such as software development, operations research and business intelligence, since these fields might provide insight into how to define a big data process methodology.&&&&&Big data,Software,Organizations,Bismuth,Operations research$$$$$Big Data,project management,team working$$$$$Big Data team effectiveness,Big Data project effectiveness,Big Data process methodology,software development,operations research,business intelligence$$$$$Big Data,Data Science,Process Methodology