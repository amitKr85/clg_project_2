Time series prediction via new support vector machines&&&&&In this paper, we present new support vector machines - least squares support vector machines (LS-SVMs). While standard SVMs solutions involve solving quadratic or linear programming problems, the least squares version of SVMs corresponds to solving a set of linear equations, due to equality instead of inequality constraints in the problem formulation. In LS-SVMs, the Mercer condition is still applicable. Hence several types of kernels such as polynomial, RBF's and MLP's can be used. Here we use LS-SVMs for time series prediction compared with radial basis function neural networks. We consider a noisy (Gaussian and uniform noise) Mackey-Glass time series. The results show that our least squares support vector machines are excellent for time series prediction even with high noise.&&&&&Support vector machines,Machine learning,Neural networks,Support vector machine classification,Gaussian noise,Statistical learning,Kernel,Chaos,Radial basis function networks,Least squares methods$$$$$learning automata,time series,learning (artificial intelligence),least squares approximations,radial basis function networks,Gaussian noise$$$$$Mackey-Glass time series,time series prediction,least squares support vector machines,linear equations,equality constraints,Mercer condition,radial basis function neural networks,Gaussian noise,uniform noise,high noise,machine learning,statistical learning theory